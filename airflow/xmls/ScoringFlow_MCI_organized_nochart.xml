<ScoringFlow_MCI_organized_nochart>
    <SleepTask>
        <input_parameters>
            <milliseconds>3000</milliseconds>
        </input_parameters>
        <output_parameters>
        </output_parameters>
    </SleepTask>
    <info:Started_scoring>
        <input_parameters>
            <DataSource1>$DataSourcePortal</DataSource1>
            <queryParametes></queryParametes>
            <sql>insert into core_runbean_attributes (runbean_id, attributes_key, attributes) values ('$WOORKFLOW_RUN_ID', 'checkpoint', 'Started scoring');</sql>
        </input_parameters>
        <output_parameters>
        <out></out>
        </output_parameters>
    </info:Started_scoring>
    <email0>
        <input_parameters>
            <command>echo "scoring run started" + $t2 +| mailx -v -r "churn@mci.ir" -s "FAA Scoring run" $emailaddress3</command>
            <host>localhost</host>
            <identity></identity>
            <password>$LocalhostPassword</password>
            <std.err.file></stderr.file>
            <stdout.file></stdout.file>
            <timeout>20000</timeout>
            <username>$LocalhostUsername</username>
        </input_parameters>
        <output_parameters>
            <exitstatus></exitstatus>
            <stderr></stderr>
            <stdout></stdout>
        </output_parameters>
    </email0>
    <Init_Parameters_for_Scoring>
        <input_parameters>
        </input_parameters>
        <output_parameters>
            <calculate_vargroup_output>"false"</calculate_vargroup_output>
            <initialize_desc_var>"false"</initialize_desc_var>
            <keys>""</keys>
            <model_fitting>$Analytical_model.equals("Fit new model")</model_fitting>
            <override_model_varables_check>'true'</override_model_varables_check>
            <redefine_preprocessing>"false"</redefine_preprocessing>
            <uc_churn_inactivity_include_postpaid>"true"</uc_churn_inactivity_include_postpaid>
            <uc_churn_inactivity_model_id>14</uc_churn_inactivity_model_id>
            <uc_churn_postpaid_model_id>-1</uc_churn_postpaid_model_id>
            <uc_postpaid_include_postpaid>"true"</uc_postpaid_include_postpaid>
            <uc_portout_model_id>13</uc_portout_model_id>
            <uc_product_model_id>-1</uc_product_model_id>
            <uc_zero_day_predicition_model_id>-1</uc_zero_day_predicition_model_id>
            <uc_zero_day_predicition_redefine_value_segments>"false"</uc_zero_day_predicition_redefine_value_segments>
            <vals>""</vals>
        <output_parameters>
    </Init_Parameters_for_Scoring>
    <Check_use_cases>
        <input_parameters>
            <DataSource1>$DataSourcePortal</DataSource1>
            <queryParametes></queryParametes>
            <sql>select active:text from core_categorybean where name_ in ('Best next product prediction', 'Churn postpaid prediction', 'Zero day prediction', 'Port Out Prediction') oder by name_</sql>
        </input_parameters>
        <output_parameters>
        <out></out>
        <uc_include_portout>$out.get(3).get(0)</uc_include_portout>
        <uc_include_zero_day_prediction>$out.get(4).get(0)</uc_include_zero_day_prediction>
        <uc_include_churn_postpaid>$out.get(2).get(0)</uc_include_churn_postpaid>
        <uc_include_churn_inacticity>$out.get(1).get(0)</uc_include_churn_inacticity>
        <uc_include_product>$out.get(0).get(0)</uc_include_product>
        </output_parameters>
    </Check_use_cases>
    <Check_Fitted_Model>
        <input_parameters>
            <DataSource1>$DataSourceAnalytics</DataSource1>
            <queryParametes></queryParametes>
            <sql>select * from work.check_fitted_model_availabilit($model_fitting);</sql>
            <code>
                CREATE FUNCTION check_fitted_model_availability(model_fitting boolean) RETURNS boolean
    AS $$

/* SUMMARY
 * This function checks, if there already is a fitted model, returns true if not.
 *
 * VERSION
 * 14.06.2013 KL  
 */

DECLARE

  mod_job_id_fit integer;
  fitting  boolean;
  
BEGIN

  IF NOT model_fitting THEN

      mod_job_id_fit := a.mod_job_id 
      FROM work.module_job_parameters a
      JOIN work.module_job_parameters b
      ON a.mod_job_id=b.mod_job_id
      JOIN work.module_models c
      ON b.value=c.model_id
      WHERE a.key = 'run_type'
      AND a.value ~ 'Fit'
      AND b.key ~ 'model_id'
      AND c.key ~ 'intercept'
      LIMIT 1; 

    IF mod_job_id_fit IS NULL THEN
      fitting = true;
    ELSE 
      fitting = false;
    END IF;

  ELSE
    fitting = model_fitting;
  END IF;
  
  RETURN fitting;

END;

            </code>
        </input_parameters>
        <output_parameters>
        <out></out>
        <model_fitting>$out.get(0).get(0)</model_fitting>
        </output_parameters>
    </Check_Fitted_Model>
    <split>
        <when>Parameter with name equal to model_fitting and str value equal to true ...</when>
        <tasks>info: Fitting Models, init Parameters for Fit, Run_mci, email1, join, Anfo: Applyind Models</tasks>
        <when>Parameter with name equal to model_fitting and str value equal to false ...</when>
        <tasks>join, Info: Applying Models</tasks>
    </split>
    <info:Fitting_Models>
        <input_parameters>
            <DataSource1>$DataSourcePortal</DataSource1>
            <queryParametes></queryParametes>
            <sql>update core_runbean_attributes set attributes = 'Fitting model on historical data' where runbean_id = '$WOORKFLOW_RUN_ID' and attributes_key = 'checkpoint'</sql>
        </input_parameters>
        <output_parameters>
        <out></out>
        </output_parameters>
    </info:Fitting_Models>
    <Init_Parameters_for_Fit>
        <input_parameters>
        </input_parameters>
        <output_parameters>
            <descvar_interval_weeks>-1</descvar_interval_weeks>
            <lda_cell_events_input_id>-1</lda_cell_events_input_id>
            <lda_cell_events_model_options>"No LDA run"</lda_cell_events_model_options>
            <lda_cell_events_output_id_old>-1</lda_cell_events_output_id_old>
            <lda_input_id>-1</lda_input_id>
            <lda_input_options>"No LDA run"</lda_input_options>
            <lda_model_options>"No LDA run"</lda_model_options>
            <lda_output_id_old>-1</lda_output_id_old>
            <mod_job_id>-1</mod_job_id>
            <redefine_preprocessing>"true"</redefine_preprocessing>
            <run_descvar>"Do not run"</run_descvar>
            <run_fitting>"true"</run_fitting>
            <run_type>"Predictors + Fit + Apply"</run_type>
            <t2>"t2"</t2>
        </output_parameters>
    </Init_Parameters_for_Fit>
    <Run_MCI>
        <init_DataSource1>
            <input_parameters>
            </input_parameters>
            <output_parameters>
                <DataSource1>$DataSourceAnalytics</DataSource1>
            </output_parameters>
        </init_DataSource1>
        <Run_initialize_MCI>
            <initialize_generic_parameters>
            </initialize_generic_parameters>
            <split>
                <when></when>
                <task>split:run_product</task>
                <when></when>
                <task>split:run_churn_inactivity</task>
            </split>
            <split:run_product>
                <when>parameter with name equal to uc_include_product and str value equal to true</when>
                <tasks>initialize_product, join, end</tasks>
                <when>parameter with name equal to uc_include_product and str equal to false</when>
                <tasks>join, end</tasks>
            </split:run_product>
            <split:run_churn_inactivity>
                <when>parameter with name equal to uc_include_churn_inactivity and str value equal to true</when>
                <tasks>initialize_churn_inactivity, join, end</tasks>
                <when>parameter with name equal to uc_include_churn_inactivity and str value equal to false</when>
                <tasks>join, end</tasks>
            </split:run_churn_inactivity>
            <split:run_churn_postpaid>
                <when>parameter with name equal to uc_include_churn_postpaid and str value equal to true</when>
                <tasks>initialize_churn_postpaid, join, end</tasks>
                <when>parameter with name equal to uc_include_churn_postpaid and str value equal to false</when>
                <tasks>join, end</tasks>
            </split:run_churn_postpaid>
            <split:run_zer_day_prediction>
                <when>parameter with name equal to uc_include_zero_day_prediction and str value equal to true</when>
                <tasks>initialize_zero_day_prediction, join, end</tasks>
                <when>parameter with name equal to uc_include_zero_day_prediction and str value equal to false</when>
                <tasks>join, end</tasks>
            </split:run_zer_day_prediction>
            <split:run_portout>
                <when>parameter with name equal to uc_include_portout and str value equal to true</when>
                <tasks>initialize_portout, join, end</tasks>
                <when>parameter with name equal to uc_include_portout and str value equal to false</when>
                <tasks>join, end</tasks>
            </split:run_portout>
            <initialize_product>
            </initialize_product>
            <initialize_churn_inactivity>
            </initialize_churn_inactivity>
            <initialize_churn_postpaid>
            </initialize_churn_postpaid>
            <initialize_zero_day_prediction>
            </initialize_zero_day_prediction>
            <initialize_portout>
            </initialize_portout>
        </Run_initialize_MCI>
        <split:calculate_initialize_MCI>
            <when>parameter with name equal to calculate_targets and str value equal to true</when>
            <tasks>Run_targets_MCI, join</tasks>
            <when>parameter with name equal to calculate_targets and str value equal to false</when>
            <tasks>join</tasks>
        </split:calculate_initialize_MCI>
        <run_targets_MCI>
            <split>
                <when></when>
                <task>split:run_product</task>
                <when></when>
                <task>split:run_churn_inactivity</task>
            </split>
            <split:run_product>
                <when>parameter with name equal to uc_include_product and str value equal to true</when>
                <tasks>create_targets_for_product, combine_target_lists, join, end</tasks>
                <when>parameter with name equal to uc_include_product and str equal to false</when>
                <tasks>join, combine_target_lists, end</tasks>
            </split:run_product>
            <split:run_churn_inactivity>
                <when>parameter with name equal to uc_include_churn_inactivity and str value equal to true</when>
                <tasks>create_targets_for_churn_inactivity, combine_target_lists, join, end</tasks>
                <when>parameter with name equal to uc_include_churn_inactivity and str value equal to false</when>
                <tasks>join, combine_target_lists, end</tasks>
            </split:run_churn_inactivity>
            <split:run_churn_postpaid>
                <when>parameter with name equal to uc_include_churn_postpaid and str value equal to true</when>
                <tasks>create_targets_for_churn_postpaid, combine_target_lists, join, end</tasks>
                <when>parameter with name equal to uc_include_churn_postpaid and str value equal to false</when>
                <tasks>join, combine_target_lists, end</tasks>
            </split:run_churn_postpaid>
            <split:run_zer_day_prediction>
                <when>parameter with name equal to uc_include_zero_day_prediction and str value equal to true</when>
                <tasks>create_targets_for_zero_day_prediction, combine_target_lists, join, end</tasks>
                <when>parameter with name equal to uc_include_zero_day_prediction and str value equal to false</when>
                <tasks>join, combine_target_lists, end</tasks>
            </split:run_zer_day_prediction>
            <split:run_portout>
                <when>parameter with name equal to uc_include_portout and str value equal to true</when>
                <tasks>create_targets_for_portout, combine_target_lists, join, end</tasks>
                <when>parameter with name equal to uc_include_portout and str value equal to false</when>
                <tasks>join, combine_target_lists, end</tasks>
            </split:run_portout>
            <create_targets_for_product>
                <input_parameters>
                    <DataSource1></DataSource1>
                    <queryparameters></queryparameters>
                    <sql>select * from work.create_target_list_product($mod_job_id)</sql>
                    <code>
                        
CREATE FUNCTION create_target_list_product(in_mod_job_id integer) RETURNS integer
    AS $$

/* SUMMARY
 * This function determines the set of aliases that will be processed during a
 * certain module job in the Best next product use case. If possible, the value of target 
 * variable is also calculated. The function writes to tmp.module_targets_tmp. After
 * calling the use case -specific create_target_list functions, calling work.create_target_list
 * combines the target lists into table work.module_targets.
 *
 * VERSION
 * 07.08.2013 HMa Consider only outgoing traffic as activity
 * 30.07.2013 KL Fixed to work right with new blacklists.
 * 17.05.2013 KL  Removed alias_id:s in aliases.blacklist from targets
 * 12.04.2013 HMa - IFIC-126
 * 31.01.2013 HMa
 */

DECLARE

  mjp            record;  
  product_arr    text[];
  prod           text;
  ind            integer;

BEGIN

  SELECT DISTINCT
    in_mod_job_id AS mod_job_id,
    max(CASE WHEN lower(d.key) = 'tcrm'                              THEN to_date(d.value, 'YYYY-MM-DD') ELSE NULL END) AS tcrm,
    max(CASE WHEN lower(d.key) = 'uc_product_products'               THEN d.value                        ELSE NULL END) AS products,
    max(CASE WHEN lower(d.key) = 'uc_product_target_calculated_from' THEN d.value                        ELSE NULL END) AS target_calculated_from,
    max(CASE WHEN lower(d.key) = 't2'                                THEN to_date(d.value, 'YYYY-MM-DD') ELSE NULL END) AS t2,
    max(CASE WHEN lower(d.key) = 'uc_product_t4'                     THEN to_date(d.value, 'YYYY-MM-DD') ELSE NULL END) AS t4,
    max(CASE WHEN lower(d.key) = 'uc_product_t5'                     THEN to_date(d.value, 'YYYY-MM-DD') ELSE NULL END) AS t5
  INTO mjp
  FROM work.module_job_parameters AS d
  WHERE d.mod_job_id = in_mod_job_id;


  product_arr    := string_to_array(trim(mjp.products),',');
  
  FOR ind IN 1..array_upper(product_arr, 1) LOOP
    
    -- Use case name:
    prod    := product_arr[ind];

    -- Check if the tmp.module_targets_tmp table already contains data for this product:
    IF ( SELECT mt.alias_id FROM tmp.module_targets_tmp AS mt WHERE mt.mod_job_id = in_mod_job_id AND use_case_name = 'product_' || prod LIMIT 1 ) IS NOT NULL THEN
      RAISE EXCEPTION 'Target list has already been calculated for the best next product use case, product %, module_job_id %', prod, in_mod_job_id;
    END IF;

    IF mjp.target_calculated_from = 'target'  THEN
      -- Targets:
      --   * Are active during the last week of the source period 
      --   * Do not have the product during the last week of the source period
      --   * Subscribers for whom it is not possible to activate the product are excluded. 
      --     If it is not known if the product can be activated, the subscriber is included. 
      -- target =  1:  Those who bought the product during the target period
      -- target = -1: Those who did not buy the product during the target period
      -- target = NULL: Those who bought the product after the source period but before the target period
      -- In the prediction phase (when we have data until t2), all the subscribers get target value -1.
      -- The table data.product is assumed to be delivered/filled weekly and dated to the Sunday of the week
      -- It should always contain a record for all the products that are activated at least part of the week. 

      INSERT INTO tmp.module_targets_tmp ( mod_job_id, alias_id, use_case_name, target )
      SELECT 
        in_mod_job_id AS mod_job_id,
        crm.alias_id,
        'product_' || prod AS use_case_name,
        CASE 
          WHEN target = 1 THEN 1
          WHEN target IS NULL THEN -1 -- Not all the subscribers who do not have the product are necessarily in the data.product data
          WHEN target = 2 THEN NULL -- Those who buy the product before the campaign
        END AS target
      FROM data.in_crm AS crm -- select subscribers in the CRM
      INNER JOIN ( -- only active subscribers
        SELECT act.alias_id FROM (
          SELECT alias_id, monday FROM data.call_types_weekly WHERE direction = 'm'
          UNION SELECT charged_id AS alias_id, date_trunc('week', timestamp)::date AS monday FROM data.topup
        ) act
        WHERE act.monday >= mjp.t2 - 7 -- activity during the last week of the source period is required
        AND act.monday < mjp.t2
        GROUP BY act.alias_id
      ) d
      ON crm.alias_id = d.alias_id
      LEFT JOIN (
        SELECT alias_id 
        FROM aliases.blacklist
        WHERE validity = 
          (SELECT max(validity) 
           FROM aliases.blacklist
           WHERE validity <= mjp.t2)
      ) bl 
      ON d.alias_id=bl.alias_id 
      LEFT JOIN ( -- Determine target value from product taken date:
        SELECT 
          ps1.alias_id,
          max(
            CASE
                WHEN ps1.product_taken_date IS NOT NULL
                AND ps1.product_taken_date >= mjp.t4 
                AND ps1.product_taken_date <  mjp.t5 
              THEN 1
                WHEN ps1.product_taken_date IS NOT NULL
                AND ps1.product_taken_date >= mjp.t2 
                AND ps1.product_taken_date < mjp.t4
              THEN 2 -- Those who buy the product before the campaign
              ELSE NULL
            END
		  ) AS target
        FROM data.product AS ps1
        WHERE ps1.product_id = prod
        AND ps1.date_inserted >= mjp.t2
        GROUP BY ps1.alias_id
      ) AS p
      ON crm.alias_id = p.alias_id
      LEFT JOIN (  -- select subscribers who do not have the product on the last week of the source period
        SELECT
          alias_id,
          product_taken_date,
          product_churn_date,
          product_possible
        FROM data.product
        WHERE date_inserted = mjp.t2 - 1
        AND product_id = prod
      ) AS p1
      ON crm.alias_id = p1.alias_id
      WHERE crm.date_inserted = mjp.tcrm
      AND bl.alias_id IS NULL
      AND (p1.product_taken_date IS NULL OR p1.product_churn_date < mjp.t2 - 7) -- targets should not have the product during the last week of the source period
      AND (p1.product_possible OR p1.product_possible IS NULL); -- the product should be possible to activate for the targets (or product_possible status not known)
      

    ELSIF mjp.target_calculated_from = 'source'  THEN
      -- Targets: 
      --   * Are active during the end of the source period 
      --   * Subscribers for whom it is not possible to activate the product are excluded. 
      --     If it is not known if the product can be activated, the subscriber is included. 
      -- Target = 1:  Have the product in the end of the source period (last week)
      -- Target = -1: Do not have the product in the end of the source period (last week)
      -- Only subscribers with target = -1 are scored

      INSERT INTO tmp.module_targets_tmp ( mod_job_id, alias_id, use_case_name, target )
      SELECT 
        in_mod_job_id AS mod_job_id,
        crm.alias_id,
        'product_' || prod AS use_case_name,
        CASE 
          WHEN target = 1 THEN 1
          WHEN target IS NULL THEN -1 -- Not all the subscribers who do not have the product are necessarily in the data.product data
          WHEN target = 2 THEN NULL -- Those who buy the product before the campaign
        END AS target
      FROM data.in_crm AS crm -- select subscribers in the CRM
      INNER JOIN ( -- only active subscribers
        SELECT act.alias_id FROM (
          SELECT alias_id, monday FROM data.call_types_weekly WHERE direction = 'm'
          UNION SELECT charged_id AS alias_id, date_trunc('week', timestamp)::date AS monday FROM data.topup
        ) act
        WHERE act.monday >= mjp.t2 - 7 -- activity during the last week of the source period is required
        AND act.monday < mjp.t2
       GROUP BY act.alias_id
      ) d
      ON crm.alias_id = d.alias_id
      LEFT JOIN (
        SELECT alias_id 
        FROM aliases.blacklist
        WHERE validity = 
          (SELECT max(validity) 
           FROM aliases.blacklist
           WHERE validity <= mjp.t2)
      ) bl
      ON d.alias_id=bl.alias_id 
      LEFT JOIN ( -- Determine target value from product status during the last week of the source period:
        SELECT 
          ps1.alias_id,
          max(
            CASE
                WHEN ps1.product_taken_date IS NOT NULL -- The product has been activated before t2
                AND  ps1.product_churn_date IS NULL     -- The product has not been deactivated before t2
              THEN 1
              ELSE NULL
            END
		  ) AS target
        FROM data.product AS ps1
        WHERE ps1.product_id = prod
--        AND (ps1.subproduct_id = subprod OR (CASE WHEN subprod IS NULL THEN TRUE END)) -- If subproduct ID is not given, any subproduct of this product is included
        AND ps1.date_inserted = mjp.t2
        GROUP BY ps1.alias_id
      ) AS p
      ON crm.alias_id = p.alias_id
      LEFT JOIN (  -- select subscribers for whom it is possible to activate the product (or not known if possible) according to the latest product data from the source period
        SELECT
          alias_id,
          product_possible
        FROM data.product
        WHERE date_inserted = mjp.t2
        AND product_id = prod
--        AND (subproduct_id = subprod OR (CASE WHEN subprod IS NULL THEN TRUE END)) -- If subproduct ID is not given, any subproduct of this product is included
      ) AS p1
      ON crm.alias_id = p1.alias_id
      WHERE crm.date_inserted = mjp.tcrm
      AND bl.alias_id IS NULL
      AND (p1.product_possible OR p1.product_possible IS NULL); -- the product should be possible to activate for the targets (or product_possible status not known)

    ELSE
    
      RAISE EXCEPTION 'Best next product use case target needs to be calcualted either from source or target period!';

    END IF;


  END LOOP;

  RETURN ( SELECT count(mt.*) FROM tmp.module_targets_tmp AS mt WHERE mt.mod_job_id = in_mod_job_id AND use_case_name ~ 'product' );

END;

                    </code>
                </input_parameters>
                <output_parameters>
                    <out></out>
                </output_parameters>
            </create_targets_for_product>
            <create_targets_for_churn_inactivity>
                <input_parameters>
                    <DataSource1></DataSource1>
                    <queryparameters></queryparameters>
                    <sql>select * from work.create_target_list_churn_inactivity($mod_job_id)</sql>
                    <code>
                        
CREATE FUNCTION create_target_list_churn_inactivity(in_mod_job_id integer) RETURNS integer
    AS $$

/* SUMMARY
 * This function determines the set of aliases that will be processed during a
 * certain module job in the churn_inactivity use case. If possible, the value of target 
 * variable is also calculated. The function writes to tmp.module_targets_tmp. After
 * calling the use case-specific create_target_list functions, calling work.create_target_list
 * combines the target lists into table work.module_targets.
 *
 * VERSION
 * 07.08.2013 HMa Consider only outgoing traffic as activity
 * 30.07.2013 KL Fixed to work right with new blacklists.
 * 28.06.2013 AV Check target count, positive/negative target percentages
 * 30.05.2013 KL  Changed some queries to execute to get function to work when creating apply quality charts
  * 17.05.2013 KL  Removed alias_id:s in aliases.blacklist from targets
 * 11.01.2013 HMa
 */

DECLARE

  mjp record;  
  query text;
  target_check boolean; 
  results record;

BEGIN

  SELECT DISTINCT
    in_mod_job_id AS mod_job_id,
    max(CASE WHEN d.key        IN ('t2', 'uc_churn_inactivity_t2') THEN to_date(d.value, 'YYYY-MM-DD') ELSE NULL END) AS t2,
    max(CASE WHEN d.key        = 'uc_churn_inactivity_include_postpaid' THEN d.value::smallint ELSE NULL END) AS uc_churn_inactivity_include_postpaid,
    max(CASE WHEN d.key        = 'uc_churn_inactivity_t4' THEN to_date(d.value, 'YYYY-MM-DD')  ELSE NULL END) AS uc_churn_inactivity_t4,
    max(CASE WHEN d.key        = 'uc_churn_inactivity_t5' THEN to_date(d.value, 'YYYY-MM-DD')  ELSE NULL END) AS uc_churn_inactivity_t5,
    max(CASE WHEN d.key        = 'uc_churn_inactivity_t6' THEN to_date(d.value, 'YYYY-MM-DD')  ELSE NULL END) AS uc_churn_inactivity_t6,
    max(CASE WHEN d.key        = 'uc_churn_inactivity_t7' THEN to_date(d.value, 'YYYY-MM-DD')  ELSE NULL END) AS uc_churn_inactivity_t7,
    max(CASE WHEN lower(d.key) IN ('tcrm', 'uc_churn_inactivity_tcrm') THEN to_date(d.value, 'YYYY-MM-DD') ELSE NULL END) AS tcrm,
    max(CASE WHEN d.key        = 'run_type' THEN d.value ELSE NULL END) AS run_type
  INTO mjp
  FROM work.module_job_parameters AS d
  WHERE d.mod_job_id = in_mod_job_id;
  
query = 'SELECT (SELECT mt.alias_id FROM tmp.module_targets_tmp AS mt WHERE mt.mod_job_id = '||in_mod_job_id||' AND use_case_name = ''churn_inactivity'' LIMIT 1 ) IS NOT NULL';

EXECUTE query into target_check;

  
  -- Check if the tmp module_target table already contains data for this use case:
  IF target_check THEN

    RAISE EXCEPTION 'Target list has already been calculated for churn_inactivity use_case, mod_job_id %', in_mod_job_id;
    
  END IF;

  INSERT INTO tmp.module_targets_tmp ( mod_job_id, alias_id, use_case_name, target )
  SELECT 
    in_mod_job_id AS mod_job_id,
    a.alias_id,
    'churn_inactivity' AS use_case_name,
    b.churn_status AS target
  FROM data.in_crm AS a
  INNER JOIN (
    SELECT
      d.alias_id,
      max(CASE WHEN d.monday < mjp.t2 THEN 1 ELSE 0 END) AS source_period_end_activity,
      CASE 
        WHEN max(d.monday) >= mjp.uc_churn_inactivity_t6 THEN -1 -- Churner if no activity during t6 - t7
        WHEN max(d.monday) >= mjp.uc_churn_inactivity_t4 AND max(d.monday) < mjp.uc_churn_inactivity_t5 THEN 1 -- Campaign period is t4 - t5 when churner has to be active
        ELSE NULL -- Churned but wasn't active during the campaign period
      END AS churn_status    
    FROM (
     SELECT alias_id, monday FROM data.call_types_weekly WHERE direction = 'm'
     UNION SELECT charged_id AS alias_id, date_trunc('week', timestamp + interval '2 days')::date - interval '2 days' AS monday FROM data.topup
    ) d
    LEFT JOIN (
      SELECT alias_id 
      FROM aliases.blacklist 
      WHERE validity = 
        (SELECT max(validity)
         FROM aliases.blacklist
         WHERE validity <= mjp.t2 
         ) 
      ) bl
    ON d.alias_id=bl.alias_id 
    WHERE 
      d.monday >= mjp.t2 - 7 
      AND d.monday < mjp.uc_churn_inactivity_t7
      AND bl.alias_id IS NULL
    GROUP BY d.alias_id
  ) AS b
  ON a.alias_id = b.alias_id
  WHERE a.date_inserted = mjp.tcrm
  AND (a.payment_type = 'prepaid' OR (CASE WHEN mjp.uc_churn_inactivity_include_postpaid = 1 THEN a.payment_type = 'postpaid' END))
  AND b.source_period_end_activity = 1; -- Activity during the end of source period is required

  SELECT count(mt.*) as total, SUM(CASE WHEN mt.target = 1 THEN 1 ELSE 0 END) AS pos, SUM(CASE WHEN mt.target = -1 THEN 1 ELSE 0 END) AS neg
  INTO results
  FROM tmp.module_targets_tmp AS mt 
  WHERE mt.mod_job_id = in_mod_job_id 
  AND use_case_name = 'churn_inactivity';

  IF (results.total = 0) THEN
     RAISE EXCEPTION 'Please check data for churn inactivity use case.';
  END IF;
  
  IF (mjp.run_type ~ 'Fit') THEN
    IF ((results.pos::double precision / results.total::double precision) < 0.001) THEN -- 0.1% for positive
      RAISE EXCEPTION 'Please check data for churn inactivity use case. There is not enough positive targets.';
    END IF;
    IF ((results.neg::double precision / results.total::double precision) < 0.1) THEN -- 10% for negative
      RAISE EXCEPTION 'Please check data for churn inactivity use case. There is not enough negative targets.';
    END IF;
  END IF;
  
  RETURN results.total;

END;

                    </code>
                </input_parameters>
                <output_parameters>
                    <out></out>
                </output_parameters>
            </create_targets_for_churn_inactivity>
            <create_targets_for_churn_postpaid>
                <input_parameters>
                    <DataSource1></DataSource1>
                    <queryparameters></queryparameters>
                    <sql>select * from work.create_target_list_churn_postpaid($mod_job_id)</sql>
                    <code>
                        
CREATE FUNCTION create_target_list_churn_postpaid(in_mod_job_id integer) RETURNS integer
    AS $$

/* SUMMARY
 * This function determines the set of aliases that will be processed during a
 * certain module job in the churn_postpaid use case. If possible, the value of target 
 * variable is also calculated. The function writes to tmp.module_targets_tmp. After
 * calling the use case-specific create_target_list functions, calling work.create_target_list
 * combines the target lists into table work.module_targets.
 *
 * VERSION
 * 
 * 30.07.2013 KL Fixed to work right with new blacklists.
 * 28.06.2013 AV Check target count, positive/negative target percentages
 * 30.05.2013 KL  Changed some queries to execute to get function to work when creating apply quality charts
 * 17.05.2013 KL  Removed alias_id:s in aliases.blacklist from targets
 * 11.01.2013 HMa
 */

DECLARE

  mjp record;  
  query text;
  target_check boolean; 
  result record;

BEGIN

  SELECT DISTINCT
    in_mod_job_id AS mod_job_id,
    max(CASE WHEN d.key       IN ('t2', 'uc_churn_postpaid_t2') THEN to_date(d.value, 'YYYY-MM-DD')   ELSE NULL END) AS t2,
    max(CASE WHEN d.key        = 'uc_churn_postpaid_t4' THEN to_date(d.value, 'YYYY-MM-DD')   ELSE NULL END) AS t4,
    max(CASE WHEN d.key        = 'uc_churn_postpaid_t5' THEN to_date(d.value, 'YYYY-MM-DD')   ELSE NULL END) AS t5,
    max(CASE WHEN lower(d.key) IN ('tcrm', 'uc_churn_postpaid_tcrm') THEN to_date(d.value, 'YYYY-MM-DD') ELSE NULL END) AS tcrm,
    max(CASE WHEN d.key        = 'run_type' THEN d.value ELSE NULL END) AS run_type
  INTO mjp
  FROM work.module_job_parameters AS d
  WHERE d.mod_job_id = in_mod_job_id;

  
query = 'SELECT (SELECT mt.alias_id FROM tmp.module_targets_tmp AS mt WHERE mt.mod_job_id = '||in_mod_job_id||' AND use_case_name = ''churn_postpaid'' LIMIT 1 ) IS NOT NULL';

EXECUTE query into target_check;

  
  -- Check if the tmp module_target table already contains data for this use case:
  IF target_check THEN

    RAISE EXCEPTION 'Target list has already been calculated for churn_postpaid use_case, mod_job_id %', in_mod_job_id;
    
  END IF;

  INSERT INTO tmp.module_targets_tmp ( mod_job_id, alias_id, use_case_name, target )
  SELECT DISTINCT 
    in_mod_job_id AS mod_job_id, 
    a.alias_id, 
    'churn_postpaid' AS use_case_name,
    CASE
      WHEN b.churn_notification_date IS NOT NULL AND b.churn_notification_date <= mjp.t5 AND b.churn_notification_date >= mjp.t4 THEN 1
      WHEN b.churn_notification_date IS NOT NULL AND b.churn_notification_date < mjp.t4 THEN NULL
      ELSE -1
    END AS target
  FROM data.in_crm AS a
  LEFT JOIN (
    SELECT crm.alias_id, max(crm.churn_notification_date) AS churn_notification_date
    FROM data.in_crm AS crm
    WHERE crm.date_inserted <= date_trunc('month',mjp.t5) + '1 month'::interval -- assumes that CRM data is delivered monthly and dated on the 1st of the next month
    GROUP BY crm.alias_id
  ) AS b
  ON a.alias_id = b.alias_id
  LEFT JOIN (
      SELECT alias_id 
      FROM aliases.blacklist 
      WHERE validity = 
        (SELECT max(validity)
         FROM aliases.blacklist
         WHERE validity <= mjp.t2 
         ) 
      ) bl
  ON b.alias_id=bl.alias_id 
  WHERE a.date_inserted = mjp.tcrm
  AND bl.alias_id IS NULL
  AND a.payment_type = 'postpaid'
  AND ( a.churn_notification_date IS NULL OR a.churn_notification_date >= mjp.t2 );

  SELECT count(mt.*) as total, SUM(CASE WHEN mt.target = 1 THEN 1 ELSE 0 END) AS pos, SUM(CASE WHEN mt.target = -1 THEN 1 ELSE 0 END) AS neg
  INTO result
  FROM tmp.module_targets_tmp AS mt 
  WHERE mt.mod_job_id = in_mod_job_id 
  AND use_case_name = 'churn_postpaid';

  IF (result.total = 0) THEN
     RAISE EXCEPTION 'Please check data for churn postpaid use case.';
  END IF;
  
  IF (mjp.run_type ~ 'Fit') THEN
    IF ((result.pos::double precision/result.total::double precision) < 0.001) THEN -- 0.1% for positive
      RAISE EXCEPTION 'Please check data for churn postpaid use case. There is not enough positive targets.';
    END IF;
    IF ((result.neg::double precision/result.total::double precision) < 0.1) THEN -- 10% for negative
      RAISE EXCEPTION 'Please check data for churn postpaid use case. There is not enough negative targets.';
    END IF;
  END IF;
  
  RETURN result.total;

END;

                    </code>
                </input_parameters>
                <output_parameters>
                    <out></out>
                </output_parameters>
            </create_targets_for_churn_postpaid>
            <create_targets_for_zero_day_prediction>
                <input_parameters>
                    <DataSource1></DataSource1>
                    <queryparameters></queryparameters>
                    <sql>select * from work.create_target_list_zero_day_prediction($mod_job_id)</sql>
                    <code>
                        
CREATE FUNCTION create_target_list_zero_day_prediction(in_mod_job_id integer) RETURNS integer
    AS $$

/* SUMMARY
 * This function determines the set of aliases that will be processed during a
 * certain module job in the zero_day_prediction use case. The function writes to 
 * tmp.module_targets_tmp. After calling the use case-specific create_target_list 
 * functions, calling work.create_target_list combines the target lists into table 
 * work.module_targets.
 *
 * Value segment is stored as the value of the target variable so that 1 = low value,
 * 2 = medium value and 3 = high value. Changing the number of value segments will
 * require some modifications to the code. When existing model is applied (i.e. value
 * segment for new subscribers is not determined from data) target is -1.
 *
 * VERSION
 * 2013-07-30 KL Fixed to work right with new blacklists.
 * 2013-05-22 KL  Removed alias_id:s in aliases.blacklist from targets
 * 2013-01-24 JVi: Cleaned and commented
 * 2013-01-18 JVi: Created
 */

DECLARE

  --Parametres

  uc_zero_day_prediction_model_id                 int;

  t2                                              date;
  t9                                              date; 
  uc_zero_day_prediction_t4                       date;
  uc_zero_day_prediction_t5                       date;
  this_uc_zero_day_prediction_segment_limits      double precision[];
  uc_zero_day_prediction_redefine_value_segments  boolean;

  run_type                                        text;
  calculate_predictors                            boolean;
  fit_model                                       boolean;
  apply_model                                     boolean;

  --Miscellaneous  
  temp_table_sql                                  text;
  mjp                                             record;  
  tmp                                             double precision;

BEGIN

  --Fetch existing parameters from module job parameters
  t9                                             := m.value::date    FROM work.module_job_parameters m 
                                                      WHERE m.mod_job_id = in_mod_job_id AND m.key = 't9';
  t2                                             := m.value::date    FROM work.module_job_parameters m 
                                                      WHERE m.mod_job_id = in_mod_job_id AND m.key = 't2';
  run_type                                       := m.value::text    FROM work.module_job_parameters m 
                                                      WHERE m.mod_job_id = in_mod_job_id AND m.key = 'run_type';
  uc_zero_day_prediction_t4                      := m.value::date    FROM work.module_job_parameters m 
                                                      WHERE m.mod_job_id = in_mod_job_id AND m.key = 'uc_zero_day_prediction_t4';
  uc_zero_day_prediction_t5                      := m.value::date    FROM work.module_job_parameters m 
                                                      WHERE m.mod_job_id = in_mod_job_id AND m.key = 'uc_zero_day_prediction_t5';
  uc_zero_day_prediction_model_id                := m.value::int     FROM work.module_job_parameters m 
                                                      WHERE m.mod_job_id = in_mod_job_id AND m.key = 'uc_zero_day_prediction_model_id';
  uc_zero_day_prediction_redefine_value_segments := CASE WHEN (SELECT m.value::text FROM work.module_job_parameters m 
                                                      WHERE m.mod_job_id = in_mod_job_id AND m.key = 'uc_zero_day_prediction_redefine_value_segments') = 'true'
                                                      THEN true ELSE false END;
                                                      
  --Evaluate which parts of the workflow are run
  calculate_predictors := run_type ~ 'Predictors';
  fit_model := run_type ~ 'Fit';
  apply_model := run_type ~ 'Apply';

  --Existing value segment limits are used if possible. If no module job with smaller (or equal) id than 
  --current job is found that has the value segments defined, they will be calculated from the
  --current data. User can always force value segment estimation from data via setting the run parameter
  --uc_zero_day_prediction_redefine_value_segments to true (workflow configuration).
  IF uc_zero_day_prediction_redefine_value_segments AND NOT (fit_model OR (calculate_predictors AND (NOT fit_model AND NOT apply_model))) THEN
    RAISE WARNING 'Unable to redefine value segment limits, make sure eight weeks of data is available (run type is Predictors or Predictors + Fit).';
    uc_zero_day_prediction_redefine_value_segments := FALSE;
  END IF;

  IF NOT uc_zero_day_prediction_redefine_value_segments THEN 
    tmp := MAX(m.mod_job_id) FROM work.module_job_parameters m 
      WHERE m.mod_job_id <= in_mod_job_id 
      AND   m.key         = 'uc_zero_day_prediction_segment_limits' 
      AND   m.value         IS NOT NULL;
    IF tmp IS NOT NULL THEN
      this_uc_zero_day_prediction_segment_limits := string_to_array(m.value,',')::double precision[] FROM work.module_job_parameters m 
                                                    WHERE m.mod_job_id = tmp AND m.key = 'uc_zero_day_prediction_segment_limits';
    ELSIF NOT (fit_model OR (calculate_predictors AND (NOT fit_model AND NOT apply_model))) THEN
      RAISE EXCEPTION 'Unable to determine value segment limits, make sure eight weeks of training data is available for zero day prediction (run type is Predictors or Predictors + Fit).';
    END IF;
  END IF;

  -- Check if the tmp module_target table already contains data for this use case:
  IF (SELECT mt.alias_id FROM tmp.module_targets_tmp AS mt 
        WHERE mt.mod_job_id = in_mod_job_id AND use_case_name = 'zero_day_prediction' LIMIT 1) IS NOT NULL THEN
    RAISE EXCEPTION 'Target list has already been calculated for zero_day_prediction use case, mod_job_id %', in_mod_job_id;
  END IF;
  
  
  --Insert use case targets into a temporary table before assigning them value segments.
  --If model fitting is required, the table also contains total topups from first 8 weeks
  --for each subscriber. These 8 weeks are counted starting from next Monday after subscriber
  --joined (i.e., if subscriber joins on Wednesday of week 1, topups are counted from weeks 2-9).
  --Note: The temporary table creation has to be done using dynamic SQL and EXECUTE due to a PostgreSQL bug
  
  IF fit_model OR (calculate_predictors AND NOT apply_model) THEN --In practice: run_type is 'Predictors' or 'Predictors + Fit + Apply'
    --If model is fitted, actual topups from eight weeks are needed
    temp_table_sql := '
      DROP TABLE IF EXISTS temp_uc_zero_day_prediction_targets;
  
      CREATE TEMPORARY TABLE temp_uc_zero_day_prediction_targets WITHOUT OIDS AS
        SELECT --Table ranking new subscribers according to sum of their topups from first 8 weeks
          aaaa.alias_id                                           AS alias_id,
          aaaa.eight_week_topups                                  AS eight_week_topups,
          rank() OVER (ORDER BY aaaa.eight_week_topups ASC)       AS rank_of_subscriber
        FROM ( --Table containing new subscribers during the period and the sum of their topups from first 8 weeks
          SELECT DISTINCT
            aaa.alias_id                                          AS alias_id,
            SUM(coalesce(aaa.topup_cost,0)) OVER (PARTITION BY aaa.alias_id)  AS eight_week_topups
          FROM ( --Table containing new subscribers during the period and all of their topups from first 8 weeks 
            SELECT 
              aa.alias_id                                         AS alias_id,
              bb.topup_cost                                       AS topup_cost
            FROM (SELECT DISTINCT alias_id, MAX(switch_on_date) OVER (PARTITION BY alias_id)   AS switch_on_date FROM data.in_crm ) AS aa  --CRM data for left join
            LEFT JOIN (
              SELECT 
                charged_id                                        AS charged_id, 
                topup_cost                                        AS topup_cost, 
                "timestamp"                                       AS "timestamp"
              FROM data.topup b     --TOPUP data for left join
              WHERE b.timestamp >= ''' || uc_zero_day_prediction_t4::text || '''::date  --Only topups during the period (to speed up join)
            ) AS bb
            ON aa.alias_id = bb.charged_id
            WHERE aa.switch_on_date >= ''' || uc_zero_day_prediction_t4::text || '''::date     --Only new subscribers
            AND   aa.switch_on_date <  ''' || uc_zero_day_prediction_t5::text || '''::date     
            --AND   aa.date_inserted  <= ''' || t9::text || '''::date                            --Only latest CRM data used (could use <= t2 as well)
            AND   bb."timestamp" BETWEEN date_trunc(''week'', aa.switch_on_date)::date + 7 
                                     AND date_trunc(''week'', aa.switch_on_date)::date + 7 * 9 --Topups from first 8 weeks of data included
          ) AS aaa
        ) AS aaaa;
      '; --End text string temp_table_sql
  ELSE
    --If model is only applied, no topup information is needed
    temp_table_sql := '
      DROP TABLE IF EXISTS temp_uc_zero_day_prediction_targets;
    
      CREATE TEMPORARY TABLE temp_uc_zero_day_prediction_targets WITHOUT OIDS AS
        SELECT DISTINCT a.alias_id FROM data.in_crm AS a  --CRM data (Note: CRM entry for subscriber is accepted regardless of CRM file date)
        WHERE a.switch_on_date >= ''' || uc_zero_day_prediction_t4::text || '''::date     --Only subscribers from last week
        AND   a.switch_on_date <  ''' || uc_zero_day_prediction_t5::text || '''::date     
      '; --End text string temp_table_sql

  END IF; --Script for populating temporary table temp_table_sql with new subscribers created

  EXECUTE(temp_table_sql);

  --If necessary, new value segment limits are assigned here based on target group topups
  IF uc_zero_day_prediction_redefine_value_segments OR this_uc_zero_day_prediction_segment_limits IS NULL THEN

    --New value segment limits are calculated from the data and added to mod job parameters table
    tmp := COUNT(*) FROM temp_uc_zero_day_prediction_targets;

    this_uc_zero_day_prediction_segment_limits[1] := MAX(eight_week_topups) 
      FROM temp_uc_zero_day_prediction_targets AS a
      WHERE a.rank_of_subscriber <= ceiling(tmp / 3);

    this_uc_zero_day_prediction_segment_limits[2] := MAX(eight_week_topups) 
      FROM temp_uc_zero_day_prediction_targets AS a
      WHERE a.rank_of_subscriber <= ceiling(2 * tmp / 3);

  END IF;

  --Determine whether segment limit variable already exists for this mod_job_id and update using the proper command
  tmp := COUNT(*) FROM work.module_job_parameters 
           WHERE mod_job_id = in_mod_job_id AND "key" = 'uc_zero_day_prediction_segment_limits';
  
  IF tmp > 0 THEN
    UPDATE work.module_job_parameters 
      SET "value" = array_to_string(this_uc_zero_day_prediction_segment_limits,',')
      WHERE "key" = 'uc_zero_day_prediction_segment_limits'
      AND   mod_job_id = in_mod_job_id;
  ELSE
    INSERT INTO work.module_job_parameters (mod_job_id, "key", "value")
    SELECT 
      in_mod_job_id,
      'uc_zero_day_prediction_segment_limits',
      array_to_string(this_uc_zero_day_prediction_segment_limits,',');
  END IF;
  
  PERFORM core.analyze('work.module_job_parameters', in_mod_job_id);
    
  -- Subscribers are inserted into tmp.module_targets_tmp along with their value segments
  -- Note: Currently 3 value segments are used, adding value segments can be done but 
  --       it will require some modifications also elsewhere in code.
  
  IF fit_model OR (calculate_predictors AND NOT apply_model) THEN --In practice: run_type is 'Predictors' or 'Predictors + Fit + Apply'
  
    INSERT INTO tmp.module_targets_tmp ( mod_job_id, alias_id, use_case_name, target )
      SELECT
        in_mod_job_id         AS mod_job_id,
        a.alias_id              AS alias_id,
        'zero_day_prediction' AS use_case_name,
        CASE WHEN a.eight_week_topups <= this_uc_zero_day_prediction_segment_limits[1] THEN 1
             WHEN a.eight_week_topups >  this_uc_zero_day_prediction_segment_limits[1] 
             AND  a.eight_week_topups <= this_uc_zero_day_prediction_segment_limits[2] THEN 2
             WHEN a.eight_week_topups >  this_uc_zero_day_prediction_segment_limits[2] THEN 3
             ELSE NULL END --target (i.e. subscriber value segment) is 1, 2, 3 or NULL depending on sum of topups in first 8 weeks
          AS target
      FROM temp_uc_zero_day_prediction_targets AS a
      LEFT JOIN (
        SELECT alias_id 
        FROM aliases.blacklist
          WHERE validity = 
            (SELECT max(validity) 
             FROM aliases.blacklist 
             WHERE validity <= t2
          ) 
      ) bl
      ON a.alias_id=bl.alias_id
      WHERE bl.alias_id IS NULL; 

  ELSE --Existing model is applied, segment is not yet known so -1 is inserted as target
    
    INSERT INTO tmp.module_targets_tmp ( mod_job_id, alias_id, use_case_name, target )
      SELECT
        in_mod_job_id         AS mod_job_id,
        a.alias_id              AS alias_id,
        'zero_day_prediction' AS use_case_name,
        -1                    AS target
    FROM temp_uc_zero_day_prediction_targets AS a
      LEFT JOIN (
        SELECT alias_id 
        FROM aliases.blacklist
          WHERE validity = 
            (SELECT max(validity) 
             FROM aliases.blacklist 
             WHERE validity <= t2
          ) 
      ) bl
      ON a.alias_id=bl.alias_id
      WHERE bl.alias_id IS NULL;  

  END IF;

  DROP TABLE IF EXISTS temp_uc_zero_day_prediction_targets;
  
  --Returns number of targets for zero day prediction use case
  RETURN ( SELECT count(mt.*) FROM tmp.module_targets_tmp AS mt WHERE mt.mod_job_id = in_mod_job_id AND use_case_name = 'zero_day_prediction' );

END;

                    </code>
                </input_parameters>
                <output_parameters>
                    <out></out>
                </output_parameters>
            </create_targets_for_zero_day_prediction>
            <create_targets_for_portout>
                <input_parameters>
                    <DataSource1></DataSource1>
                    <queryparameters></queryparameters>
                    <sql>select * from work.create_target_list_portout($mod_job_id)</sql>
                    <code>
                        
CREATE FUNCTION create_target_list_portout(in_mod_job_id integer) RETURNS integer
    AS $$

/* SUMMARY
 * This function determines the set of aliases that will be processed during a
 * certain module job in the portout use case. If possible, the value of target 
 * variable is also calculated. The function writes to tmp.module_targets_tmp. After
 * calling the use case-specific create_target_list functions, calling work.create_target_list
 * combines the target lists into table work.module_targets.
 *
 * VERSION
 * 11.09.2017 LZU MCI portout prediction
 */

DECLARE

  mjp record;  
  query text;
  target_check boolean; 
  results record;

BEGIN

  SELECT DISTINCT
    in_mod_job_id AS mod_job_id,
    max(CASE WHEN d.key        IN ('t2', 'uc_portout_t2') THEN to_date(d.value, 'YYYY-MM-DD') ELSE NULL END) AS t2,
    max(CASE WHEN d.key        = 'uc_portout_include_postpaid' THEN d.value::smallint ELSE NULL END) AS uc_portout_include_postpaid,
    max(CASE WHEN d.key        = 'uc_portout_t4' THEN to_date(d.value, 'YYYY-MM-DD')  ELSE NULL END) AS uc_portout_t4,
    max(CASE WHEN d.key        = 'uc_portout_t5' THEN to_date(d.value, 'YYYY-MM-DD')  ELSE NULL END) AS uc_portout_t5,
    max(CASE WHEN d.key        = 'uc_portout_t6' THEN to_date(d.value, 'YYYY-MM-DD')  ELSE NULL END) AS uc_portout_t6,
    max(CASE WHEN d.key        = 'uc_portout_t7' THEN to_date(d.value, 'YYYY-MM-DD')  ELSE NULL END) AS uc_portout_t7,
    max(CASE WHEN lower(d.key) IN ('tcrm', 'uc_portout_tcrm') THEN to_date(d.value, 'YYYY-MM-DD') ELSE NULL END) AS tcrm,
    max(CASE WHEN d.key        = 'run_type' THEN d.value ELSE NULL END) AS run_type
  INTO mjp
  FROM work.module_job_parameters AS d
  WHERE d.mod_job_id = in_mod_job_id;
  
  query := 'SELECT (SELECT mt.alias_id FROM tmp.module_targets_tmp AS mt WHERE mt.mod_job_id = ' || in_mod_job_id || ' AND use_case_name = ''portout'' LIMIT 1) IS NOT NULL';

  EXECUTE query into target_check;

  -- Check if the tmp module_target table already contains data for this use case:
  IF target_check THEN
    RAISE EXCEPTION 'Target list has already been calculated for portout use_case, mod_job_id %', in_mod_job_id;
  END IF;

  INSERT INTO tmp.module_targets_tmp (mod_job_id, alias_id, use_case_name, target)
  SELECT 
    in_mod_job_id AS mod_job_id,
    a.alias_id,
    'portout' AS use_case_name,
    CASE
      WHEN port.request_date IS NOT NULL AND port.request_date BETWEEN mjp.uc_portout_t4 AND mjp.uc_portout_t5 THEN 1 -- requested port out during t4 - t5
      WHEN port.request_date IS NOT NULL AND port.request_date < mjp.uc_portout_t4 THEN NULL -- exclude requests before t4
      ELSE -1 -- otherwise, no port-out requests made up to t5
    END AS target
  FROM data.in_crm AS a
  LEFT JOIN (
    SELECT alias_id, max(request_date)::date AS request_date
    FROM data.port_out_requests
    WHERE request_date <= mjp.uc_portout_t5 + '1 week'::interval -- assuming non-first-time port-out will not happen within a week
    GROUP BY alias_id
  ) AS port
  ON a.alias_id = port.alias_id
  INNER JOIN (
    SELECT
      alias_id,
      max(CASE WHEN monday < mjp.t2 THEN 1 ELSE 0 END) AS source_period_end_activity -- activity during end of source period
    FROM (
      SELECT alias_id, monday FROM data.call_types_weekly WHERE direction = 'm'
      UNION
      SELECT charged_id AS alias_id, date_trunc('week', timestamp::date + 2)::date - 2 AS monday FROM data.topup
    ) AS cdrtop
    WHERE cdrtop.monday BETWEEN mjp.t2 - 7 AND mjp.t2
    GROUP BY alias_id
  ) d
  ON a.alias_id = d.alias_id
  LEFT JOIN (
    SELECT alias_id
    FROM aliases.blacklist
    WHERE validity = (
      SELECT max(validity)
      FROM aliases.blacklist
      WHERE validity <= mjp.t2
    ) 
  ) bl
  ON a.alias_id = bl.alias_id
  WHERE a.date_inserted = mjp.tcrm
  AND bl.alias_id IS NULL
  AND (a.payment_type = 'prepaid' OR (CASE WHEN mjp.uc_portout_include_postpaid = 1 THEN a.payment_type = 'postpaid' END))
  AND d.source_period_end_activity = 1 -- activity during the end of source period is required
  ;

  SELECT
    count(mt.*) AS total,
    sum(CASE WHEN mt.target = 1 THEN 1 ELSE 0 END) AS pos,
    sum(CASE WHEN mt.target = -1 THEN 1 ELSE 0 END) AS neg
  INTO results
  FROM tmp.module_targets_tmp AS mt 
  WHERE mt.mod_job_id = in_mod_job_id 
  AND use_case_name = 'portout';

  IF (results.total = 0) THEN
     RAISE EXCEPTION 'Please check data for portout use case.';
  END IF;
  
  IF (mjp.run_type ~ 'Fit') THEN
    IF ((results.pos::double precision / results.total::double precision) < 0.00001) THEN -- 0.001% for positive
      RAISE EXCEPTION 'Please check data for portout use case. There is not enough positive targets.';
    END IF;
    IF ((results.neg::double precision / results.total::double precision) < 0.1) THEN -- 10% for negative
      RAISE EXCEPTION 'Please check data for portout use case. There is not enough negative targets.';
    END IF;
  END IF;
  
  RETURN results.total;

END;
                    </code>
                </input_parameters>
                <output_parameters>
                    <out></out>
                </output_parameters>
            </create_targets_for_portout>
            <combine_target_lists>
                <input_parameters>
                    <DataSource1></DataSource1>
                    <queryparameters></queryparameters>
                    <sql>select * from work.combine_target_lists($mod_job_id)</sql>
                    <code>
                        
CREATE FUNCTION combine_target_lists(in_mod_job_id integer) RETURNS integer
    AS $$

/* SUMMARY
 * This function combines the target lists of different use cases of the given 
 * module job. 
 * Reads from tmp.module_targets_tmp (in stack format) and 
 * writes to work.module_targets. 
 *
 * VERSION
 * 30.05.2013 KL  Changed some queries to execute to get function to work when creating apply quality charts
 * 31.01.2013 HMa 
 */

DECLARE
  use_cases text[];
  use_case text;
  uc_model_id integer;
  n_rows integer;
  sql_string text;
  colname_string text := '';
  max_string text := '';
  case_string text := '';
  query text;
  target_check boolean;
  return_value integer;

BEGIN

query='SELECT (SELECT mt.alias_id FROM work.module_targets AS mt WHERE mt.mod_job_id ='|| in_mod_job_id||' LIMIT 1 ) IS NOT NULL';

EXECUTE query INTO target_check;

  IF target_check THEN
    RAISE EXCEPTION 'Target list has already been calculated for module_job_id %', in_mod_job_id;
  END IF;

 query = 'SELECT 
      use_case_name 
    FROM tmp.module_targets_tmp
    WHERE mod_job_id = '||in_mod_job_id||'
    GROUP BY use_case_name;';
  
  FOR use_case IN 
   EXECUTE query
  
  LOOP
    
    colname_string = colname_string || 'target_'||use_case||', '|| 
                                       'audience_'||use_case||', ';

    max_string = max_string || 'max(target_'||use_case||') AS target_'||use_case||', '||
                               'max(audience_'||use_case||') AS audience_'||use_case||', ';

    case_string = case_string || 'CASE WHEN mtt.use_case_name = '''||use_case||''' THEN target ELSE NULL END AS target_'||use_case||', ' ||
                                 'CASE WHEN mtt.use_case_name = '''||use_case||''' THEN 1 ELSE NULL END AS audience_'||use_case||', ';
                                 
  END LOOP;
  
  -- Trim strings:
  colname_string := trim(TRAILING ', ' FROM colname_string);
  max_string     := trim(TRAILING ', ' FROM max_string);
  case_string    := trim(TRAILING ', ' FROM case_string);

  sql_string :=
 'INSERT INTO work.module_targets (
    mod_job_id, 
    alias_id, ' ||
    colname_string || '
  ) 
  SELECT '||
    in_mod_job_id || ' AS mod_job_id, 
    alias_id, ' ||
    max_string || '
  FROM
  (
    SELECT
      mtt.alias_id, ' ||
      case_string || '
    FROM tmp.module_targets_tmp AS mtt
    WHERE mtt.mod_job_id = ' || in_mod_job_id ||'
  ) a
  GROUP BY alias_id;';
  
  EXECUTE sql_string;
  

  query= 'SELECT count(mt.*) FROM work.module_targets AS mt WHERE mt.mod_job_id =' ||in_mod_job_id;

  EXECUTE query into return_value;

  RETURN return_value;


END;

                    </code>
                </input_parameters>
                <output_parameters>
                    <out></out>
                </output_parameters>
            </combine_target_lists>
        </run_targets_MCI>
        <split:calculate_targets>
            <when>parameter with name equal to calculate_predictors and value equal to false</when>
            <tasks>join, split:descriptice_variables</tasks>
            <when>parameter with name equal to calculate_predictors and value equal to true</when>
            <tasks>run_predictors_MCI,join, split:descriptice_variables</tasks>
        </split:calculate_targets>
        <run_predictors_MCI>
            <CalculateCommonPredictrs_MCI_nogeoloc>
                <Disable_LDA_Predictors>
                    <input_parameters>
                    </input_parameters>
                    <output_parameters>
                        <calculate_lda_cell_events_predictors>false</calculate_lda_cell_events_predictors>
                        <run_lda_cell_events>false</run_lda_cell_events>
                        <run_lda>false</run_lda>
                        <calculate_lda_predictors>false</calculate_lda_predictors>
                    </output_parameters>
                </Disable_LDA_Predictors>
                <split>
                    <when></when>
                    <tasks>create_predictors_1, create_predictors_4_made, create_predictors_4_rec, create_predictors_topup_1, create_predictors_topup_2, create_predictors_topup_3, create_predictors_topup_channels, join1, create_predictors_2, create_predictors_3, join2</tasks>
                    <when></when>
                    <tasks>calculate_monthly_arpu, emai;, network_scorer:out_net, email, join1, create_predictors_2, create_predictors_3, join2</tasks>
                    <when></when>
                    <tasks>split:run_lda?, split:run_lda_events?, join2</tasks>
                </split>
                <split:run_lda?>
                    <when>parameter with name equal to run_lda and value equal to true</when>
                    <tasks>RunLDA_MCI, join2</tasks>
                    <when>parameter with name equal to run_lda and value equal to false</when>
                    <tasks>join2</tasks>
                </split:run_lda?>
                <split:run_lda_events?>
                    <when>parameter with name equal to run_lda_cell_events and value equal to true</when>
                    <tasks>RunLDA_cell_events_MCI, join2</tasks>
                    <when>parameter with name equal to run_lda_cell_events and value equal to false</when>
                    <tasks>join2</tasks>
                </split:run_lda_events?>
                <create_predictors_1>
                    <input_parameters>
                        <DataSource1></DataSource1>
                        <queryparameters></queryparameters>
                        <sql>select * from work.create_modelling_data1($mod_job_id)</sql>
                        <code>
                            
CREATE FUNCTION create_modelling_data1(in_mod_job_id integer) RETURNS text
    AS $$

/* SUMMARY
 * This function processes modelling variables that use data.in_split_aggregates as
 * source table. If model_id is available in work.module_job_parameters table, then
 * only the variables that the model use are processed. Otherwise, all variables
 * are processed. Results go to work.modelling_data_matrix_1.
 *
 * INPUT
 * Identifier of module job
 *
 * OUTPUT
 * Query by which this function has inserted data into
 * work.modelling_data_matrix_1 table
 *
 * VERSION
 * 04.06.2014 HMa - ICIF-181 Stack removal
 * 06.03.2013 MOJ - ICIF-112
 * 25.01.2013 HMa - Read model ids of all the use cases
 * 15.01.2013 HMa - ICIF-97
 * 14.06.2010 TSi - Bug 188 fixed
 * 19.04.2010 JMa
 */

DECLARE

  t1_date date := (
    SELECT DISTINCT to_date(mjp.value, 'YYYY-MM-DD')
    FROM work.module_job_parameters AS mjp
    WHERE mjp.mod_job_id = in_mod_job_id AND mjp.key = 't1'
  );
  t2_date date := (
    SELECT DISTINCT to_date(mjp.value, 'YYYY-MM-DD')
    FROM work.module_job_parameters AS mjp
    WHERE mjp.mod_job_id = in_mod_job_id AND mjp.key = 't2'
  );
  override_model_variables_check boolean := (
    SELECT coalesce(lower(trim(max(mjp.value))) = 'true', FALSE)
    FROM work.module_job_parameters AS mjp 
    WHERE mjp.mod_job_id = in_mod_job_id 
    AND mjp.key = 'override_model_variables_check'
  );

  all_var_names text[];
  included_var_names text[];
  model_ids record;  
  t1_week text := core.date_to_yyyyww(t1_date);
  t2_week text := core.date_to_yyyyww(t2_date);
  num_of_weeks double precision := (t2_date - t1_date)::double precision / 7.0;
  aggr_block text := '';
  var_names text := '';
  aggr_new text;
  query text;
  n text;

BEGIN

  -- Find model ids of all the use cases in this module job:
  SELECT 
    mp.value 
  INTO model_ids 
  FROM work.module_job_parameters AS mp
  WHERE mp.mod_job_id = in_mod_job_id 
  AND mp.key LIKE '%model_id';

  all_var_names :=  array_agg(columns)
    FROM  work.get_table_columns_with_prefix('work','modelling_data_matrix_1','','mod_job_id,alias_id'); 


  IF model_ids IS NULL OR override_model_variables_check THEN
    included_var_names := all_var_names;
  ELSE
    query := 
    'SELECT array_agg(DISTINCT mm.value) AS var_name
     FROM work.module_models AS mm
     WHERE mm.value IN ('''||array_to_string(all_var_names, ''',''')||''')
     AND mm.key = ''varname''
     AND mm.model_id IN (
       SELECT 
         mp.value::integer AS model_id
       FROM work.module_job_parameters AS mp
       WHERE mp.mod_job_id = '||in_mod_job_id||'
       AND mp.key LIKE ''%model_id''
       UNION ALL
       SELECT DISTINCT mm3.value::integer
       FROM work.module_models AS mm2
       INNER JOIN work.module_models AS mm3
       ON mm2.model_id = mm3.model_id
       WHERE mm2.key = ''model_type''
       AND mm2.value = ''submodel_list''
       AND mm3.key = ''model_id'' 
       AND mm2.model_id IN (
         SELECT 
           mp.value::integer AS model_id
         FROM work.module_job_parameters AS mp
         WHERE mp.mod_job_id = '||in_mod_job_id||'
         AND mp.key LIKE ''%model_id''
       )
     );';
    EXECUTE query INTO included_var_names;
  END IF;

  n := array_upper(included_var_names, 1);

  IF n = '0' THEN
    RETURN 'WARNING: No variables processed';
  END IF;

  -- Parse some auxiliary texts
  FOR n IN (
    SELECT included_var_names[s] FROM generate_series(1,array_upper(included_var_names, 1)) AS s
  ) LOOP
    aggr_new := (SELECT CASE
      WHEN n IN (
        'voicecount', 'voicesum', 'voicecountweekday', 'voicesumweekday',
        'voicecountday', 'voicesumday', 'voicecountevening', 'voicesumevening',
        'smscount', 'smscountweekday', 'smscountday', 'smscountevening'
      ) THEN 
        'sum(sa.'||n||')::double precision / '||num_of_weeks||' AS '||n
      WHEN n = 'alias_count' THEN
        'sum(sa.mc_'||n||')::double precision / '||num_of_weeks||' AS '||n
      WHEN n = 'mr_ratio' THEN 
        'CASE
           WHEN sum(sa.rc_alias_count) > 0
           THEN avg(sa.mc_alias_count)::double precision / avg(sa.rc_alias_count)
           ELSE NULL
         END AS mr_ratio'
      WHEN n = 'mr_count_ratio' THEN
        'CASE
           WHEN sum(sa.rc_voicecount + sa.rc_smscount) > 0
           THEN avg(sa.voicecount + sa.smscount)::double precision / avg(sa.rc_voicecount + sa.rc_smscount)
           ELSE NULL
         END AS mr_count_ratio'
      WHEN n = 'callsmsratio' THEN
        'CASE
           WHEN sum(sa.smscount) > 0
           THEN avg(sa.voicecount)::double precision / avg(sa.smscount)
           ELSE NULL
         END AS callsmsratio'
      WHEN n = 'week_mean' THEN
        'CASE
           WHEN sum(sa.weight_of_week) > 0
           THEN sum(sa.weight_of_week * sa.period_scaled)::double precision / sum(sa.weight_of_week)
           ELSE NULL
         END AS week_mean'
      WHEN n = 'week_entropy' AND num_of_weeks > 1 THEN
        'CASE
           WHEN sum(sa.weight_of_week) > 0
           THEN (ln(sum(sa.weight_of_week)) - sum(sa.weight_of_week * sa.ln_weight_of_week)::double precision / sum(sa.weight_of_week)) / ln('||num_of_weeks||')
           ELSE NULL
         END AS week_entropy'
      ELSE NULL
    END);
    IF aggr_new IS NOT NULL THEN
      var_names := var_names || n || ', ';
      aggr_block := aggr_block||aggr_new||', ';
    END IF;
  END LOOP;

  -- Fine-tune the auxiliary texts
  var_names := trim(TRAILING ', ' FROM var_names);
  aggr_block := trim(TRAILING ', ' FROM aggr_block);

  -- Parse the query (part I)
  query :=
    'INSERT INTO work.modelling_data_matrix_1
     (mod_job_id, alias_id, '||var_names||')
     SELECT 
       '||in_mod_job_id||' AS mod_job_id,
        sa.alias_id, 
       '||aggr_block||'
     FROM (
       SELECT 
         sa1.*,       
         sa1.voicecount + sa1.smscount AS weight_of_week,
         ln(CASE WHEN sa1.voicecount + sa1.smscount <= 0 THEN 1.0 ELSE sa1.voicecount + sa1.smscount END) AS ln_weight_of_week,
         (core.yyyyww_to_date(sa1.period) - to_date('''||to_char(t1_date, 'YYYY-MM-DD')||''', ''YYYY-MM-DD''))::double precision / '||(t2_date - t1_date - 7)|| ' AS period_scaled
       FROM data.in_split_aggregates AS sa1
       INNER JOIN work.module_targets AS mt
       ON sa1.alias_id = mt.alias_id
       WHERE mt.mod_job_id = '||in_mod_job_id||'
       AND sa1.period >= '||t1_week||' 
       AND sa1.period < '||t2_week||'
     ) AS sa
     GROUP BY sa.alias_id';


  -- Execute and return the query, do some maintenance in between
  EXECUTE query;
  PERFORM core.analyze('work.modelling_data_matrix_1', in_mod_job_id);
  RETURN query;

END;

                        </code>
                    </input_parameters>
                    <output_parameters>
                        <out></out>
                    </output_parameters>
                </create_predictors_1>
                <create_predictors_4_made>
                    <input_parameters>
                        <DataSource1></DataSource1>
                        <queryparameters></queryparameters>
                        <sql>select * from work.create_modelling_data4_made($mod_job_id, $lag_length, $lag_count)</sql>
                        <code>

CREATE FUNCTION create_modelling_data4_made(in_mod_job_id integer, in_lag_length integer, in_lag_count integer) RETURNS void
    AS $$

/* SUMMARY
 * This function calculates time-lagged aggregates for different made call
 * types. data.call_types_weekly is used as source table. Results go to
 * work.modelling_data_matrix_4_made.
 *
 * INPUT
 * in_mod_job_id : Identifier of module job
 * in_lag_length : Length of one lag unit in days
 * in_lag_count  : Number of lag units
 *
 * VERSION
 * 2014-06-04 HMa - ICIF-181 Stack removal (modified from work.create_modelling_data4)
 * 2013-01-07 JVi - Commented out low-utility predictors using --070113 to conserve
 *                  computing power. Find and Replace '--070113' to '' to take 
 *                  these predictors into use. (e.g. if data includes lots of mms and video).
 *                  Also need to remove similar comments from work.modelling_data_matrix
 * 2012-12-21 JVi - Added new predictors based on HMa's x81 specific version
 * 2011-09-16 TSi - Bug 230 fixed
 * 2011-09-13 TSi - Bug 393 fixed
 */

DECLARE

  t2 date := (
    SELECT DISTINCT to_date(mjp.value, 'YYYY-MM-DD')
    FROM work.module_job_parameters AS mjp
    WHERE mjp.mod_job_id = in_mod_job_id AND mjp.key = 't2'
  );
  lag1_date date := (t2 - (in_lag_count * in_lag_length || ' days')::interval)::date;
  lag2_date date := t2;
  call_type_map record;
  call_direction_made character(1) := 'm';
  case_text text;
  var_names text;
  query text;
  
BEGIN

  -- Change this if the identifiers of call types differ from the default values
  SELECT 1 AS voice, 2 AS sms, 3 AS video, 4 AS mms, 5 AS "data" INTO call_type_map;

/* -------------- Start processing made calls. -------------- */
  
  SELECT
    array_to_string(array_agg(case_t),' '),
    array_to_string(array_agg(var_n),', ')
  INTO case_text, var_names
  FROM (
    SELECT
      'max(CASE WHEN lag_id = ' || lag_id || ' THEN ' || base_name || ' END) AS ' || b.base_name || c.lag_id || ', ' AS case_t,
       b.base_name || c.lag_id AS var_n
    FROM (
      SELECT 'daily_voice_activity'       AS base_name UNION ALL
      SELECT 'daily_sms_activity'         AS base_name UNION ALL
      SELECT 'daily_data_activity'        AS base_name UNION ALL
      SELECT 'weekly_data_usage'          AS base_name UNION ALL 
      SELECT 'weekly_voice_neigh_count'   AS base_name UNION ALL -- New variable 2012-12-21 JVi
      SELECT 'weekly_voice_count'         AS base_name UNION ALL -- New variable 2012-12-21 JVi
      SELECT 'weekly_sms_count'           AS base_name UNION ALL -- New variable 2012-12-21 JVi
      SELECT 'weekly_data_count'          AS base_name UNION ALL -- New variable 2012-12-21 JVi
      SELECT 'weekly_voice_duration'      AS base_name UNION ALL -- New variable 2012-12-21 JVi
      SELECT 'weekly_voice_cost'          AS base_name UNION ALL -- New variable 2012-12-21 JVi
      SELECT 'weekly_sms_cost'            AS base_name UNION ALL -- New variable 2012-12-21 JVi
      SELECT 'weekly_cost'                AS base_name UNION ALL -- New variable 2012-12-21 JVi
      SELECT 'weekly_cell_id_count'       AS base_name           -- New variable 2012-12-21 JVi
    ) AS b
    CROSS JOIN (
      SELECT generate_series(1, in_lag_count) AS lag_id
    ) AS c
    UNION
    SELECT
      'min(inact) AS inact, ' AS case_t,
       'inact' AS var_n
  ) AS d;

  case_text := trim(TRAILING ', ' FROM case_text);

  query := '
  INSERT INTO work.modelling_data_matrix_4_made
  (mod_job_id, alias_id, ' || var_names || ')
  SELECT
    ' || in_mod_job_id || ' AS mod_job_id,
    vals.alias_id,
    ' || case_text || '
  FROM (
    SELECT
      d.alias_id,
      d.lag_id,
      (sum(d.voice_daycount)        OVER (PARTITION BY d.alias_id ORDER BY d.lag_id ASC)) / (' || in_lag_length || ' * d.lag_id)::double precision       AS daily_voice_activity, -- proportion of days when has had voice activity
      (sum(d.sms_daycount)          OVER (PARTITION BY d.alias_id ORDER BY d.lag_id ASC)) / (' || in_lag_length || ' * d.lag_id)::double precision       AS daily_sms_activity,
      (sum(d.data_daycount)         OVER (PARTITION BY d.alias_id ORDER BY d.lag_id ASC)) / (' || in_lag_length || ' * d.lag_id)::double precision       AS daily_data_activity,
      (sum(d.sum_data_usage)        OVER (PARTITION BY d.alias_id ORDER BY d.lag_id ASC)) / (' || in_lag_length || ' * d.lag_id)::double precision * 7.0 AS weekly_data_usage, -- average weekly data amount (data)
      (sum(d.sum_voice_neigh_count) OVER (PARTITION BY d.alias_id ORDER BY d.lag_id ASC)) / (' || in_lag_length || ' * d.lag_id)::double precision * 7.0 AS weekly_voice_neigh_count, -- average number of alias_b weekly (voice)
      (sum(d.sum_voice_count)       OVER (PARTITION BY d.alias_id ORDER BY d.lag_id ASC)) / (' || in_lag_length || ' * d.lag_id)::double precision * 7.0 AS weekly_voice_count, -- average number of transactios weekly 
      (sum(d.sum_sms_count)         OVER (PARTITION BY d.alias_id ORDER BY d.lag_id ASC)) / (' || in_lag_length || ' * d.lag_id)::double precision * 7.0 AS weekly_sms_count,
      (sum(d.sum_data_count)        OVER (PARTITION BY d.alias_id ORDER BY d.lag_id ASC)) / (' || in_lag_length || ' * d.lag_id)::double precision * 7.0 AS weekly_data_count,
      (sum(d.voice_duration)        OVER (PARTITION BY d.alias_id ORDER BY d.lag_id ASC)) / (' || in_lag_length || ' * d.lag_id)::double precision * 7.0 AS weekly_voice_duration, -- average voice duration daily
      (sum(d.voice_cost)            OVER (PARTITION BY d.alias_id ORDER BY d.lag_id ASC)) / (' || in_lag_length || ' * d.lag_id)::double precision * 7.0 AS weekly_voice_cost, -- average cost weekly (voice)
      (sum(d.sms_cost)              OVER (PARTITION BY d.alias_id ORDER BY d.lag_id ASC)) / (' || in_lag_length||' * d.lag_id)::double precision * 7.0 AS weekly_sms_cost,
      (sum(d.sum_cost)              OVER (PARTITION BY d.alias_id ORDER BY d.lag_id ASC)) / (' || in_lag_length || ' * d.lag_id)::double precision * 7.0 AS weekly_cost, 
      (sum(d.sum_cell_id_count)     OVER (PARTITION BY d.alias_id ORDER BY d.lag_id ASC)) / (' || in_lag_length || ' * d.lag_id)::double precision * 7.0 AS weekly_cell_id_count, -- average weekly cell id count
      (extract(''days'' FROM inact) + extract(''hours'' FROM inact) / 24.0 + extract(''minutes'' FROM inact) / 1440.0 + extract(''seconds'' FROM inact) / 86400.0) AS inact
    FROM (
      SELECT
        mt.alias_id,
        lags.lag_id,
        sum(CASE WHEN tt.call_type = ' || call_type_map.voice || ' THEN tt.day_count         ELSE NULL END) AS voice_daycount,        -- activity days proportion
        sum(CASE WHEN tt.call_type = ' || call_type_map.sms   || ' THEN tt.day_count         ELSE NULL END) AS sms_daycount,
        sum(CASE WHEN tt.call_type = ' || call_type_map.data  || ' THEN tt.day_count         ELSE NULL END) AS data_daycount,
        sum(CASE WHEN tt.call_type = ' || call_type_map.data  || ' THEN tt.sum_call_duration ELSE NULL END) AS sum_data_usage,        -- data usage (data)
        sum(CASE WHEN tt.call_type = ' || call_type_map.voice || ' THEN tt.neigh_count       ELSE NULL END) AS sum_voice_neigh_count, -- neighbor count 
        sum(CASE WHEN tt.call_type = ' || call_type_map.voice || ' THEN tt.row_count         ELSE NULL END) AS sum_voice_count,       -- transaction count (voice)
        sum(CASE WHEN tt.call_type = ' || call_type_map.sms   || ' THEN tt.row_count         ELSE NULL END) AS sum_sms_count, 
        sum(CASE WHEN tt.call_type = ' || call_type_map.data  || ' THEN tt.row_count         ELSE NULL END) AS sum_data_count, 
        sum(CASE WHEN tt.call_type = ' || call_type_map.voice || ' THEN tt.sum_call_duration ELSE NULL END) AS voice_duration,        -- duration (voice only)
        sum(CASE WHEN tt.call_type = ' || call_type_map.voice || ' THEN tt.sum_call_cost     ELSE NULL END) AS voice_cost,            -- cost (voice)
        sum(CASE WHEN tt.call_type = ' || call_type_map.sms   || ' THEN tt.sum_call_cost     ELSE NULL END) AS sms_cost, 
        sum(tt.sum_call_cost)                                                                               AS sum_cost,              -- cost (all calltypes)
        sum(tt.cell_id_count)                                                                               AS sum_cell_id_count,     -- cell_id count, all calltypes
        justify_hours(''' || t2 || '''::timestamp without time zone - max(tt.max_call_time))                AS inact
      FROM work.module_targets AS mt     
      CROSS JOIN (
        SELECT generate_series(1, ' || in_lag_count || ') AS lag_id
      ) AS lags
      LEFT JOIN (
        SELECT
          ctw.alias_id,
          ceiling((''' || t2 || '''::date - ctw.monday)::double precision / ' || in_lag_length || ')::integer AS lag_id,
          ctw.call_type,
          ctw."row_count", 
          ctw.day_count,
          ctw.neigh_count,
          ctw.sum_call_duration, 
          ctw.sum_call_cost,
          ctw.cell_id_count,
          ctw.max_call_time
        FROM data.call_types_weekly AS ctw
        WHERE (
          ctw.call_type = ' || call_type_map.voice || ' OR
          ctw.call_type = ' || call_type_map.sms   || ' OR
          ctw.call_type = ' || call_type_map.video || ' OR
          ctw.call_type = ' || call_type_map.mms   || ' OR
          ctw.call_type = ' || call_type_map.data  || '
        )
        AND ctw.monday < '''   || lag2_date || '''::date
        AND ctw.monday >= '''  || lag1_date || '''::date
        AND ctw.direction = '''|| call_direction_made || '''
      ) AS tt
      ON mt.alias_id = tt.alias_id
      AND lags.lag_id = tt.lag_id
      WHERE mt.mod_job_id = ' || in_mod_job_id || '
      GROUP BY mt.alias_id, lags.lag_id
    ) AS d   
  ) AS vals
  GROUP BY vals.alias_id;';

  EXECUTE query;

  PERFORM core.analyze('work.modelling_data_matrix_4_made', in_mod_job_id);

END;

                        </code>
                    </input_parameters>
                    <output_parameters>
                        <out></out>
                    </output_parameters>
                </create_predictors_4_made>
                <create_predictors_4_rec>
                    <input_parameters>
                        <DataSource1></DataSource1>
                        <queryparameters></queryparameters>
                        <sql>select * from work.create_modelling_data4_rec($mod_job_id, $lag_length, $lag_count)</sql>
                        <code>
                            
CREATE FUNCTION create_modelling_data4_rec(in_mod_job_id integer, in_lag_length integer, in_lag_count integer) RETURNS void
    AS $$

/* SUMMARY
 * This function calculates time-lagged aggregates for different received call
 * types. data.call_types_weekly is used as source table. Results go to
 * work.modelling_data_matrix_4_rec.
 *
 * INPUT
 * in_mod_job_id : Identifier of module job
 * in_lag_length : Length of one lag unit in days
 * in_lag_count  : Number of lag units
 *
 * VERSION
 * 2014-06-05 HMa - ICIF-181 Stack removal (modified from work.create_modelling_data4)
 * 2013-01-07 JVi - Commented out low-utility predictors using --070113 to conserve
 *                  computing power. Find and Replace '--070113' to '' to take 
 *                  these predictors into use. (e.g. if data includes lots of mms and video).
 *                  Also need to remove similar comments from work.modelling_data_matrix
 * 2012-12-21 JVi - Added new predictors based on HMa's x81 specific version
 * 2011-09-16 TSi - Bug 230 fixed
 * 2011-09-13 TSi - Bug 393 fixed
 */

DECLARE

  t2 date := (
    SELECT DISTINCT to_date(mjp.value, 'YYYY-MM-DD')
    FROM work.module_job_parameters AS mjp
    WHERE mjp.mod_job_id = in_mod_job_id AND mjp.key = 't2'
  );
  lag1_date date := (t2 - (in_lag_count * in_lag_length || ' days')::interval)::date;
  lag2_date date := t2;
  call_type_map record;
  call_direction_received character(1) := 'r';
  case_text text;
  var_names text;
  query text;
  
BEGIN

  -- Change this if the identifiers of call types differ from the default values
  SELECT 1 AS voice, 2 AS sms, 3 AS video, 4 AS mms, 5 AS "data" INTO call_type_map;


  SELECT
    array_to_string(array_agg(case_t),' '),
    array_to_string(array_agg(var_n),', ')
  INTO case_text, var_names
  FROM (
    SELECT
      'MAX(CASE WHEN lag_id = ' || lag_id || ' THEN ' || base_name || ' END) AS ' || b.base_name || c.lag_id || ', ' AS case_t,
       b.base_name || c.lag_id AS var_n
    FROM (
      SELECT 'daily_voice_activity_rec'    AS base_name UNION ALL
      SELECT 'daily_sms_activity_rec'      AS base_name UNION ALL
      SELECT 'weekly_voice_count_rec'      AS base_name UNION ALL
      SELECT 'weekly_sms_count_rec'        AS base_name UNION ALL 
      SELECT 'weekly_voice_duration_rec'   AS base_name UNION ALL 
      SELECT 'weekly_cell_id_count_rec'    AS base_name
    ) AS b
    CROSS JOIN (
      SELECT generate_series(1, in_lag_count) AS lag_id
    ) AS c
    UNION
    SELECT
      'MIN(inact_rec) AS inact_rec, ' AS case_t,
       'inact_rec' AS var_n
  ) AS d;

  case_text := trim(TRAILING ', ' FROM case_text);



  query := '
  INSERT INTO work.modelling_data_matrix_4_rec
  (mod_job_id, alias_id, '||var_names||')
  SELECT
    '||in_mod_job_id||' AS mod_job_id,
    vals.alias_id,
    '||case_text||'
  FROM (
    SELECT
      d.alias_id,
      d.lag_id,
      (sum(d.voice_daycount)        OVER (PARTITION BY d.alias_id ORDER BY d.lag_id ASC)) / ('||in_lag_length||' * d.lag_id)::double precision       AS daily_voice_activity_rec, -- proportion of days when has had voice activity
      (sum(d.sms_daycount)          OVER (PARTITION BY d.alias_id ORDER BY d.lag_id ASC)) / ('||in_lag_length||' * d.lag_id)::double precision       AS daily_sms_activity_rec,
      (sum(d.sum_voice_count)       OVER (PARTITION BY d.alias_id ORDER BY d.lag_id ASC)) / ('||in_lag_length||' * d.lag_id)::double precision * 7.0 AS weekly_voice_count_rec, -- average number of transactios weekly 
      (sum(d.sum_sms_count)         OVER (PARTITION BY d.alias_id ORDER BY d.lag_id ASC)) / ('||in_lag_length||' * d.lag_id)::double precision * 7.0 AS weekly_sms_count_rec,
      (sum(d.voice_duration)        OVER (PARTITION BY d.alias_id ORDER BY d.lag_id ASC)) / ('||in_lag_length||' * d.lag_id)::double precision * 7.0 AS weekly_voice_duration_rec, -- average voice duration daily
      (sum(d.sum_cell_id_count)     OVER (PARTITION BY d.alias_id ORDER BY d.lag_id ASC)) / ('||in_lag_length||' * d.lag_id)::double precision * 7.0 AS weekly_cell_id_count_rec,  -- average weekly cell id count
      (extract(''days'' FROM inact_rec) + extract(''hours'' FROM inact_rec) / 24.0 + extract(''minutes'' FROM inact_rec) / 1440.0 + extract(''seconds'' FROM inact_rec) / 86400.0 ) AS inact_rec
     FROM (
      SELECT
        mt.alias_id,
        lags.lag_id,
        sum(CASE WHEN tt.call_type = '||call_type_map.voice||'  THEN tt.day_count           ELSE NULL END) AS voice_daycount, -- activity days proportion
        sum(CASE WHEN tt.call_type = '||call_type_map.sms||'    THEN tt.day_count           ELSE NULL END) AS sms_daycount,
        sum(CASE WHEN tt.call_type = '||call_type_map.voice||'  THEN tt.row_count           ELSE NULL END) AS sum_voice_count, -- transaction count (voice)
        sum(CASE WHEN tt.call_type = '||call_type_map.sms||'    THEN tt.row_count           ELSE NULL END) AS sum_sms_count, 
        sum(CASE WHEN tt.call_type = '||call_type_map.voice||'  THEN tt.sum_call_duration   ELSE NULL END) AS voice_duration, -- duration (voice only)
        sum(tt.cell_id_count)                                                                              AS sum_cell_id_count, -- cell_id count, all calltypes
        justify_hours('''||t2||'''::timestamp without time zone - max(tt.max_call_time))                   AS inact_rec
      FROM work.module_targets AS mt     
      CROSS JOIN (
        SELECT generate_series(1, '||in_lag_count||') AS lag_id
      ) AS lags
      LEFT JOIN (
        SELECT
          ctw.alias_id,
          ceiling(('''||t2||'''::date - ctw.monday)::double precision / '||in_lag_length||')::integer AS lag_id,
          ctw.call_type,
          ctw."row_count", 
          ctw.day_count,
          ctw.neigh_count,
          ctw.sum_call_duration, 
          ctw.cell_id_count,
          ctw.max_call_time
        FROM data.call_types_weekly AS ctw
        WHERE (
          ctw.call_type = '||call_type_map.voice||' OR
          ctw.call_type = '||call_type_map.sms||' 
        )
        AND ctw.monday < '''||lag2_date||'''::date
        AND ctw.monday >= '''||lag1_date||'''::date
        AND ctw.direction = '''||call_direction_received||'''
      ) AS tt
      ON mt.alias_id = tt.alias_id
      AND lags.lag_id = tt.lag_id
      WHERE mt.mod_job_id = '||in_mod_job_id||'
      GROUP BY mt.alias_id, lags.lag_id
    ) AS d   
  ) AS vals
  GROUP BY vals.alias_id;';

  EXECUTE query;

  PERFORM core.analyze('work.modelling_data_matrix_4_rec', in_mod_job_id);

END;

                        </code>
                    </input_parameters>
                    <output_parameters>
                        <out></out>
                    </output_parameters>
                </create_predictors_4_rec>
                <create_predictors_topup_1>
                    <input_parameters>
                        <DataSource1></DataSource1>
                        <queryparameters></queryparameters>
                        <sql>select * from work.create_modelling_data_topup1($mod_job_id, $lag_length, $lag_count)</sql>
                        <code>
                            
CREATE FUNCTION create_modelling_data_topup1(in_mod_job_id integer, in_lag_length integer, in_lag_count integer) RETURNS void
    AS $$

/* SUMMARY
 * This function calculates time-lagged aggregates from data.toupup table. 
 * Results go to work.modelling_data_matrix_topup1. 
 * 
 * INPUT
 * in_mod_job_id : Identifier of module job
 * in_lag_length : Length of one lag unit in days
 * in_lag_count  : Number of lag units
 *
 * VERSION
 * 2014-06-05 HMa - ICIF-181 Stack removal (modified from work.create_modelling_data5 and work.create_modelling_data6)
 * 2012-05-14 MOj - Bug 888 fixed
 * 2011-09-15 TSi - Bug 394 and 528 fixed
 */
DECLARE

  t2 date := (
    SELECT DISTINCT to_date(mjp.value, 'YYYY-MM-DD')
    FROM work.module_job_parameters AS mjp
    WHERE mjp.mod_job_id = in_mod_job_id AND mjp.key = 't2'
  );
  lag1_time timestamp without time zone := (t2 - (in_lag_count * in_lag_length || ' days')::interval)::timestamp without time zone;
  lag2_time timestamp without time zone := t2::timestamp without time zone;
  is_credit_amount boolean := (
    SELECT count(d.*) > 0
    FROM (
      SELECT *
      FROM data.topup AS t
      WHERE NOT t.is_credit_transfer
      AND t.timestamp < lag2_time
      AND t.timestamp >= lag1_time
      AND coalesce(t.credit_amount, 0) > 0
      LIMIT 1
    ) AS d
  );
  is_topup_cost boolean := (
    SELECT count(d.*) > 0
    FROM (
      SELECT *
      FROM data.topup AS t
      WHERE NOT t.is_credit_transfer
      AND t.timestamp < lag2_time
      AND t.timestamp >= lag1_time
      AND coalesce(t.topup_cost, 0) > 0
      LIMIT 1
    ) AS d
  );
  case_text text;
  var_names text;
  query text;

BEGIN

  IF NOT (is_credit_amount OR is_topup_cost) THEN
    RETURN;
  END IF;


  SELECT
    array_to_string(array_agg(case_t),' '),
    array_to_string(array_agg(var_n),', ')
  INTO case_text, var_names
  FROM (
    SELECT
      'MAX(CASE WHEN lag_id = ' || lag_id || ' THEN ' || base_name || ' END) AS ' || b.base_name || c.lag_id || ', ' AS case_t,
       b.base_name || c.lag_id AS var_n
    FROM (
      SELECT is_credit_amount                   AS is_available, 'topup_amount_avg'           AS base_name UNION ALL
      SELECT is_credit_amount AND is_topup_cost AS is_available, 'topup_bonus_avg'            AS base_name UNION ALL
      SELECT is_credit_amount AND is_topup_cost AS is_available, 'topup_bonus_per_amount_avg' AS base_name UNION ALL
      SELECT is_credit_amount OR  is_topup_cost AS is_available, 'topup_count_weekly'         AS base_name UNION ALL
      SELECT is_credit_amount AND is_topup_cost AS is_available, 'topup_free_avg'             AS base_name UNION ALL
      SELECT is_credit_amount AND is_topup_cost AS is_available, 'topup_free_count_weekly'    AS base_nam
    ) AS b
    CROSS JOIN (
      SELECT generate_series(1, in_lag_count) AS lag_id
    ) AS c
    WHERE b.is_available
  ) AS d;

  case_text := trim(TRAILING ', ' FROM case_text);


  query := '
  INSERT INTO work.modelling_data_matrix_topup1
  (mod_job_id, alias_id, '||var_names||')
  SELECT
    '||in_mod_job_id||' AS mod_job_id,
    vals.alias_id,
    '||case_text||'
  FROM (
    SELECT
      d.alias_id,
      d.lag_id,
      sum(d.sum_amount)                    OVER (PARTITION BY d.alias_id ORDER BY d.lag_id ASC) / sum(d.sum_count)          OVER (PARTITION BY d.alias_id ORDER BY d.lag_id ASC) AS topup_amount_avg,
      sum(d.sum_bonus_if_bonus)            OVER (PARTITION BY d.alias_id ORDER BY d.lag_id ASC) / sum(d.sum_count_if_bonus) OVER (PARTITION BY d.alias_id ORDER BY d.lag_id ASC) AS topup_bonus_avg,
      sum(d.sum_bonus_per_amount_if_bonus) OVER (PARTITION BY d.alias_id ORDER BY d.lag_id ASC) / sum(d.sum_count_if_bonus) OVER (PARTITION BY d.alias_id ORDER BY d.lag_id ASC) AS topup_bonus_per_amount_avg,
      sum(d.sum_count)                     OVER (PARTITION BY d.alias_id ORDER BY d.lag_id ASC) / ('||in_lag_length||' * d.lag_id) * 7.0                                         AS topup_count_weekly,
      sum(d.sum_free_topup)                OVER (PARTITION BY d.alias_id ORDER BY d.lag_id ASC) / sum(d.count_free_topup)   OVER (PARTITION BY d.alias_id ORDER BY d.lag_id ASC) AS topup_free_avg,
      sum(d.count_free_topup)              OVER (PARTITION BY d.alias_id ORDER BY d.lag_id ASC) / ('||in_lag_length||' * d.lag_id) * 7.0                                         AS topup_free_count_weekly
    FROM (
      SELECT
        mt.alias_id,
        lags.lag_id,
        sum(tt.amount) AS sum_amount,
        sum(tt.count)  AS sum_count,
        sum(CASE WHEN tt.amount > tt.cost THEN tt.amount - tt.cost ELSE NULL END)               AS sum_bonus_if_bonus,
        sum(CASE WHEN tt.amount > tt.cost THEN (tt.amount - tt.cost) / tt.amount ELSE NULL END) AS sum_bonus_per_amount_if_bonus,
        sum(CASE WHEN tt.amount > tt.cost THEN tt.count ELSE NULL END)                          AS sum_count_if_bonus,
        sum(CASE WHEN tt.cost = 0 AND tt.amount > 0 THEN tt.amount ELSE NULL END)               AS sum_free_topup,
        sum(CASE WHEN tt.cost = 0 AND tt.amount > 0 THEN tt.count ELSE NULL END)                AS count_free_topup
      FROM work.module_targets AS mt      
      CROSS JOIN (
        SELECT generate_series(1, '||in_lag_count||') AS lag_id
      ) AS lags
      LEFT JOIN (
        SELECT
          t.charged_id AS alias_id,
          ceiling(('''||t2||'''::date - t.timestamp::date)::double precision / '||in_lag_length||'::double precision)::integer AS lag_id,
          greatest(t.credit_amount, 0)::double precision AS amount,
          greatest(t.topup_cost, 0)::double precision AS cost,
          1::double precision AS "count" -- Use greatest(t.count, 0)::integer if several topups can be reported on the same line
        FROM data.topup AS t
        WHERE NOT t.is_credit_transfer
        AND t.timestamp < '''||lag2_time||'''::date
        AND t.timestamp >= '''||lag1_time||'''::date
      ) AS tt
      ON mt.alias_id = tt.alias_id
      AND lags.lag_id = tt.lag_id
      WHERE mt.mod_job_id = '||in_mod_job_id||'
      GROUP BY mt.alias_id, lags.lag_id
    ) AS d    
  ) AS vals
  GROUP BY vals.alias_id;';

  EXECUTE query;

  PERFORM core.analyze('work.modelling_data_matrix_topup1', in_mod_job_id);

END;

                        </code>
                    </input_parameters>
                    <output_parameters>
                        <out></out>
                    </output_parameters>
                </create_predictors_topup_1>
                <create_predictors_topup_2>
                    <input_parameters>
                        <DataSource1></DataSource1>
                        <queryparameters></queryparameters>
                        <sql>select * from work.create_modelling_data_topup2($mod_job_id)</sql>
                        <code>
                            
CREATE FUNCTION create_modelling_data_topup2(in_mod_job_id integer) RETURNS void
    AS $$

/* SUMMARY
 * This function calculates predictors from data.toupup table. 
 * Results go to work.create_modelling_data_topup2. 
 * 
 * INPUT
 * in_mod_job_id : Identifier of module job
 *
 * VERSION
 * 2014-06-05 HMa - ICIF-181 Stack removal (modified from work.create_modelling_data5)
 * 2012-05-14 MOj - Bug 888 fixed
 * 2011-09-15 TSi - Bug 394 and 528 fixed
 */
DECLARE

  t1 date := (
    SELECT DISTINCT to_date(mjp.value, 'YYYY-MM-DD')
    FROM work.module_job_parameters AS mjp
    WHERE mjp.mod_job_id = in_mod_job_id AND mjp.key = 't1'
  );
  t2 date := (
    SELECT DISTINCT to_date(mjp.value, 'YYYY-MM-DD')
    FROM work.module_job_parameters AS mjp
    WHERE mjp.mod_job_id = in_mod_job_id AND mjp.key = 't2'
  );
  tcrm date := (
    SELECT DISTINCT to_date(mjp.value, 'YYYY-MM-DD')
    FROM work.module_job_parameters AS mjp
    WHERE mjp.mod_job_id = in_mod_job_id AND lower(mjp.key) = 'tcrm'
  );

BEGIN

  INSERT INTO work.modelling_data_matrix_topup2
  (mod_job_id, alias_id, topup_days_from_last, topup_days_to_next1, topup_days_to_next2, topup_days_interval)
  SELECT
    in_mod_job_id AS mod_job_id,
    z.alias_id,
    z.time_from_last1 AS topup_days_from_last,
    CASE
      WHEN z.sum_credit_amount > 0
      THEN z.credit_amount1 * (t2 - greatest(c.switch_on_date, t1)) / z.sum_credit_amount - z.time_from_last1
      ELSE NULL
    END AS topup_days_to_next1, -- This assumes that credit balance was zero at the moment of the last topup
    CASE
      WHEN z.sum_credit_amount > 0
      THEN z.credit_amount2 * (t2 - greatest(c.switch_on_date, t1)) / z.sum_credit_amount - z.time_from_last2
      ELSE NULL
    END AS topup_days_to_next2, -- This assumes that credit balance was zero at the moment of the second last topup
    extract('days' FROM z.avg_interval) 
    + extract('hours' FROM z.avg_interval) / 24.0
    + extract('minutes' FROM z.avg_interval) / 1440.0 
    + extract('seconds' FROM z.avg_interval) / 86400.0 AS topup_days_interval
  FROM (
    SELECT
      d.alias_id,
      sum(CASE WHEN d.row_id < 2 THEN d.credit_amount ELSE 0 END) AS credit_amount1,
      sum(CASE WHEN d.row_id < 3 THEN d.credit_amount ELSE 0 END) AS credit_amount2,
      t2 - max(d.timestamp)::date AS time_from_last1,
      t2 - max(CASE WHEN d.row_id > 1 THEN d.timestamp ELSE NULL END)::date AS time_from_last2,
      sum(d.credit_amount) AS sum_credit_amount,
      justify_hours(avg(d.interval)) AS avg_interval
    FROM (
      SELECT
        mt.alias_id,
        row_number() OVER (PARTITION BY mt.alias_id ORDER BY t.timestamp DESC) AS row_id,
        t.timestamp,
        greatest(t.credit_amount, 0.0) AS credit_amount,
        t.timestamp - lag(t.timestamp, 1, NULL) OVER (PARTITION BY mt.alias_id ORDER BY t.timestamp ASC) AS "interval"
      FROM work.module_targets AS mt
      INNER JOIN data.topup AS t
      ON t.charged_id = mt.alias_id
      WHERE NOT t.is_credit_transfer
      AND t.timestamp < t2
      AND t.timestamp >= t1
      AND mt.mod_job_id = in_mod_job_id
    ) AS d
    GROUP BY d.alias_id
  ) AS z
  LEFT JOIN data.in_crm AS c
  ON z.alias_id = c.alias_id
  WHERE c.date_inserted = tcrm;

  PERFORM core.analyze('work.modelling_data_matrix_topup2', in_mod_job_id);

END;

                        </code>
                    </input_parameters>
                    <output_parameters>
                        <out></out>
                    </output_parameters>
                </create_predictors_topup_2>
                <create_predictors_topup_3>
                    <input_parameters>
                        <DataSource1></DataSource1>
                        <queryparameters></queryparameters>
                        <sql>select * from work.create_modelling_data_topup3($mod_job_id, $lag_length, $lag_count)</sql>
                        <code>
                            
CREATE FUNCTION create_modelling_data_topup3(in_mod_job_id integer, in_lag_length integer, in_lag_count integer) RETURNS void
    AS $$
/* SUMMARY
 * This function calculates predictors from topup behavior during different times of day.
 * Calculation of predictors combining topup and CDR data has been disabled until implemented differently. 
 * Results go to work.modelling_data_matrix_topup3. 
 * 
 * INPUT
 * in_mod_job_id : Identifier of module job
 *
 * VERSION
 * 2014-06-05 HMa - ICIF-181 Stack removal (modified from work.create_modelling_data6)
 * 2012-11-30 JVi - Created from HMa's x81 function, added new topup predictors 
 *                  and removed those shared with create_modelling_data5
 * 2012-09-04 HMa - x81 specific version [of create_modelling_data5 -JVi]
 * 2012-05-14 MOj - Bug 888 fixed
 * 2011-09-15 TSi - Bug 394 and 528 fixed
 */
DECLARE

  t2 date := (
    SELECT DISTINCT to_date(mjp.value, 'YYYY-MM-DD')
    FROM work.module_job_parameters AS mjp
    WHERE mjp.mod_job_id = in_mod_job_id AND mjp.key = 't2'
  );
  lag1_time timestamp without time zone := (t2 - (in_lag_count * in_lag_length || ' days')::interval)::timestamp without time zone;
  lag2_time timestamp without time zone := t2::timestamp without time zone;
  case_text text;
  var_names text;
  query text;

BEGIN

  
  /* Temporary table topup_aliases_and_timestamps_temp is used for calculating 
   * some of the topup variables. It contains the alias ids and topup timestamps
   * as well as the lag period id corresponding to each topup timestamp.
   */
  CREATE TEMPORARY TABLE topup_aliases_and_timestamps_temp WITHOUT OIDS AS
  SELECT 
    mt.alias_id AS alias_id,
    lags.lag_id AS lag_id,
    tt.topup_timestamp AS topup_timestamp
  FROM
    work.module_targets AS mt
  CROSS JOIN (
    SELECT generate_series(1, in_lag_count) AS lag_id
  ) AS lags
  INNER JOIN (
    SELECT
      t.receiving_id,
      ceiling((t2 - t.timestamp::date)::double precision / in_lag_length::double precision)::integer AS lag_id,
      t.timestamp AS topup_timestamp
    FROM
      data.topup 
      AS t
    WHERE NOT t.is_credit_transfer
    AND t.timestamp < lag2_time
    AND t.timestamp >= lag1_time
  ) AS tt
  ON mt.alias_id = tt.receiving_id
  AND lags.lag_id = tt.lag_id
  WHERE mt.mod_job_id = in_mod_job_id
  GROUP BY mt.alias_id, lags.lag_id, tt.topup_timestamp;

  

  
  /* ------------ Start calculating time-of-day / time-of-week topup variables. ------------ */
  

  SELECT
    array_to_string(array_agg(case_t),' '),
    array_to_string(array_agg(var_n),', ')
  INTO case_text, var_names
  FROM (
    SELECT
      'MAX(CASE WHEN lag_id = ' || lag_id || ' THEN ' || base_name || ' END) AS ' || b.base_name || c.lag_id || ', ' AS case_t,
       b.base_name || c.lag_id AS var_n
    FROM (
      SELECT 'topup_count_weekly_daytime'    AS base_name UNION ALL
      SELECT 'topup_count_weekly_evening'    AS base_name UNION ALL
      SELECT 'topup_count_weekly_nighttime'  AS base_name UNION ALL
      SELECT 'topup_count_weekly_weekend'    AS base_name 
    ) AS b
    CROSS JOIN (
      SELECT generate_series(1, in_lag_count) AS lag_id
    ) AS c
  ) AS d;

  case_text := trim(TRAILING ', ' FROM case_text);


  query := '
  INSERT INTO work.modelling_data_matrix_topup3
  (mod_job_id, alias_id, '||var_names||')
  SELECT
    '||in_mod_job_id||' AS mod_job_id,
    vals.alias_id,
    '||case_text||'
  FROM (
        SELECT
          tt.alias_id,
          tt.lag_id,
          tt.topup_count_daytime::double precision   / (tt.lag_id * '||in_lag_length||' ) * 7    AS topup_count_weekly_daytime,
          tt.topup_count_evening::double precision   / (tt.lag_id * '||in_lag_length||' ) * 7    AS topup_count_weekly_evening,
          tt.topup_count_nighttime::double precision / (tt.lag_id * '||in_lag_length||' ) * 7    AS topup_count_weekly_nighttime,
          tt.topup_count_weekend::double precision   / (tt.lag_id * '||in_lag_length||' ) * 7    AS topup_count_weekly_weekend
        FROM (
          SELECT 
            alias_id,
            lag_id,
            sum(is_daytime)    OVER (PARTITION BY t.alias_id ORDER BY t.lag_id ASC) AS topup_count_daytime, 
            sum(is_evening)    OVER (PARTITION BY t.alias_id ORDER BY t.lag_id ASC) AS topup_count_evening, 
            sum(is_nighttime)  OVER (PARTITION BY t.alias_id ORDER BY t.lag_id ASC) AS topup_count_nighttime, 
            sum(is_weekend)    OVER (PARTITION BY t.alias_id ORDER BY t.lag_id ASC) AS topup_count_weekend
          FROM (
            SELECT
              alias_id,
              lag_id,
              topup_timestamp,
              CASE WHEN (EXTRACT(dow FROM topup_timestamp) BETWEEN 1 AND 5)  AND  (EXTRACT(hour FROM topup_timestamp)    BETWEEN      7  AND 16)                       THEN 1 ELSE 0 END  AS is_daytime,
              CASE WHEN (EXTRACT(dow FROM topup_timestamp) BETWEEN 1 AND 4)  AND  (EXTRACT(hour FROM topup_timestamp)    BETWEEN     17  AND 21)                       THEN 1 ELSE 0 END  AS is_evening,
              CASE WHEN (EXTRACT(dow FROM topup_timestamp) BETWEEN 1 AND 4)  AND  (EXTRACT(hour FROM topup_timestamp)    NOT BETWEEN  7  AND 21)                       THEN 1 ELSE 0 END  AS is_nighttime,
              CASE WHEN (EXTRACT(dow FROM topup_timestamp) IN (0,6))         OR   (EXTRACT(dow FROM topup_timestamp) = 5 AND EXTRACT(hour FROM topup_timestamp) > 16)  THEN 1 ELSE 0 END  AS is_weekend
              FROM topup_aliases_and_timestamps_temp
          ) AS t
        ) AS tt
    ) AS vals
  GROUP BY vals.alias_id;';

  EXECUTE query;

  PERFORM core.analyze('work.modelling_data_matrix_topup3', in_mod_job_id);

  /* ------------ Finish calculating time-of-day / time-of-week topup variables. ------------ */


  /* ------------ Start calculating statistics combining topup and CDR data. ------------ */
  
/*
  SELECT
    array_to_string(array_agg(case_t),' '),
    array_to_string(array_agg(var_n),', ')
  INTO case_text, var_names
  FROM (
    SELECT
      'MAX(CASE WHEN lag_id = ' || lag_id || ' AND vals.call_type = '|| call_t ||' THEN ' || base_name || ' END) AS ' || b.base_name || '_' || CASE WHEN call_t = 1 THEN 'call' WHEN call_t = 2 THEN 'sms' END || '_avg' || c.lag_id || ', ' AS case_t,
       b.base_name || '_' || CASE WHEN call_t = 1 THEN 'call' WHEN call_t = 2 THEN 'sms' END || '_avg' || c.lag_id AS var_n
    FROM (
      SELECT 1 AS call_t, 'topup_interval_from_last' AS base_name UNION ALL
      SELECT 2 AS call_t, 'topup_interval_from_last' AS base_name UNION ALL
      SELECT 1 AS call_t, 'topup_interval_to_next'   AS base_name UNION ALL
      SELECT 2 AS call_t, 'topup_interval_to_next'   AS base_name 
    ) AS b
    CROSS JOIN (
      SELECT generate_series(1, in_lag_count) AS lag_id
    ) AS c
  ) AS d;

  case_text := trim(TRAILING ', ' FROM case_text);

  
  query := '
  INSERT INTO work.modelling_data_matrix_topup3a
  (mod_job_id, alias_id, '||var_names||')
  SELECT
    '||in_mod_job_id||' AS mod_job_id,
    vals.alias_id,
    '||case_text||'
  FROM (
      SELECT
        tt.alias_id,
        tt.lag_id,
        tt.call_type,
        extract(epoch from avg(tt.topup_interval_from_last_call)) AS topup_interval_from_last,
        extract(epoch from avg(tt.topup_interval_to_next_call))   AS topup_interval_to_next
      FROM (
        SELECT
          t.alias_id,
          t.lag_id,
          t.topup_timestamp,
          c.call_type,
          t.topup_timestamp - max(CASE WHEN c.call_time < t.topup_timestamp THEN c.call_time ELSE NULL END) AS topup_interval_from_last_call,
          min(CASE WHEN c.call_time > t.topup_timestamp THEN c.call_time ELSE NULL END) - t.topup_timestamp AS topup_interval_to_next_call
        FROM 
          topup_aliases_and_timestamps_temp
        AS t
        LEFT JOIN (
          SELECT
            alias_a,
            call_type,
            call_time
            FROM data.cdr
            WHERE call_time >= '''||lag1_time||'''::date
            AND   call_time <  '''||lag2_time||'''::date
          ) AS c
        ON t.alias_id = c.alias_a
        GROUP BY t.alias_id, t.lag_id, t.topup_timestamp, c.call_type
      ) AS tt
      GROUP BY tt.alias_id, tt.lag_id, tt.call_type
    ) AS vals 
  GROUP BY vals.alias_id;';

  EXECUTE query;
*/


  PERFORM core.analyze('work.modelling_data_matrix_topup3a', in_mod_job_id);

  
  /* ------------ Finish calculating statistics combining topup and CDR data. ------------ */



  DROP TABLE IF EXISTS topup_aliases_and_timestamps_temp;
  

END;

                        </code>
                    </input_parameters>
                    <output_parameters>
                        <out></out>
                    </output_parameters>
                </create_predictors_topup_3>
                <create_predictors_topup_channels>
                    <input_parameters>
                        <DataSource1></DataSource1>
                        <queryparameters></queryparameters>
                        <sql>select * from work.create_modelling_data_topup_channels($mod_job_id)</sql>
                        <code>
                            
CREATE FUNCTION create_modelling_data_topup_channels(in_mod_job_id integer) RETURNS void
    AS $$

/* SUMMARY
 * This function calculates predictors from the topup channels used. 
 * Results go to work.modelling_data_matrix_topup_channel.
 * The function needs to be customized for each customer (add channels to topup_channels array).  
 * 
 * INPUT
 * in_mod_job_id : Identifier of module job
 *
 * VERSION
 * 2014-06-05 HMa - ICIF-181 Stack removal (modified from work.create_modelling_data5)
 * 2012-05-14 MOj - Bug 888 fixed
 * 2011-09-15 TSi - Bug 394 and 528 fixed
 */
DECLARE

  topup_channels text[] := array[NULL]; -- Add case specific channels here! As default, the frequency profile of topup channels is not computed.
  t1 date := (
    SELECT DISTINCT to_date(mjp.value, 'YYYY-MM-DD')
    FROM work.module_job_parameters AS mjp
    WHERE mjp.mod_job_id = in_mod_job_id AND mjp.key = 't1'
  );
  t2 date := (
    SELECT DISTINCT to_date(mjp.value, 'YYYY-MM-DD')
    FROM work.module_job_parameters AS mjp
    WHERE mjp.mod_job_id = in_mod_job_id AND mjp.key = 't2'
  );
  i integer;
  n text;
  agg_text text;
  var_names text; 

BEGIN


  IF array_upper(topup_channels, 1) > 1 OR coalesce(char_length(topup_channels[1]), 0) > 0 THEN

    var_names := '';
    agg_text  := '';

    FOR i IN 1..array_upper(topup_channels, 1) LOOP

      n := regexp_replace(lower(trim(topup_channels[i])), '[^a-z0-9]', '_', 'g');

      var_names := var_names || n || ', ';
      agg_text  := agg_text||'max(CASE WHEN d.topup_channel = '''||n||''' THEN d.count ELSE NULL END) / greatest(sum(d.count), 1e-12) AS profile_'||n||', ';
 
    END LOOP;

    agg_text  := trim(TRAILING ', ' FROM agg_text);
    var_names := trim(TRAILING ', ' FROM var_names);
     
    EXECUTE '
    INSERT INTO work.modelling_data_matrix_topup_channel
    (mod_job_id, alias_id, '||var_names||')
    SELECT 
      '||in_mod_job_id||' AS mod_job_id,
       d.alias_id, 
      '||agg_text||'
    FROM (
      SELECT
        mt.alias_id,
        regexp_replace(lower(trim(t.channel)), ''[^a-z0-9]'', ''_'', ''g'') AS topup_channel,
        count(*)::double precision AS "count"
      FROM work.module_targets AS mt
      INNER JOIN data.topup AS t
      ON t.charged_id = mt.alias_id
      WHERE NOT t.is_credit_transfer
      AND t.timestamp < '''||to_char(t2, 'YYYY-MM-DD HH24:MI:SS')||'''::timestamp without time zone
      AND t.timestamp >= '''||to_char(t1, 'YYYY-MM-DD HH24:MI:SS')||'''::timestamp without time zone
      AND mt.mod_job_id = '||in_mod_job_id||'
      GROUP BY mt.alias_id, topup_channel
    ) AS d
    GROUP BY d.alias_id;';

  END IF;

  PERFORM core.analyze('work.modelling_data_matrix_topup_channel', in_mod_job_id);

END;


                        </code>
                    </input_parameters>
                    <output_parameters>
                        <out></out>
                    </output_parameters>
                </create_predictors_topup_channels>
                <calculate_monthly_arpu>
                    <input_parameters>
                        <DataSource1></DataSource1>
                        <queryparameters></queryparameters>
                        <sql>select * from work.calculate_monthly_arpu($mod_job_id)</sql>
                        <code>
                            
CREATE FUNCTION calculate_monthly_arpu(in_mod_job_id integer) RETURNS void
    AS $$
/* SUMMARY:
 * This function calculates normalized (30 days) monthly arpu for specified source period for prepaid and postpaid subscribers, as well as the arpu segmentations
 * Function also takes into account the length of the subscription and for new customer.
 * Inputs:
 *
 * in_mod_job_id             mod_job_id of the job
 *
 * 2014-06-19 QY
 * 2013-06-28 KL 
 */

DECLARE
  --Variables and parameters
  t1                          date; --source period start
  t2                          date; --source period end
  tcrm                        date;
  

BEGIN

  SELECT value::date INTO t2
  FROM work.module_job_parameters a
  WHERE a.key = 't2'
  AND a.mod_job_id = in_mod_job_id;

  SELECT value::date INTO t1
  FROM work.module_job_parameters a
  WHERE a.key = 't1'
  AND a.mod_job_id = in_mod_job_id;

  SELECT value::date INTO tcrm
  FROM work.module_job_parameters a
  WHERE lower(a.key) = 'tcrm'
  AND a.mod_job_id = in_mod_job_id;

  INSERT INTO work.monthly_arpu
    (mod_job_id, alias_id, monthly_arpu,segmentation) 
  SELECT 
    in_mod_job_id AS mod_job_id,
    alias_id,
    monthly_arpu, 
    CASE WHEN s.intile = 1 THEN 'Low value'
         WHEN s.intile = 2 THEN 'Medium value'
         WHEN s.intile = 3 THEN 'High value'
    END AS segmentation
  FROM (	   
    SELECT
      alias_id,
      max(arpu) AS monthly_arpu,
      -- rank() OVER (ORDER BY max(arpu) ASC)  
     NTILE(3) OVER (ORDER BY max(arpu) ASC) AS intile
    FROM (
      SELECT
        charged_id AS alias_id, 
        COALESCE(sum(credit_amount), 0) * 30 / (CASE WHEN (t2 - switch_on_date) < (t2 - t1) THEN t2 - switch_on_date ELSE t2 - t1 END) AS arpu
      FROM data.topup a
      JOIN (
        SELECT alias_id, max(switch_on_date) AS switch_on_date
        FROM data.in_crm
        WHERE switch_on_date < t2
        GROUP BY alias_id
      ) b
      ON a.charged_id = b.alias_id
      WHERE
        is_credit_transfer = false
        AND a.timestamp < t2
        AND a.timestamp >= t1
        GROUP BY a.charged_id, b.switch_on_date
      UNION ALL
      SELECT
        alias_id,
        -- CASE WHEN subscriber_value > 0 THEN subscriber_value
        -- ELSE 0 END  AS arpu  
        0 AS arpu 
      FROM data.in_crm
      WHERE payment_type = 'postpaid'
      AND date_inserted = tcrm
    ) a
    GROUP BY alias_id
  ) s;

END;

                        </code>
                    </input_parameters>
                    <output_parameters>
                        <out></out>
                    </output_parameters>
                </calculate_monthly_arpu>
                <email>
                    <input_parameters>
                        <command>echo "starting network_scorer" + $t2 | mailx -v -r "churn@mci.ir" -s "FAA scoring run" -S smtp="mail.mci.ir:25" -S smtp-use-starttls -S smtp-auth=login -S smtp-auth-user="churn@mci.ir -S smtp-auth-password="CHch@4782" -S ssl-verify=ignore -S nss-config-dir="/etc/pki/nssdb" ext.ho.ashtari@mci.or"</command>
                        <host>localhost</host>
                        <identity></identity>
                        <password>$LocalhostPassword</password>
                        <std.err.file></stderr.file>
                        <stdout.file></stdout.file>
                        <timeout>20000</timeout>
                        <username>$LocalhostUsername</username>
                    </input_parameters>
                    <output_parameters>
                        <exitstatus></exitstatus>
                        <stderr></stderr>
                        <stdout></stdout>
                    </output_parameters>
                </email>
                <Network_Scorer:Out_net>
                </Network_Scorer:Out_net>
                <email>
                    <input_parameters>
                        <command>echo "finished network_scorer" + $t2 | mailx -v -r "churn@mci.ir" -s "FAA scoring run" -S smtp="mail.mci.ir:25" -S smtp-use-starttls -S smtp-auth=login -S smtp-auth-user="churn@mci.ir -S smtp-auth-password="CHch@4782" -S ssl-verify=ignore -S nss-config-dir="/etc/pki/nssdb" ext.ho.ashtari@mci.or"</command>
                        <host>localhost</host>
                        <identity></identity>
                        <password>$LocalhostPassword</password>
                        <std.err.file></stderr.file>
                        <stdout.file></stdout.file>
                        <timeout>20000</timeout>
                        <username>$LocalhostUsername</username>
                    </input_parameters>
                    <output_parameters>
                        <exitstatus></exitstatus>
                        <stderr></stderr>
                        <stdout></stdout>
                    </output_parameters>
                </email>
                <create_predictors_2>
                    <input_parameters>
                        <DataSource1></DataSource1>
                        <queryparameters></queryparameters>
                        <sql>select * from work.create_modelling_data2($mod_job_id)</sql>
                        <code>
                            
CREATE FUNCTION create_modelling_data2(in_mod_job_id integer) RETURNS text
    AS $$

/* SUMMARY
 * This function processes modelling variables that are aggregates over
 * neighbour data. If model_id is available in work.module_job_parameters table, then
 * only the variables that the model use are processed. Otherwise, all variables
 * are processed. Results go to work.modelling_data_matrix_2. 
 *
 * INPUT
 * Identifier of module job
 *
 * OUTPUT
 * Query by which this function has inserted data into
 * work.modelling_data_matrix_2 table
 *
 * VERSION
 * 04.06.2014 HMa - ICIF-181 Stack removal
 * 13.05.2013 KL  - Added secondary network knn_2 to modelling_variables and data
 * 06.03.2013 MOJ - ICIF-112
 * 17.01.2013 HMa - Removed job_type 
 * 15.01.2013 HMa - ICIF-97
 * 22.02.2012 TSi - Bug 841 fixed
 * 17.10.2011 TSi - Bug 742 fixed
 * 01.10.2010 TSi - Bugs 225 and 175 fixed
 * 16.09.2010 TSi - Bug 208 fixed
 * 14.06.2010 TSi - Bug 188 fixed
 * 19.04.2010 TSi - Bug 177 fixed
 * 19.04.2010 JMa
 */

DECLARE

  t1_text text := (
    SELECT DISTINCT 'to_date('''||mjp.value||''', ''YYYY-MM-DD'')'
    FROM work.module_job_parameters AS mjp
    WHERE mjp.mod_job_id = in_mod_job_id AND lower(mjp.key) = 't1'
  );
  t2_text text := (
    SELECT DISTINCT 'to_date('''||mjp.value||''', ''YYYY-MM-DD'')'
    FROM work.module_job_parameters AS mjp
    WHERE mjp.mod_job_id = in_mod_job_id AND lower(mjp.key) = 't2'
  );
  t1_week text := (
    SELECT DISTINCT core.date_to_yyyyww(mjp.value::date)::text
    FROM work.module_job_parameters AS mjp
    WHERE mjp.mod_job_id = in_mod_job_id AND lower(mjp.key) = 't1'
  );
  t2_week text := (
    SELECT DISTINCT core.date_to_yyyyww(mjp.value::date)::text
    FROM work.module_job_parameters AS mjp
    WHERE mjp.mod_job_id = in_mod_job_id AND lower(mjp.key) = 't2'
  );
  source_period_length integer := (
    SELECT
      max(CASE WHEN lower(mjp.key) = 't2' THEN mjp.value::date ELSE NULL END)
      - max(CASE WHEN lower(mjp.key) = 't1' THEN mjp.value::date ELSE NULL END)
    FROM work.module_job_parameters AS mjp
    WHERE mjp.mod_job_id = in_mod_job_id
  );
  tcrm_text text := (
    SELECT DISTINCT 'to_date('''||mjp.value||''', ''YYYY-MM-DD'')'
    FROM work.module_job_parameters AS mjp
    WHERE mjp.mod_job_id = in_mod_job_id AND lower(mjp.key) = 'tcrm'
  );
  net_job_id integer := (
    SELECT DISTINCT mjp.value::integer
    FROM work.module_job_parameters AS mjp
    WHERE mjp.mod_job_id = in_mod_job_id AND lower(mjp.key) = 'xsl_job_id'
  );
  net_job_id_2 integer := (
    SELECT DISTINCT mjp.value::integer
    FROM work.module_job_parameters AS mjp
    WHERE mjp.mod_job_id = in_mod_job_id AND lower(mjp.key) = 'xsl_job_id_2'
  );
  override_model_variables_check boolean := (
    SELECT coalesce(lower(trim(max(mjp.value))) = 'true', FALSE)
    FROM work.module_job_parameters AS mjp 
    WHERE mjp.mod_job_id = in_mod_job_id 
    AND mjp.key = 'override_model_variables_check'
  );
  model_ids record;  
  all_var_names text[];
  included_var_names text[];
  aggr_block text := '';
  var_names text := '';
  aggr_new text;
  query text;
  n text;

BEGIN

  -- Find model ids of all the use cases in this module job:
  SELECT 
    mp.value 
  INTO model_ids 
  FROM work.module_job_parameters AS mp
  WHERE mp.mod_job_id = in_mod_job_id 
  AND mp.key LIKE '%model_id';

  all_var_names :=  array_agg(columns) 
    FROM  work.get_table_columns_with_prefix('work','modelling_data_matrix_2','','mod_job_id,alias_id'); 


  IF model_ids IS NULL OR override_model_variables_check THEN
    included_var_names := all_var_names;
  ELSE
    query := 
    'SELECT array_agg(DISTINCT mm.value) AS var_name
     FROM work.module_models AS mm
     WHERE mm.value IN ('''||array_to_string(all_var_names, ''',''')||''')
     AND mm.key = ''varname''
     AND mm.model_id IN (
       SELECT 
         mp.value::integer AS model_id
       FROM work.module_job_parameters AS mp
       WHERE mp.mod_job_id = '||in_mod_job_id||'
       AND mp.key LIKE ''%model_id''
       UNION ALL
       SELECT DISTINCT mm3.value::integer
       FROM work.module_models AS mm2
       INNER JOIN work.module_models AS mm3
       ON mm2.model_id = mm3.model_id
       WHERE mm2.key = ''model_type''
       AND mm2.value = ''submodel_list''
       AND mm3.key = ''model_id'' 
       AND mm2.model_id IN (
         SELECT 
           mp.value::integer AS model_id
         FROM work.module_job_parameters AS mp
         WHERE mp.mod_job_id = '||in_mod_job_id||'
         AND mp.key LIKE ''%model_id''
       )
     );';
    EXECUTE query INTO included_var_names;
  END IF;

  n := array_upper(included_var_names, 1);

  IF n = '0' THEN
    RETURN 'WARNING: No variables processed';
  END IF;


  -- Parse some auxiliary texts
  FOR n IN (
    SELECT included_var_names[s] FROM generate_series(1,array_upper(included_var_names, 1)) AS s
  ) LOOP
    aggr_new := (SELECT CASE
      WHEN n = 'k_offnet' THEN
        'sum((1 - f.in_out_network) * b.weight)::double precision / greatest(sum(b.weight), 1e-16) AS k_offnet'
      WHEN n = 'k_target' THEN
        'avg(c.source_period_inactivity) AS k_target'
      WHEN n = 'knn' THEN
        'round(avg(e.k)) AS knn'
      WHEN n = 'knn_2' THEN
        'round(avg(ee.k)) AS knn_2'
      ELSE NULL
    END);
    IF aggr_new IS NOT NULL THEN
      var_names := var_names||n||', ';
      aggr_block := aggr_block||aggr_new||', ';
    END IF;
  END LOOP;

  -- Fine-tune the auxiliary texts
  var_names := trim(TRAILING ', ' FROM var_names);
  aggr_block := trim(TRAILING ', ' FROM aggr_block);

  -- Parse the query
  query := 
      'INSERT INTO work.modelling_data_matrix_2
      (mod_job_id, alias_id, '||var_names||')
       SELECT 
         '||in_mod_job_id||' AS mod_job_id,
          b.alias_a AS alias_id, 
         '||aggr_block||'
       FROM work.module_targets AS a 
       INNER JOIN work.out_network AS b
       ON b.alias_a = a.alias_id
       LEFT JOIN (
         SELECT 
           sa.alias_id, 
           1 - (core.yyyyww_to_date(max(sa.period))-'||t1_text||')::double precision / '||source_period_length-7||'::double precision AS source_period_inactivity
         FROM data.in_split_aggregates AS sa
         WHERE sa.in_out_network = 1
         AND sa.voicecount + sa.smscount > 0
         AND sa.period >= '||t1_week||' 
         AND sa.period <  '||t2_week||'
         GROUP BY sa.alias_id
       ) AS c
       ON c.alias_id = b.alias_b
       LEFT JOIN work.out_scores AS e 
       ON e.alias_id = b.alias_b
       AND e.job_id = '||net_job_id||'
       LEFT JOIN work.out_scores AS ee
       ON ee.alias_id = b.alias_b
       AND ee.job_id = '||net_job_id_2||'
       LEFT JOIN work.aliases AS f
       ON f.alias_id = b.alias_b
       AND f.job_id = '||net_job_id||'
       WHERE b.job_id = '||net_job_id||'
       AND a.mod_job_id = '||in_mod_job_id||'
       GROUP BY b.alias_a';

  -- Execute and return the query, do some maintenance in between
  EXECUTE query;
  PERFORM core.analyze('work.modelling_data_matrix_2', in_mod_job_id);
  RETURN query;

END;

                        </code>
                    </input_parameters>
                    <output_parameters>
                        <out></out>
                    </output_parameters>
                </create_predictors_2>
                <create_predictors_3>
                    <input_parameters>
                        <DataSource1></DataSource1>
                        <queryparameters></queryparameters>
                        <sql>select * from work.create_modelling_data3($mod_job_id)</sql>
                        <code>
                            
CREATE FUNCTION create_modelling_data3(in_mod_job_id integer) RETURNS text
    AS $$

/* SUMMARY
 * This function processes modelling variables that can be found from some
 * existing table without massive processing such as aggregating. If model_id is
 * available in work.module_job_parameters table, then only the so-called necessary
 * variables and the variables that the model use are processed. Otherwise, all
 * variables are processed. Results go to work.modelling_data_matrix_3.
 *
 * INPUT
 * Identifier of module job
 *
 * OUTPUT
 * Query by which this function has inserted data into
 * work.modelling_data_matrix_3 table
 *
 * VERSION
 * 04.06.2014 HMa - ICIF-181 Stack removal
 * 13.05.2013 KL  - Added secondary network plus monthly_arpu to modelling_variables + data
 * 06.03.2013 MOJ - ICIF-112
 * 15.01.2013 HMa - Removed variable 'target'
 * 15.01.2013 HMa - ICIF-97
 * 14.09.2012 MOJ - ICIF-65 fixed
 * 22.02.2012 TSi - Bug 841 fixed
 * 13.10.2011 MOj - Bug 739 fixed
 * 16.09.2011 TSi - Bug 230 and 429 fixed
 * 11.02.2011 MOj - Bug 431 fixed
 * 03.02.2011 MOj - Bug 415 fixed
 * 26.01.2011 TSi - Bug 352 fixed
 * 14.06.2010 TSi - Bug 188 fixed
 * 19.04.2010 JMa
 */

DECLARE
  t2_text text := (
    SELECT DISTINCT 'to_date(''' || mjp.value || ''', ''YYYY-MM-DD'')'
    FROM work.module_job_parameters AS mjp
    WHERE mjp.mod_job_id = in_mod_job_id AND lower(mjp.key) = 't2'
  );
  tcrm_text text := (
    SELECT DISTINCT 'to_date(''' || mjp.value || ''', ''YYYY-MM-DD'')'
    FROM work.module_job_parameters AS mjp
    WHERE mjp.mod_job_id = in_mod_job_id AND lower(mjp.key) = 'tcrm'
  );
  net_job_id integer := (
    SELECT DISTINCT mjp.value::integer
    FROM work.module_job_parameters AS mjp
    WHERE mjp.mod_job_id = in_mod_job_id AND lower(mjp.key) = 'xsl_job_id'
  );
  net_job_id_2 integer := (
    SELECT DISTINCT mjp.value::integer
    FROM work.module_job_parameters AS mjp
    WHERE mjp.mod_job_id = in_mod_job_id AND lower(mjp.key) = 'xsl_job_id_2'
  );
  override_model_variables_check boolean := (
    SELECT coalesce(lower(trim(max(mjp.value))) = 'true', FALSE)
    FROM work.module_job_parameters AS mjp 
    WHERE mjp.mod_job_id = in_mod_job_id 
    AND mjp.key = 'override_model_variables_check'
  );
  all_var_names text[];
  necessary_var_names text[] := array[ -- Add here variables that you want to become processed in any case
    'k',
    'separate_node'
  ];
  included_var_names text[];
  model_ids record;  
  aggr_block text := '';
  var_names text := '';
  aggr_new text;
  query text;
  tenure_query text;
  n text;

BEGIN

  -- Find model ids of all the use cases in this module job:
  SELECT 
    mp.value
  INTO model_ids
  FROM work.module_job_parameters AS mp
  WHERE mp.mod_job_id = in_mod_job_id 
  AND mp.key LIKE '%model_id';

  all_var_names := array_agg(columns) FROM work.get_table_columns_with_prefix('work','modelling_data_matrix_3','','mod_job_id,alias_id');   


  IF model_ids IS NULL OR override_model_variables_check THEN
    included_var_names := all_var_names;
  ELSE
    query := 
      'SELECT array_agg(DISTINCT z.var_name)
       FROM (
         SELECT mm.value AS var_name
         FROM work.module_models AS mm
         WHERE mm.value IN (''' || array_to_string(all_var_names, ''',''') || ''') 
         AND mm.key = ''varname''
         AND mm.model_id IN (
           SELECT 
           mp.value::integer AS model_id
           FROM work.module_job_parameters AS mp
           WHERE mp.mod_job_id = ' || in_mod_job_id || '
           AND mp.key LIKE ''%model_id''
           UNION ALL
           SELECT DISTINCT mm3.value::integer
           FROM work.module_models AS mm2
           INNER JOIN work.module_models AS mm3
           ON mm2.model_id = mm3.model_id
           WHERE mm2.key = ''model_type''
           AND mm2.value = ''submodel_list''
           AND mm3.key = ''model_id'' 
           AND mm2.model_id IN (
             SELECT 
               mp.value::integer AS model_id
             FROM work.module_job_parameters AS mp
             WHERE mp.mod_job_id = ' || in_mod_job_id || '
             AND mp.key LIKE ''%model_id''
           )
         )
         UNION ALL SELECT ''' || array_to_string(necessary_var_names, ''' AS var_name UNION ALL SELECT ''') || ''' AS var_name
       ) AS z;';
    EXECUTE query INTO included_var_names;
  END IF;

  n := array_upper(included_var_names, 1);

  IF n = '0' THEN
    RETURN 'WARNING: No variables processed';
  END IF;


  -- Parse some auxiliary texts
  FOR n IN (
    SELECT included_var_names[s] FROM generate_series(1, array_upper(included_var_names, 1)) AS s
  ) LOOP
    aggr_new := (SELECT CASE
      WHEN n IN (
        'gender', -- 'subscriber_value', 'tariff_plan', 'handset_model', 'country', 'language',
        -- 'subscriber_segment', 'flg', 'zip', 'churn_score', 'subscription_type', 
		'city','payment_type'
      ) THEN 
        'crm.' || n || ' AS ' || n
      WHEN n = 'age' THEN 
        '(' || t2_text || ' - crm.birth_date)::double precision / 365.0 AS age'        
      -- WHEN n = 'handset_age' THEN
      --  '(' || t2_text || ' - crm.handset_begin_date)::double precision / 30.0 AS handset_age'
      WHEN n = 'contr_length' THEN
        '(' || t2_text || ' - crm.switch_on_date::date)::double precision / 30.0 AS contr_length'
      -- WHEN n = 'contr_remain' THEN
      --  '(crm.binding_contract_start_date + crm.binding_contract_length - ' || t2_text || ')::double precision / 30.0 AS contr_remain'
      WHEN n = 'no_churn_score' THEN
        '0 As no_churn_score'
		-- 'CASE
        --    WHEN crm.churn_score IS NULL THEN 1
        --    ELSE 0
        --  END AS no_churn_score'
      WHEN n IN ('alpha', 'k', 'c', 'wec', 'kshell', 'socrev', 'socrevest') THEN
        'os.' || n || ' AS ' || n
      WHEN n IN ('alpha_2', 'k_2', 'c_2', 'wec_2', 'kshell_2', 'socrev_2', 'socrevest_2') THEN
        'os_2.' || trim(trailing '_2' FROM n) || ' AS ' || n
      WHEN n = 'separate_node' THEN
        'CASE
           WHEN os.k = 0 OR os.k IS NULL THEN 1 
           ELSE 0 
         END AS separate_node'
      WHEN n = 'monthly_arpu' THEN
        'ma.' || n || ' AS monthly_arpu'
      ELSE NULL
    END);
    IF aggr_new IS NOT NULL THEN
      var_names := var_names || n || ', ';
      aggr_block := aggr_block || aggr_new || ', ';
    END IF;
  END LOOP;

  -- Fine-tune the auxiliary texts
  var_names := trim(TRAILING ', ' FROM var_names);
  aggr_block := trim(TRAILING ', ' FROM aggr_block);

  -- Parse the query
  query :=
    'INSERT INTO work.modelling_data_matrix_3
     (mod_job_id, alias_id, ' || var_names || ')
     SELECT 
      ' || in_mod_job_id || ' AS mod_job_id,
       mt.alias_id, 
      ' || aggr_block || '
     FROM work.module_targets AS mt
     LEFT JOIN 
       work.monthly_arpu ma
     ON mt.alias_id = ma.alias_id
     AND ma.mod_job_id = ' || in_mod_job_id || '
     LEFT JOIN (
       SELECT crm1.* 
       FROM data.in_crm AS crm1
       WHERE crm1.date_inserted = ' || tcrm_text || '
     ) AS crm
     ON mt.alias_id = crm.alias_id
     LEFT JOIN (
       SELECT 
         os1.alias_id,
         os1.alpha,
         os1.k,
         os1.c,
         os1.wec,
         os1.kshell,
         os1.socrev,
         os1.socrevest
       FROM work.out_scores AS os1
       WHERE os1.job_id = ' || net_job_id || '
     ) AS os
     ON mt.alias_id = os.alias_id
     LEFT JOIN (
       SELECT os1.*
       FROM work.out_scores AS os1
       WHERE os1.job_id = ' || net_job_id_2 || '
     ) AS os_2
     ON mt.alias_id = os_2.alias_id
     WHERE mt.mod_job_id = ' || in_mod_job_id;

  -- Query for tenure_days segmentation
  tenure_query :=
    'INSERT INTO work.tenure_days 
     (mod_job_id, alias_id, tenure_days, segmentation)
     SELECT 
      ' || in_mod_job_id || ' AS mod_job_id,
       mdt.alias_id, 
       ceiling(mdt.contr_length * 30) AS tenure_days,
	   CASE WHEN ceiling(mdt.contr_length * 30)  < 30 AND ceiling(mdt.contr_length * 30) >= 0   THEN ''Less than a month''
            WHEN ceiling(mdt.contr_length * 30) >= 30 AND ceiling(mdt.contr_length * 30) <  91  THEN ''One to three months''
            WHEN ceiling(mdt.contr_length * 30) >= 92 AND ceiling(mdt.contr_length * 30) <  365 THEN ''Three months to one year''
            WHEN ceiling(mdt.contr_length * 30) >= 365 THEN ''More than one year''
		   ELSE NULL END
           AS segmentation
      FROM work.modelling_data_matrix_3 AS mdt
      WHERE mdt.mod_job_id = ' || in_mod_job_id;
	 
  -- Execute and return the query, do some maintenance in between
  EXECUTE query;  
  PERFORM core.analyze('work.modelling_data_matrix_3', in_mod_job_id);
  EXECUTE tenure_query;
  RETURN query;

END;

                        </code>
                    </input_parameters>
                    <output_parameters>
                        <out></out>
                    </output_parameters>
                </create_predictors_3>
                <RunLDA_MCI>
                    <split:Calculate_LDA_input?>
                        <when>parameter with name equal to calculate_lda_input_data and value equal to true</when>
                        <tasks>create_handset_LDA_input_data, join</tasks>
                        <when>parameter with name equal to calculate_lda_input_data and value equal to false</when>
                        <tasks>join</tasks>
                    </split:Calculate_LDA_input?>
                    <create_handset_LDA_input_data>
                        <input_parameters>
                            <DataSource1></DataSource1>
                            <queryparameters></queryparameters>
                            <sql>select * from work.handset_lda_input_data($mod_job_id)</sql>
                            <code>
                                
CREATE FUNCTION handset_lda_input_data(in_mod_job_id integer) RETURNS SETOF integer
    AS $$ 
/* 
 * A function to calculate the input data for LDA that calculates handset topics from the location profiles
 * 
 * Parameters: in_mod_job_id: module job id defining the input data period (t1...t2)
 * 
 * Reads cell_id data from data.cdr and handset model data form data.crm. 
 * Generates a new LDA input ID
 * Writes to work.lda_input and work.lda_input_parameters
 * 
 * VERSION
 * 30.11.2012 HMa
 * 
 */


DECLARE
  lda_inp_id INTEGER;  
  t1_text text := (
    SELECT DISTINCT mjp.value
    FROM work.module_job_parameters AS mjp
    WHERE mjp.mod_job_id = in_mod_job_id AND mjp.key = 't1'
  );  
  t2_text text := (
    SELECT DISTINCT mjp.value
    FROM work.module_job_parameters AS mjp
    WHERE mjp.mod_job_id = in_mod_job_id AND mjp.key = 't2'
  );
  tcrm_text text := (
    SELECT DISTINCT mjp.value
    FROM work.module_job_parameters AS mjp
    WHERE mjp.mod_job_id = in_mod_job_id AND lower(mjp.key) = 'tcrm'
  );
--  t_start text := greatest(to_date(t1_text,'YYYY-MM-DD'),(to_date(t2_text,'YYYY-MM-DD')-'3 month'::interval))::text; -- Calculate from the last month of the source period (or shorter if shorter source period)
 
  
BEGIN
  
  SELECT core.nextval_with_partitions('work.lda_input_sequence') INTO lda_inp_id;
  
  INSERT INTO work.lda_input_parameters 
  (lda_input_id, "key", "value")
  VALUES 
  (lda_inp_id, 'LDA type', 'handset_topic_from_cellid'),
  (lda_inp_id, 'doc', 'cell_id'),
  (lda_inp_id, 'term', 'handset_model'),
  (lda_inp_id, 't_start', t1_text),
  (lda_inp_id, 't_end', t2_text);

  INSERT INTO work.lda_input_parameters
  (lda_input_id, "key", "value")
  SELECT lda_inp_id AS lda_job_id, 'input_data_inserted' AS "key", to_char(CURRENT_TIMESTAMP(0), 'YYYY-MM-DD HH24:MI:SS') AS "value";

    -- Calculate the LDA input data:
    INSERT INTO work.lda_input (lda_id, doc, term, n) 
    SELECT 
      lda_inp_id,
      cell_id::text as doc, 
      trim(lower(handset_model)) as term, 
      count(distinct alias_id) AS n
    FROM (
      SELECT 
        crm.alias_id, 
        cdr.a_cell_id AS cell_id, 
        crm.handset_model
      FROM data.in_crm AS crm
      INNER JOIN data.cdr AS cdr
      ON crm.alias_id = cdr.alias_a -- select all innet subs who made a call and whose handset model and current cell_id is known
      WHERE crm.date_inserted = to_date(tcrm_text,'YYYY-MM-DD')
      AND crm.handset_model IS NOT NULL
      AND cdr.a_cell_id IS NOT NULL
      AND cdr.call_time >= to_timestamp(t1_text,'YYYY-MM-DD') 
      AND cdr.call_time < to_timestamp(t2_text,'YYYY-MM-DD') 
      AND cdr.call_time >= crm.handset_begin_date
      UNION ALL
      SELECT 
        crm.alias_id, 
        cdr.b_cell_id AS cell_id, 
        crm.handset_model
      FROM data.in_crm AS crm
      INNER JOIN data.cdr AS cdr
      ON crm.alias_id = cdr.alias_b -- select all innet subs who received a call and whose handset model and current cell_id is known
      WHERE crm.date_inserted = to_date(tcrm_text,'YYYY-MM-DD')
      AND crm.handset_model IS NOT NULL
      AND cdr.b_cell_id IS NOT NULL
      AND cdr.call_time >= to_timestamp(t1_text,'YYYY-MM-DD') 
      AND cdr.call_time < to_timestamp(t2_text,'YYYY-MM-DD') 
      AND cdr.call_time >= crm.handset_begin_date
    ) c
    GROUP BY cell_id, handset_model;  

  RETURN NEXT lda_inp_id;

END;

                            </code>
                        </input_parameters>
                        <output_parameters>
                            <out></out>
                            <lda_input_id>$out.get(0).get(0)</lda_input_id>
                        </output_parameters>
                    </create_handset_LDA_input_data>
                    <Handset_LDA_Task>
                    </Handset_LDA_Task>
                    <Get_LDA_output_ID>
                        <input_parameters>
                            <DataSource1></DataSource1>
                            <queryparameters></queryparameters>
                            <sql>select coalesce(max(lda_output_id), -1) from work.lda_output_parameters</sql>
                        </input_parameters>
                        <output_parameters>
                            <out></out>
                            <lda_output_id>$out.get(0).get(0)</lda_output_id>
                        </output_parameters>
                    </Get_LDA_output_ID>
                </RunLDA_MCI>
                <RunLDA_cell_events_MCI>
                    <split:calculate_lda_input?>
                        <when>parameter with name equal to calculate_lda_cell_events_input_data and value equal to true</when>
                        <tasks>create_events_lda_input_data, join</tasks>
                        <when>parameter with name equal to calculate_lda_cell_events_input_data and value equal to false</when>
                        <tasks>join</tasks>
                    </split:calculate_lda_input?>
                    <create_events_lda_input_data>
                        <input_parameters>
                            <DataSource1></DataSource1>
                            <queryparameters></queryparameters>
                            <sql>select * from work.cellid_lda_input_data($mod_job_id)</sql>
                            <code>
                                
CREATE FUNCTION cellid_lda_input_data(in_mod_job_id integer) RETURNS SETOF integer
    AS $$ 
/* 
 * A function to calculate the input data for LDA that calculates cell id topics from the location profiles
 * 
 * Parameters: in_mod_job_id: module job id defining the input data period (t1...t2)
 * 
 * Reads cell_id data from data.cdr and aggregates form data.in_split_weekly. 
 * Generates a new LDA input ID
 * Writes to work.lda_input and work.lda_input_parameters
 * 
 * VERSION
 * 20.09.2014 QYu 
 * 30.11.2012 HMa
 * 
 */


DECLARE
  lda_inp_id INTEGER;  
  insert_query text;
  t1_text text := (
    SELECT DISTINCT mjp.value
    FROM work.module_job_parameters AS mjp
    WHERE mjp.mod_job_id = in_mod_job_id AND mjp.key = 't1'
  );  	
  t2_text text := (
    SELECT DISTINCT mjp.value
    FROM work.module_job_parameters AS mjp
    WHERE mjp.mod_job_id = in_mod_job_id AND mjp.key = 't2'
  );
  tcrm_text text := (
    SELECT DISTINCT mjp.value
    FROM work.module_job_parameters AS mjp
    WHERE mjp.mod_job_id = in_mod_job_id AND lower(mjp.key) = 'tcrm'
  );
--  t_start text := greatest(to_date(t1_text,'YYYY-MM-DD'),(to_date(t2_text,'YYYY-MM-DD')-'3 month'::interval))::text; -- Calculate from the last month of the source period (or shorter if shorter source period)
 
  
BEGIN
  
  SELECT core.nextval_with_partitions('work.lda_input_sequence') INTO lda_inp_id;
  
  INSERT INTO work.lda_input_parameters 
  (lda_input_id, "key", "value")
  VALUES 
  (lda_inp_id, 'LDA type', 'cellid_topic_from_events'),
  (lda_inp_id, 'doc', 'events_type'),
  (lda_inp_id, 'term', 'cell_id'),
  (lda_inp_id, 't_start', t1_text),
  (lda_inp_id, 't_end', t2_text);

  INSERT INTO work.lda_input_parameters
  (lda_input_id, "key", "value")
  SELECT lda_inp_id AS lda_job_id, 'input_data_inserted' AS "key", to_char(CURRENT_TIMESTAMP(0), 'YYYY-MM-DD HH24:MI:SS') AS "value";
  
 	FOR i IN 1..(24*7) LOOP
	  insert_query := 
	 'INSERT INTO work.lda_input (lda_id, doc, term, n) 
	  SELECT ' 
	  ||lda_inp_id ||','
	  ||'''voice_c_'||i ||'''as doc, 
	  cell_id::text as term, 
	  sum(cc.n_sub) AS n	  
	  FROM (
		  SELECT 
		  alias_a AS alias_id, 
		  a_cell_id AS cell_id,
          count(*) AS n_sub		  
		  FROM data.cdr aa 
		  WHERE aa.a_cell_id IS NOT NULL 
		  AND aa.call_time >= to_timestamp('''||t1_text||''',''YYYY-MM-DD'') 
		  AND aa.call_time < to_timestamp('''||t2_text||''',''YYYY-MM-DD'')
          AND aa.call_time >= date_trunc(''week'', aa.call_time + '''|| i-1 || ' hours''::interval)::timestamp without time zone 
          AND aa.call_time < date_trunc(''week'', aa.call_time + '''|| i || ' hours''::interval)::timestamp without time zone 
		  GROUP BY a_cell_id, alias_a 
		  UNION ALL 
		  SELECT
		  alias_b AS alias_id, 
		  b_cell_id AS cell_id,
          count(*) AS n_sub	 
		  FROM data.cdr bb 
		  --INNER JOIN aliases.aliases_updated au
		  --ON bb.alias_b = au.alias_id
		  --WHERE au.net_id =0
		  WHERE bb.b_cell_id IS NOT NULL
		  AND bb.call_time >= to_timestamp('''||t1_text||''',''YYYY-MM-DD'') 
		  AND bb.call_time < to_timestamp('''||t2_text||''',''YYYY-MM-DD'')
          AND bb.call_time >= date_trunc(''week'', bb.call_time + '''|| i-1 || ' hours''::interval)::timestamp without time zone 
          AND bb.call_time < date_trunc(''week'', bb.call_time + '''|| i || ' hours''::interval)::timestamp without time zone 		  
		  GROUP BY b_cell_id, alias_b
	  ) cc
	  GROUP BY cell_id';
	  execute insert_query;
  
    
	  
  END LOOP;
  
  --TRUNCATE work.lda_input_tmp;
	
  RETURN NEXT lda_inp_id;

END;

                            </code>
                        </input_parameters>
                        <output_parameters>
                            <out></out>
                            <lda_cell_events_input_id>$out.get(0).get(0)</lda_cell_events_input_id>
                        </output_parameters>    
                    </create_events_lda_input_data>
                    <Handset_LDA_Task>
                    </Handset_LDA_Task>
                    <Get_LDA_output_ID>
                        <input_parameters>
                            <DataSource1></DataSource1>
                            <queryparameters></queryparameters>
                            <sql>select coalesce(max(lda_output), -1) from work.lda_output_parameters</sql>
                        </input_parameters>
                        <output_parameters>
                            <out></out>
                        </output_parameters>
                    </Get_LDA_output_ID>
                </RunLDA_cell_events_MCI>
                <split:calculate_lda_predictors>
                    <when>parameter with name equal to calculate_lda_predictors and value equal to true</when>
                    <tasks>create_predictors_handset_model, join</tasks>
                    <when>parameter with name equal to calculate_lda_predictors and value equal to false</when>
                    <tasks>join</tasks>
                </split:calculate_lda_predictors>
                <create_predictors_handset_model>
                    <input_parameters>
                        <DataSource1></DataSource1>
                        <queryparameters></queryparameters>
                        <sql>select * from work.create_modelling_data_handset_model($mod_job_id, $lda_output_id)</sql>
                        <code>
                            
CREATE FUNCTION create_modelling_data_handset_model(in_mod_job_id integer, in_lda_output_id integer) RETURNS void
    AS $$
/*
 * A function to add handset topics obtained from LDA runner to predictors.
 * Reads from work.lda_output
 * Inserts into work.modelling_data_matrix_handset_topic.
 * 
 * Parameters: 
 * in_mod_job_id: module job id
 * in_lda_output_id: LDA output id 
 * 
 * VERSION 
 * 05.06.2014 HMa - ICIF-181 Stack removal
 * 30.11.2012 HMa
 */


DECLARE

  tcrm_date date := (
    SELECT DISTINCT to_date(mjp.value, 'YYYY-MM-DD')
    FROM work.module_job_parameters AS mjp
    WHERE mjp.mod_job_id = in_mod_job_id AND lower(mjp.key) = 'tcrm'
  );
  i integer;
  n_topics integer;
  var_names text := '';
  aggr_block text := '';
  query text;

BEGIN

  IF in_lda_output_id = -1 THEN
    RAISE NOTICE 'in_lda_output_id % is not valid, quitting.',in_lda_output_id;
    RETURN;
  END IF;

  SELECT value::integer FROM work.lda_output_parameters WHERE lda_output_id = in_lda_output_id AND key = 'n_topics' INTO n_topics;

  
  FOR i IN 1..(n_topics-1) LOOP -- leave one topic out because of collinearity
    var_names := var_names || 'handset_topic_' || i || ', ';
    aggr_block := aggr_block || 'MAX(CASE WHEN topic = ''handset_topic_' || i || ''' THEN value END) AS handset_topic_' || i || ', '; 
  END LOOP;
  
  var_names  := trim(TRAILING ', ' FROM var_names);
  aggr_block := trim(TRAILING ', ' FROM aggr_block);

  -- Values of new variables added to work.modelling_data (stack)
  query := 
  'INSERT INTO work.modelling_data_matrix_handset_topic
  ( mod_job_id, alias_id, '||var_names||' )
      SELECT
        '||in_mod_job_id||' AS mod_job_id,
        vals.alias_id,    
        '||aggr_block||'
      FROM (
        SELECT 
          mt.alias_id, 
          hmp.topic,
          hmp.value
        FROM work.module_targets AS mt 
        LEFT JOIN 
          data.in_crm AS b
        ON mt.alias_id = b.alias_id
        LEFT JOIN 
          work.lda_output AS hmp
        ON lower(replace(b.handset_model,'' '','''')) = lower(replace(hmp.term,'' '','''')) -- because of possible formatting differences, remove spaces and convert to lowercase
        WHERE mt.mod_job_id = '||in_mod_job_id ||'
        AND b.date_inserted = '''||tcrm_date||'''::date
        AND hmp.lda_id = '||in_lda_output_id||'
      ) AS vals
      GROUP BY alias_id;';

  EXECUTE query;

  PERFORM core.analyze('work.modelling_data_matrix_handset_topic', in_mod_job_id);
END;

                        </code>
                    </input_parameters>
                    <output_parameters>
                        <out></out>
                    </output_parameters>
                </create_predictors_handset_model>
                <split:calculate_lda_predictors>
                    <when>parameter with name equal to calculate_lda_cell_events_predictors and value equal to true</when>
                    <tasks>create_predictors_cell_events_model, join</tasks>
                    <when>parameter with name equal to calculate_lda_cell_events_predictors and value equal to false</when>
                    <tasks>join</tasks>
                </split:calculate_lda_predictors>
                <create_predictors_cell_events_model>
                    <input_parameters>
                        <DataSource1></DataSource1>
                        <queryparameters></queryparameters>
                        <sql>select * from work.create_modelling_data_cell_events_model($mod_job_id, $lda_cell_events_output_id)</sql>
                        <code>
                            
CREATE FUNCTION create_modelling_data_cell_events_model(in_mod_job_id integer, in_lda_output_id integer) RETURNS void
    AS $$
/*
 * A function to add cell topics obtained from LDA runner to predictors.
 * Reads from work.lda_output
 * Inserts into work.modelling_data_matrix_cell_events_topic.
 * 
 * Parameters: 
 * in_mod_job_id: module job id
 * in_lda_output_id: LDA output id 
 * 
 * VERSION 
 * 20.09.2014 QYu 
 * 05.06.2014 HMa - ICIF-181 Stack removal
 * 30.11.2012 HMa
 */


DECLARE
   t1_text text := (
    SELECT DISTINCT mjp.value
    FROM work.module_job_parameters AS mjp
    WHERE mjp.mod_job_id = in_mod_job_id AND mjp.key = 't1'
  );  
  t2_text text := (
    SELECT DISTINCT mjp.value
    FROM work.module_job_parameters AS mjp
    WHERE mjp.mod_job_id = in_mod_job_id AND mjp.key = 't2'
  );
  i integer;
  n_topics integer;
  var_names text := '';
  aggr_block text := '';
  query text;

BEGIN

  IF in_lda_output_id = -1 THEN
    RAISE NOTICE 'in_lda_output_id % is not valid, quitting.',in_lda_output_id;
    RETURN;
  END IF;

  SELECT value::integer FROM work.lda_output_parameters WHERE lda_output_id = in_lda_output_id AND key = 'n_topics' INTO n_topics;

  
  FOR i IN 1..(n_topics-1) LOOP -- leave one topic out because of collinearity
    var_names := var_names || 'cell_events_topic_' ||i||'_1, cell_events_topic_' ||i||'_2, ';
    aggr_block := aggr_block || 'MAX(CASE WHEN topic = ''cell_events_topic_' || i || ''' AND vals.cell_rank = 1 THEN value END) AS cell_events_topic_' 
	|| i || '_1 , MAX(CASE WHEN topic = ''cell_events_topic_' || i || ''' AND vals.cell_rank = 2 THEN value END) AS cell_events_topic_' 
	|| i || '_2,'; 
  END LOOP;
  
  var_names  := trim(TRAILING ', ' FROM var_names);
  aggr_block := trim(TRAILING ', ' FROM aggr_block);

  -- Values of new variables added to work.modelling_data (stack)
  query := 
  'INSERT INTO work.modelling_data_matrix_cell_events_topic
  ( mod_job_id, alias_id, '||var_names||' )
      SELECT
        '||in_mod_job_id||' AS mod_job_id,
        vals.alias_id,    
        '||aggr_block||'
      FROM (
        SELECT 
          mt.alias_id, 
          hmp.topic,
          hmp.value,
		  b.cell_rank
        FROM work.module_targets AS mt 
        LEFT JOIN 
          (
		  SELECT
		    alias_id,
			cell_id,
			rank() OVER (PARTITION BY alias_id ORDER BY sum(n_sub) DESC) AS cell_rank
		  FROM	  
		     (SELECT 
			  alias_a AS alias_id, 
			  a_cell_id AS cell_id,
			  count(*) AS n_sub		  
			  FROM data.cdr aa 
			  WHERE aa.a_cell_id IS NOT NULL 
			  AND aa.call_time >= to_timestamp('''||t1_text||''',''YYYY-MM-DD'') 
			  AND aa.call_time < to_timestamp('''||t2_text||''',''YYYY-MM-DD'')
			  GROUP BY a_cell_id, alias_a 
			  UNION ALL 
			  SELECT
			  alias_b AS alias_id, 
			  b_cell_id AS cell_id,
			  count(*) AS n_sub	 
			  FROM data.cdr bb 
			  INNER JOIN aliases.aliases_updated au
			  ON bb.alias_b = au.alias_id
			  WHERE au.alias_id =0
			  AND bb.b_cell_id IS NOT NULL
			  AND bb.call_time >= to_timestamp('''||t1_text||''',''YYYY-MM-DD'') 
			  AND bb.call_time < to_timestamp('''||t2_text||''',''YYYY-MM-DD'')
			  GROUP BY b_cell_id, alias_b ) kk
		    GROUP BY alias_id, cell_id  
		  )AS b
        ON mt.alias_id = b.alias_id
		LEFT JOIN 
          work.lda_output AS hmp
        ON b.cell_id = hmp.term -- because of possible formatting differences, remove spaces and convert to lowercase
        WHERE mt.mod_job_id = '||in_mod_job_id ||'
		AND b.cell_rank IN (1,2)
        AND hmp.lda_id = '||in_lda_output_id||'
      ) AS vals
      GROUP BY alias_id;';

  EXECUTE query;

  PERFORM core.analyze('work.modelling_data_matrix_cell_events_topic', in_mod_job_id);
END;

                        </code>
                    </input_parameters>
                    <output_parameters>
                        <out></out>
                    </output_parameters>
                </create_predictors_cell_events_model>
            </CalculateCommonPredictrs_MCI_nogeoloc>
            <split:run_products>
                <when>parameter with name equal to uc_inclue_product and value equal to true</when>
                <tasks>UC_Product_predictors, join</tasks>
                <when>parameter with name equal to uc_inclue_product and value equal to false</when>
                <tasks>join</tasks>
            </split:run_products>
            <UC_Product_predictors>
                <start></start>
                <end></end>
            </UC_Product_predictors>
            <split:run_churn_inactivity>
                <when>parameter with name equal to uc_inclue_churn_inactivity and value equal to true</when>
                <tasks>UC_ChurnInactivity_predictors_MCI, join</tasks>
                <when>parameter with name equal to uc_inclue_churn_inactivity and value equal to false</when>
                <tasks>join</tasks>
            </split:run_churn_inactivity>
            <UC_ChurnInactivity_predictors_MCI>
                <start></start>
                <end></end>
            </UC_ChurnInactivity_predictors_MCI>
            <split:run_churn_postpaid>
                <when>parameter with name equal to uc_inclue_churn_postpaid and value equal to true</when>
                <tasks>UC_ChurnPostpaid_predictors, join</tasks>
                <when>parameter with name equal to uc_inclue_churn_postpaid and value equal to false</when>
                <tasks>join</tasks>
            </split:run_churn_postpaid>
            <UC_ChurnPostpaid_predictors>
                <start></start>
                <end></end>
            </UC_ChurnPostpaid_predictors>
            <split:run_zero_day_prediction>
                <when>parameter with name equal to uc_include_zero_day_prediction and value equal to true</when>
                <tasks>UC_ZeroDayPrediction_predictors, join</tasks>
                <when>parameter with name equal to uc_include_zero_day_prediction and value equal to false</when>
                <tasks>join</tasks>
            </split:run_zero_day_prediction>
            <UC_ZeroDayPrediction_predictors>
                <Zero_day_prediction:create_predictors>
                    <input_parameters>
                        <DataSource1></DataSource1>
                        <queryparameters></queryparameters>
                        <sql>select * from work.uc_zero_day_prediction_create_predictors($mod_job_id)</sql>
                        <code>
                            
CREATE FUNCTION uc_zero_day_prediction_create_predictors(in_mod_job_id integer) RETURNS void
    AS $$

/* SUMMARY
 * This function will create predictors for zero day value segment prediction from raw CDR data.
 * Predictors that are currently not implemented in SQL and/or workflow: 
 * those based on received calls, LDA predictors, something else?
 *
 * INPUT
 * in_mod_job_id : Identifier of module job
 *
 * VERSION
 * 2014-06-06 HMa - ICIF-181 Stack removal
 * 2013-02-23 JVi - Moved to modules db schema functions
 * 2013-01-29 JVi - Created
 */

DECLARE

  max_var_id                 integer;
  uc_zero_day_prediction_t4  date;
  uc_zero_day_prediction_t5  date;
  call_type_map              record;


BEGIN

  -- Change this if the identifiers of call types differ from the default values
  SELECT 1 AS voice, 2 AS sms, 3 AS video, 4 AS mms, 5 AS "data" INTO call_type_map;

  uc_zero_day_prediction_t4 := m.value::date FROM work.module_job_parameters m 
                               WHERE m.mod_job_id = in_mod_job_id AND m.key = 'uc_zero_day_prediction_t4';
  uc_zero_day_prediction_t5 := m.value::date FROM work.module_job_parameters m 
                               WHERE m.mod_job_id = in_mod_job_id AND m.key = 'uc_zero_day_prediction_t5';

  -- First, insert predictors formed directly from raw CDR data

  
  INSERT INTO work.modelling_data_matrix_zdp1 (
    mod_job_id, 
    alias_id, 
    uc_zero_day_prediction_inactive_to_active_days_ratio,
    uc_zero_day_prediction_weekend_to_weekday_voice_count_ratio,
    uc_zero_day_prediction_evening_to_daytime_ratio_voice_made,
    uc_zero_day_prediction_nighttime_to_daytime_ratio_voice_made,
    uc_zero_day_prediction_average_duration_daytime_voice_made,
    uc_zero_day_prediction_average_duration_evening_voice_made,
    uc_zero_day_prediction_average_duration_nighttime_voice_made,
    uc_zero_day_prediction_average_duration_weekend_voice_made,
    uc_zero_day_prediction_average_duration_weekday_voice_made,
    uc_zero_day_prediction_average_daily_count_weekday_voice_made,
    uc_zero_day_prediction_average_daily_count_weekend_voice_made,
    uc_zero_day_prediction_average_daily_count_sms_made,
    uc_zero_day_prediction_sms_to_voice_ratio,
    uc_zero_day_prediction_average_first_voice_hour,
    uc_zero_day_prediction_average_daily_voice_hour
  )
  SELECT
    in_mod_job_id AS mod_job_id,
    ccdr.alias_id,
    ((uc_zero_day_prediction_t5 - min(ccdr."date"))::integer - count(*))::real 
      / CASE WHEN count(*) > 0 THEN count(*) ELSE NULL END                                                   AS inactive_to_active_days_ratio,
    sum(CASE WHEN weekday IN (0,6) AND count_voice_made IS NOT NULL THEN count_voice_made ELSE NULL END) 
      / sum(CASE WHEN weekday BETWEEN 1 AND 5 AND count_voice_made > 0
                 THEN count_voice_made ELSE NULL END)
      *
    sum(CASE WHEN weekday BETWEEN 1 AND 5 THEN 1 ELSE 0 END)::real
      / sum(CASE WHEN weekday IN (0,6) THEN 1 ELSE NULL END)                                         AS weekend_to_weekday_voice_count_ratio, --Weighted for weekdays and weekend days included in data
    coalesce(sum(count_evening_voice_made), 0)::real
      / sum(CASE WHEN count_daytime_voice_made > 0 THEN count_daytime_voice_made ELSE NULL END)      AS evening_to_daytime_ratio_voice_made,
    coalesce(sum(count_nighttime_voice_made), 0)::real
     / sum(CASE WHEN count_daytime_voice_made > 0 THEN count_daytime_voice_made ELSE NULL END)       AS nighttime_to_daytime_ratio_voice_made,
    sum(duration_daytime_voice_made) 
      / sum(CASE WHEN count_daytime_voice_made > 0 THEN count_daytime_voice_made ELSE NULL END)      AS average_duration_daytime_voice_made, 
    sum(duration_evening_voice_made) 
      / sum(CASE WHEN count_evening_voice_made > 0 THEN count_evening_voice_made ELSE NULL END)      AS average_duration_evening_voice_made, 
    sum(duration_nighttime_voice_made) 
      / sum(CASE WHEN count_nighttime_voice_made > 0 THEN count_nighttime_voice_made ELSE NULL END)  AS average_duration_nighttime_voice_made, 
    sum(duration_weekend_voice_made) 
      / sum(CASE WHEN count_weekend_voice_made > 0 THEN count_weekend_voice_made ELSE NULL END)      AS average_duration_weekend_voice_made, 
    sum(duration_weekday_voice_made) 
      / sum(CASE WHEN count_weekday_voice_made > 0 THEN count_weekday_voice_made ELSE NULL END)      AS average_duration_weekday_voice_made,
    coalesce(sum(count_weekday_voice_made), 0)::real 
      / sum(CASE WHEN count_weekday_voice_made > 0 THEN 1 ELSE NULL END)                             AS average_daily_count_weekday_voice_made,
    coalesce(sum(count_weekend_voice_made), 0)::real
      / sum(CASE WHEN count_weekend_voice_made > 0 THEN 1 ELSE NULL END)                             AS average_daily_count_weekend_voice_made,
    coalesce(sum(count_sms_made), 0)::real 
      / (uc_zero_day_prediction_t5 - min(ccdr."date"))                                               AS average_daily_count_sms_made,
    coalesce(sum(count_sms_made), 0)::real 
      / (CASE WHEN sum(count_voice_made) > 0 THEN sum(count_voice_made) ELSE NULL END)               AS sms_to_voice_ratio,
    avg(first_voice_hour)                                                                            AS average_first_voice_hour,
    sum(average_voice_hour * count_voice_made) 
      / (CASE WHEN sum(count_voice_made) > 0 THEN sum(count_voice_made) ELSE NULL END)               AS average_daily_voice_hour
  FROM ( --ccdr -- Note: this is effectively a daily aggregates table and can be replaced with one that is filled in the data loading phase
    SELECT DISTINCT
      cdr.alias_a                                                                                        AS alias_id,
      cdr."date"                                                                                         AS date,
      sum(CASE WHEN cdr.call_type = call_type_map.sms   THEN 1 ELSE NULL END)::integer                   AS count_sms_made,
      sum(CASE WHEN cdr.is_daytime_voice = 1     THEN 1 ELSE NULL END )::integer                         AS count_daytime_voice_made,
      sum(CASE WHEN cdr.is_evening_voice = 1     THEN 1 ELSE NULL END )::integer                         AS count_evening_voice_made,
      sum(CASE WHEN cdr.is_nighttime_voice = 1   THEN 1 ELSE NULL END )::integer                         AS count_nighttime_voice_made,
      sum(CASE WHEN cdr.is_weekend_voice = 1     THEN 1 ELSE NULL END )::integer                         AS count_weekend_voice_made, --Note:Friday evening included
      sum(CASE WHEN cdr.is_weekday_voice = 1     THEN 1 ELSE NULL END )::integer                         AS count_weekday_voice_made, --Note:Friday evening not included
      sum(CASE WHEN cdr.is_daytime_voice = 1     THEN call_length ELSE NULL END )::real                  AS duration_daytime_voice_made,
      sum(CASE WHEN cdr.is_evening_voice = 1     THEN call_length ELSE NULL END )::real                  AS duration_evening_voice_made,
      sum(CASE WHEN cdr.is_nighttime_voice = 1   THEN call_length ELSE NULL END )::real                  AS duration_nighttime_voice_made,
      sum(CASE WHEN cdr.is_weekend_voice = 1     THEN call_length ELSE NULL END )::real                  AS duration_weekend_voice_made, --Note:Friday evening included
      sum(CASE WHEN cdr.is_weekday_voice = 1     THEN call_length ELSE NULL END )::real                  AS duration_weekday_voice_made,
      min(CASE WHEN cdr.call_type = call_type_map.voice THEN cdr.call_hour ELSE NULL END)::integer       AS first_voice_hour,
      avg(CASE WHEN cdr.call_type = call_type_map.voice THEN cdr.call_hour ELSE NULL END)::integer       AS average_voice_hour,
      count(CASE WHEN cdr.call_type = call_type_map.voice THEN 1 ELSE NULL END)::integer                 AS count_voice_made,
      EXTRACT("dow" FROM cdr."date")::integer                                                            AS weekday
    FROM --cdr 
    work.module_targets AS mt
    LEFT JOIN ( 
      SELECT 
        *,
        CASE WHEN c.call_type = call_type_map.voice 
      AND  EXTRACT("hour" FROM c.call_time) BETWEEN 8 AND 16
      AND  EXTRACT("dow" FROM c.call_time)  BETWEEN 1 AND  5
             THEN 1 ELSE 0 END                                                          AS is_daytime_voice,
        CASE WHEN c.call_type = call_type_map.voice 
             AND  EXTRACT("hour" FROM c.call_time) BETWEEN 17 AND 21
             AND  EXTRACT("dow" FROM c.call_time)  BETWEEN 1 AND  4
             THEN 1 ELSE 0 END                                                          AS is_evening_voice,
        CASE WHEN c.call_type = call_type_map.voice 
                AND (    EXTRACT("hour" FROM c.call_time) NOT BETWEEN 8 AND 21
                     AND EXTRACT("dow" FROM c.call_time)  BETWEEN 1 AND  4 )
                OR  (    EXTRACT ("hour" FROM c.call_time) < 8
                     AND EXTRACT("dow" FROM c.call_time)   = 5 )
             THEN 1 ELSE 0 END                                                          AS is_nighttime_voice,
        CASE WHEN c.call_type = call_type_map.voice 
                AND  ((  EXTRACT("dow"  FROM c.call_time)  BETWEEN 1 AND 5 )
                     AND NOT (    EXTRACT("dow"  FROM c.call_time)  = 5  
                              AND EXTRACT("hour" FROM c.call_time)  > 16 ))
            THEN 1 ELSE 0 END                                                           AS is_weekday_voice,
        CASE WHEN c.call_type = call_type_map.voice 
                AND  ((  EXTRACT("dow"  FROM c.call_time)  NOT BETWEEN 1 AND 5 )
                      OR (    EXTRACT("dow"  FROM c.call_time)  = 5  
                          AND EXTRACT("hour" FROM c.call_time)  > 16 ))
            THEN 1 ELSE 0 END                                                           AS is_weekend_voice,
        c.call_time::date                                                               AS date,
        EXTRACT("hour" FROM c.call_time)                                                AS call_hour
      FROM data.cdr AS c
    ) AS cdr
    ON mt.alias_id = cdr.alias_a
    WHERE mt.mod_job_id = in_mod_job_id 
    AND mt.audience_zero_day_prediction IS NOT NULL 
    AND cdr.call_time >= uc_zero_day_prediction_t4 AND cdr.call_time < uc_zero_day_prediction_t5
    GROUP BY cdr.alias_a, cdr."date"
  ) AS ccdr
  GROUP BY ccdr.alias_id;


        
  -- Then, insert predictors formed from topup data
  
  INSERT INTO work.modelling_data_matrix_zdp2 (
    mod_job_id, 
    alias_id, 
    uc_zero_day_prediction_average_daily_topup_count,
    uc_zero_day_prediction_total_topup_count_capped_at_two,
    uc_zero_day_prediction_average_topup_cost
  )
  SELECT
    in_mod_job_id                                                                              AS mod_job_id                          ,
    tt.alias_id                                                                                AS alias_id                            ,
    coalesce(sum(tt.topup_count),0) / (uc_zero_day_prediction_t5 - switch_on_date)             AS average_daily_topup_count           ,     
    least(coalesce(sum(tt.topup_count),0), 2)                                                  AS total_topup_count_capped_at_two     ,
    sum(tt.total_daily_topup_cost) 
      / sum(CASE WHEN tt.topup_count > 0 THEN tt.topup_count ELSE NULL END)                    AS average_topup_cost             
  FROM (
    SELECT
      t.charged_id           AS alias_id                ,
      count(*)               AS topup_count             ,
      sum(t.topup_cost)      AS total_daily_topup_cost  ,
      t."timestamp"::date    AS topup_date              ,
      min(mt.switch_on_date) AS switch_on_date 
    FROM (
      SELECT mt0.alias_id, min(c.switch_on_date) AS switch_on_date --If for some reason there are more than one switch on date
      FROM
        work.module_targets AS mt0
        LEFT JOIN data.in_crm  AS c
        ON mt0.alias_id = c.alias_id
        WHERE mt0.mod_job_id = in_mod_job_id
        AND mt0.audience_zero_day_prediction IS NOT NULL
        AND c.switch_on_date BETWEEN uc_zero_day_prediction_t4 AND uc_zero_day_prediction_t5 - 1 -- t5 - 1 day because BETWEEN is inclusive
        GROUP BY mt0.alias_id
    ) mt
    LEFT JOIN data.topup AS t
    ON mt.alias_id = t.charged_id
    WHERE t."timestamp" >= uc_zero_day_prediction_t4 AND t."timestamp" < uc_zero_day_prediction_t5
    GROUP BY t.charged_id, topup_date
  ) as tt
  GROUP BY tt.alias_id, switch_on_date;




  --Insert predictors from CRM data 

  INSERT INTO work.modelling_data_matrix_zdp4 (
    mod_job_id, 
    alias_id, 
    uc_zero_day_prediction_tenure,
    uc_zero_day_prediction_tenure_group,
    uc_zero_day_prediction_activation_weekday
  )
  SELECT
    in_mod_job_id AS mod_job_id,
    cc.alias_id,
    uc_zero_day_prediction_t5 - cc.switch_on_date AS tenure,
    CASE WHEN uc_zero_day_prediction_t5 - cc.switch_on_date BETWEEN 0 AND 2 THEN 1 
         WHEN uc_zero_day_prediction_t5 - cc.switch_on_date BETWEEN 3 AND 5 THEN 2 
         WHEN uc_zero_day_prediction_t5 - cc.switch_on_date BETWEEN 6 AND 7 THEN 3
         ELSE NULL END AS tenure_group,
    CASE EXTRACT("dow" FROM cc.switch_on_date)
      WHEN 0 THEN 'Sunday'
      WHEN 1 THEN 'Monday'
      WHEN 2 THEN 'Tuesday'
      WHEN 3 THEN 'Wednesday'
      WHEN 4 THEN 'Thursday'
      WHEN 5 THEN 'Friday'
      WHEN 6 THEN 'Saturday'
      ELSE NULL
      END AS activation_weekday
  FROM (
    SELECT DISTINCT
      c.alias_id,
      min(c.switch_on_date) AS switch_on_date --If for some reason there are more than one switch on date
    FROM data.in_crm AS c
    LEFT JOIN work.module_targets AS mt
    ON c.alias_id = mt.alias_id
    WHERE mt.mod_job_id = in_mod_job_id 
    AND mt.audience_zero_day_prediction IS NOT NULL 
    AND c.switch_on_date BETWEEN uc_zero_day_prediction_t4 AND uc_zero_day_prediction_t5 - 1 -- t5 - 1 day because BETWEEN is inclusive
    GROUP BY c.alias_id
  ) AS cc;

END;

                        </code>
                    </input_parameters>
                    <output_parameters>
                        <out></out>
                    </output_parameters>
                </Zero_day_prediction:create_predictors>
            </UC_ZeroDayPrediction_predictors>
            <split:run_portout_prediction>
                <when>parameter with name equal to uc_include_portout and value equal to true</when>
                <tasks>UC_Portout_predictors_MCI, join</tasks>
                <when>parameter with name equal to uc_include_portout and value equal to false</when>
                <tasks>join</tasks>
            </split:run_portout_prediction>
            <UC_Portout_predictors_MCI>
                <start></start>
                <end></end>
            </UC_Portout_predictors_MCI>
            <Combine_modelling_data>
            </Combine_modelling_data>
            <network_statistics_chart>
                <input_parameters>
                    <DataSource1></DataSource1>
                    <queryparameters></queryparameters>
                    <sql>select * from work.social_network_statistics($network_id,$mod_job_id)</sql>
                    <code>
                        
CREATE FUNCTION social_network_statistics(netjob_id integer, in_mod_job_id integer) RETURNS void
    AS $$
/*
 * SUMMARY
 * 
 * Fills in the information about social network to network statistics chart.
 * 
 * VERSION
 * 27.02.2013 HMa - Only run in the apply phase. data_date = t2
 * 04.02.2013 KL
 */
	
DECLARE

  job_type text;
  t2 date;

BEGIN

  SELECT value INTO job_type
  FROM work.module_job_parameters
  WHERE key='run_type'
  AND mod_job_id=in_mod_job_id;

  SELECT value INTO t2
  FROM work.module_job_parameters
  WHERE key='t2'
  AND mod_job_id=in_mod_job_id;

  -- Churn statistics will only be calculated if the in_mod_job_id corresponds to apply period. 
  IF job_type='Predictors + Apply' THEN 


    INSERT INTO charts.chart_data(mod_job_id, stat_name, group_name, data_date, var_name, var_value, order_id)
    SELECT in_mod_job_id, 'NETWORK_STATS', 'ALL', t2, 'Total number of subscribers in network', count(*), 11 
    FROM work.out_scores 
    WHERE job_id=netjob_id;
    
    
    INSERT INTO charts.chart_data(mod_job_id, stat_name, group_name, data_date, var_name, var_value, order_id)
    SELECT in_mod_job_id, 'NETWORK_STATS', 'ALL', t2, 'Total number of on-net subscribers', count(*), 12 
    FROM work.out_scores  network 
    JOIN (
      SELECT alias_id 
      FROM (
        SELECT alias_id 
        FROM data.in_crm 
        UNION 
        SELECT charged_id 
        FROM data.topup 
        WHERE is_credit_transfer is false 
      ) a
      GROUP BY alias_id
    ) onnet 
    ON network.alias_id=onnet.alias_id 
    WHERE job_id=netjob_id;
    
    
    INSERT INTO charts.chart_data(mod_job_id, stat_name, group_name, data_date, var_name, var_value, order_id)
    SELECT in_mod_job_id, 'NETWORK_STATS', 'ALL', t2, 'Total number of on-net active subscribers', count(*) AS value, 14 
    FROM work.out_scores a 
    JOIN work.modelling_data_matrix b 
    ON a.alias_id=b.alias_id 
    WHERE job_id=netjob_id 
    AND mod_job_id=in_mod_job_id;
    
    INSERT INTO charts.chart_data(mod_job_id, stat_name, group_name, data_date, var_name, var_value, order_id)
    SELECT in_mod_job_id, 'NETWORK_STATS', 'ALL', t2, 'Percentage of on-net subscribers', round((act_subs/all_subs*100)::numeric,2), 13 
    FROM (
      SELECT var_value AS all_subs 
      FROM charts.chart_data 
      WHERE mod_job_id=in_mod_job_id
      AND var_name='Total number of subscribers in network'
    ) b 
    CROSS JOIN (
      SELECT var_value AS act_subs 
      FROM charts.chart_data 
      WHERE mod_job_id=in_mod_job_id
      AND var_name='Total number of on-net subscribers'
    ) c;
    
    
    INSERT INTO charts.chart_data(mod_job_id, stat_name, group_name, data_date, var_name, var_value, order_id)
    SELECT in_mod_job_id, 'NETWORK_STATS', 'ALL', t2, 'Percentage of on-net active subscribers', round((act_subs/all_subs*100)::numeric,2), 15 
    FROM (
      SELECT var_value AS all_subs 
      FROM charts.chart_data 
      WHERE mod_job_id=in_mod_job_id
      AND var_name='Total number of subscribers in network'
    ) b 
    CROSS JOIN (
      SELECT var_value AS act_subs 
      FROM charts.chart_data 
      WHERE mod_job_id=in_mod_job_id
      AND var_name='Total number of on-net active subscribers'
    ) c;

  END IF;

END;

                    </code>
                </input_parameters>
                <output_parameters>
                    <out></out>
                </output_parameters>
            </network_statistics_chart>
        </run_predictors_MCI>
        <split:descriptive_variables>
            <when>parameter with name equal to calculate_descvar and value equal to true</when>
            <tasks>CalculateDescriptiveVariables_MCI_unforcenewnet, join</tasks>
            <when>parameter with name equal to calculate_descvar and value equal to true</when>
            <tasks>join</tasks>
        </split:descriptive_variables>
        <CalculateDescriptiveVariables_MCI_unforcenewnet>
            <email>
                <input_parameters>
                    <command>echo "starting descvar" + $t2 | mailx -v -r "churn@mci.ir" -s "FAA scoring run" -S smtp="mail.mci.ir:25" -S smtp-use-starttls -S smtp-auth=login -S smtp-auth-user="churn@mci.ir -S smtp-auth-password="CHch@4782" -S ssl-verify=ignore -S nss-config-dir="/etc/pki/nssdb" ext.ho.ashtari@mci.or"</command>
                    <host>localhost</host>
                    <identity></identity>
                    <password>$LocalhostPassword</password>
                    <std.err.file></stderr.file>
                    <stdout.file></stdout.file>
                    <timeout>20000</timeout>
                    <username>$LocalhostUsername</username>
                </input_parameters>
                <output_parameters>
                    <exitstatus></exitstatus>
                    <stderr></stderr>
                    <stdout></stdout>
                </output_parameters>
            </email>
            <unforced_network>
                <input_parameters>
                </input_parameters>
                <output_parameters>
                    <force_new_network>false</force_new_network>
                </output_parameters>
            </unforced_network>
            <initialize_descriptive_variables_workflow>
            </initialize_descriptive_variables_workflow>
            <split>
                <when>parameter with name equal to network_exists and value equal to -1</when>
                <tasks>Calculate_Monthly_Arpu, NetworkScorerTask, join</tasks>
                <when>parameter with name equal to network_exists and value not equal to -1</when>
                <tasks>join</tasks>
            </split>
            <Calculate_Monthly_Arpu>
                <input_parameters>
                    <DataSource1></DataSource1>
                    <queryparameters></queryparameters>
                    <sql>select * from work.calculate_monthly_arpu($module_id_descvar)</sql>
                    <code>
                        
CREATE FUNCTION calculate_monthly_arpu(in_mod_job_id integer) RETURNS void
    AS $$
/* SUMMARY:
 * This function calculates normalized (30 days) monthly arpu for specified source period for prepaid and postpaid subscribers, as well as the arpu segmentations
 * Function also takes into account the length of the subscription and for new customer.
 * Inputs:
 *
 * in_mod_job_id             mod_job_id of the job
 *
 * 2014-06-19 QY
 * 2013-06-28 KL 
 */

DECLARE
  --Variables and parameters
  t1                          date; --source period start
  t2                          date; --source period end
  tcrm                        date;
  

BEGIN

  SELECT value::date INTO t2
  FROM work.module_job_parameters a
  WHERE a.key = 't2'
  AND a.mod_job_id = in_mod_job_id;

  SELECT value::date INTO t1
  FROM work.module_job_parameters a
  WHERE a.key = 't1'
  AND a.mod_job_id = in_mod_job_id;

  SELECT value::date INTO tcrm
  FROM work.module_job_parameters a
  WHERE lower(a.key) = 'tcrm'
  AND a.mod_job_id = in_mod_job_id;

  INSERT INTO work.monthly_arpu
    (mod_job_id, alias_id, monthly_arpu,segmentation) 
  SELECT 
    in_mod_job_id AS mod_job_id,
    alias_id,
    monthly_arpu, 
    CASE WHEN s.intile = 1 THEN 'Low value'
         WHEN s.intile = 2 THEN 'Medium value'
         WHEN s.intile = 3 THEN 'High value'
    END AS segmentation
  FROM (	   
    SELECT
      alias_id,
      max(arpu) AS monthly_arpu,
      -- rank() OVER (ORDER BY max(arpu) ASC)  
     NTILE(3) OVER (ORDER BY max(arpu) ASC) AS intile
    FROM (
      SELECT
        charged_id AS alias_id, 
        COALESCE(sum(credit_amount), 0) * 30 / (CASE WHEN (t2 - switch_on_date) < (t2 - t1) THEN t2 - switch_on_date ELSE t2 - t1 END) AS arpu
      FROM data.topup a
      JOIN (
        SELECT alias_id, max(switch_on_date) AS switch_on_date
        FROM data.in_crm
        WHERE switch_on_date < t2
        GROUP BY alias_id
      ) b
      ON a.charged_id = b.alias_id
      WHERE
        is_credit_transfer = false
        AND a.timestamp < t2
        AND a.timestamp >= t1
        GROUP BY a.charged_id, b.switch_on_date
      UNION ALL
      SELECT
        alias_id,
        -- CASE WHEN subscriber_value > 0 THEN subscriber_value
        -- ELSE 0 END  AS arpu  
        0 AS arpu 
      FROM data.in_crm
      WHERE payment_type = 'postpaid'
      AND date_inserted = tcrm
    ) a
    GROUP BY alias_id
  ) s;

END;

                    </code>
                </input_parameters>
                <output_parameters>
                    <out></out>
                </output_parameters>
            </Calculate_Monthly_Arpu>
            <NetworkScorerTask>
            </NetworkScorerTask>
            <Fill_Subscriber_List>
                <input_parameters>
                    <DataSource1></DataSource1>
                    <queryparameters></queryparameters>
                    <sql>select * from work.descriptive_variables_fill_subscriber_list($module_id_descvar)</sql>
                    <code>
                        
CREATE FUNCTION descriptive_variables_fill_subscriber_list(in_mod_job_id integer) RETURNS SETOF integer
    AS $$
/*
 * This function runs creates the target list and fills the table 
 * descriptive_variables_subscriberlist, which contains the alias_id's 
 * for which the descriptive variables will be calculated.
 *
 * VERSION
 *
 * 2.1.2013KL bug 545 (ICIF-37), ICIF-56, ICIF-77
 * 14.12. 2012 KL corrected missing start and end dates
  * 20.02.2011 LBe 
 */
DECLARE
  netjobid integer = (SELECT value::integer FROM work.module_job_parameters WHERE mod_job_id = in_mod_job_id and key = 'xsl_job_id');
  start_date date = (SELECT value::date FROM work.module_job_parameters WHERE mod_job_id = in_mod_job_id and key = 't1');
  end_date date = (SELECT value::date FROM work.module_job_parameters WHERE mod_job_id = in_mod_job_id and key = 't2');
  from_mod_job_id integer= (SELECT value::integer FROM work.module_job_parameters WHERE mod_job_id = in_mod_job_id and key = 'in_mod_job_id');
BEGIN

  IF (SELECT count(*) FROM work.aliases WHERE job_id = netjobid) = 0 THEN
    PERFORM work.fetch_aliases(netjobid, start_date, end_date);
  END IF;


  IF (SELECT count(*) FROM work.module_targets WHERE mod_job_id = from_mod_job_id) = 0 THEN
    SELECT min(b.mod_job_id) INTO from_mod_job_id
    FROM work.module_targets a
    JOIN work.module_job_parameters b
    ON a.mod_job_id = b.mod_job_id   
    WHERE b.key = 't2'
    AND b.value::date = end_date; 
  END IF;

  TRUNCATE TABLE tmp.descriptive_variables_subscriberlist;

  INSERT INTO tmp.descriptive_variables_subscriberlist (alias_id)
  SELECT m.alias_id 
  FROM (
    SELECT a.alias_id 
    FROM work.module_targets m 
    JOIN work.aliases a 
    ON m.alias_id=a.alias_id 
    WHERE mod_job_id = from_mod_job_id 
    AND a.job_id=netjobid 
  ) m
  LEFT JOIN (
    SELECT alias_id 
    FROM work.networkscorer_blacklist a 
    WHERE job_id=netjobid
  ) a 
  ON a.alias_id=m.alias_id 
  WHERE a.alias_id IS NULL
  GROUP BY m.alias_id;

 END;

                    </code>
                </input_parameters>
                <output_parameters>
                    <out></out>
                </output_parameters>
            </Fill_Subscriber_List>
            <get_temp_out_network>
                <input_parameters>
                    <DataSource1></DataSource1>
                    <queryparameters></queryparameters>
                    <sql>select * from work.descriptive_variables_temporary_networks($network_id_descvar)</sql>
                    <code>
                        
CREATE FUNCTION descriptive_variables_temporary_networks(netjob_id integer) RETURNS SETOF integer
    AS $$
/*
 * SUMMARY
 * Fill the table tmp.out_network_raw for the descriptive variables computations. 
 * 
 * VERSION
 * 18.02.2013 MOj ICIF-107
 * 2.1.2013 KL bug 545 (ICIF-37), ICIF-56, ICIF-77
 * 18.12.2012 KL
 * 21.02.2011 LBe
 */
BEGIN

 /*****************************************************************************
   * Copy the primary network into tmp table. Most of the descriptive variables 
   * are calculated FROM the this network. 
   */

  TRUNCATE TABLE tmp.out_network_raw;

  INSERT INTO tmp.out_network_raw (alias_a, alias_b, job_id, weight)
  SELECT alias_a, alias_b, aa.job_id, weight
  FROM work.out_network aa
  LEFT OUTER JOIN work.networkscorer_blacklist bb 
  ON aa.job_id = bb.job_id AND aa.alias_a = bb.alias_id
  LEFT OUTER JOIN work.networkscorer_blacklist cc 
  ON aa.job_id = cc.job_id AND aa.alias_b = cc.alias_id
  WHERE aa.job_id = netjob_id 
  AND bb.alias_id IS NULL 
  AND cc.alias_id IS NULL;

  PERFORM core.analyze('tmp.out_network_raw', netjob_id);

END;  

                    </code>
                </input_parameters>
                <output_parameters>
                    <out></out>
                </output_parameters>
            </get_temp_out_network>
            <calculate_network_roles>
                <input_parameters>
                    <DataSource1></DataSource1>
                    <queryparameters></queryparameters>
                    <sql>select * from work.calculate_network_roles_tmp_network($network_id_descvar)</sql>
                    <code>
                        
CREATE FUNCTION calculate_network_roles_tmp_network(integer) RETURNS integer
    AS $_$
/* Version
 * 2.1.2013 KL corrected a bug: maximum is taken now from numeric value not from text
 * <4.12.2012 Someone
 */   
DECLARE
  xsljobid ALIAS for $1;
  klimit integer; 
  klimitpos integer;  
  wlimit double precision;
  wlimitpos integer;
BEGIN
  DELETE FROM work.network_roles 
  WHERE job_id = xsljobid;
  
  SELECT (count(*)*0.1 )::integer INTO klimitpos 
  FROM work.out_scores 
  WHERE job_id = xsljobid;

  SELECT (count(*)*0.9)::integer INTO wlimitpos 
  FROM tmp.out_network_raw;

  SELECT min(k) INTO klimit 
  FROM (
    SELECT k 
    FROM work.out_scores 
    WHERE job_id = xsljobid 
    ORDER BY k DESC 
    LIMIT klimitpos
  ) aa;

  SELECT max(weight) INTO wlimit 
  FROM (
    SELECT weight 
    FROM tmp.out_network_raw 
    ORDER BY weight
    LIMIT wlimitpos
  ) aa;

  TRUNCATE tmp.nwr_linkcount;
  TRUNCATE tmp.nwr_linkcount_tmp;
  TRUNCATE tmp.hublist;
  TRUNCATE tmp.bridgelist;
  TRUNCATE tmp.network_roles_fix;

  INSERT INTO tmp.nwr_linkcount_tmp (alias_id, lc) 
  SELECT 
    alias_a, 
    CASE WHEN weight > wlimit THEN 1 ELSE 0 END 
  FROM tmp.out_network_raw;

  INSERT INTO tmp.nwr_linkcount_tmp (alias_id, lc) 
  SELECT 
    alias_b, 
    CASE WHEN weight > wlimit THEN 1 ELSE 0 END 
  FROM tmp.out_network_raw;
 
  INSERT INTO tmp.nwr_linkcount (alias_id, lc) 
  SELECT alias_id, sum(lc) 
  FROM tmp.nwr_linkcount_tmp 
  GROUP BY alias_id;

  INSERT INTO tmp.hublist 
  SELECT alias_id 
  FROM (
    SELECT aa.alias_id 
    FROM tmp.out_network_raw oo 
    JOIN tmp.nwr_linkcount aa ON aa.alias_id = oo.alias_a 
    JOIN tmp.nwr_linkcount bb ON bb.alias_id = oo.alias_b 
    WHERE aa.lc > klimit AND bb.lc <= klimit
    UNION 
    SELECT bb.alias_id 
    FROM tmp.out_network_raw oo
    JOIN tmp.nwr_linkcount aa ON aa.alias_id = oo.alias_a 
    JOIN tmp.nwr_linkcount bb ON bb.alias_id = oo.alias_b
    WHERE aa.lc <= klimit AND bb.lc > klimit
  ) bb
  GROUP BY bb.alias_id;
  
  INSERT INTO tmp.network_roles_fix (job_id,alias_id, ishub, isbridge, isoutlier)
  SELECT 
    xsljobid,
    aa.alias_id,
    CASE WHEN aa.lc > klimit THEN 1 ELSE 0 END AS ishub,
    CASE WHEN bb.sumk > 1 THEN 1 ELSE 0 END AS isbridge,
    CASE WHEN aa.lc < 2 THEN 1 ELSE 0 END AS isoutlier
  FROM tmp.nwr_linkcount aa, (
    SELECT alias_id, sum(sumk) AS sumk 
    FROM (
      SELECT dd.alias_a AS alias_id, count(*) AS sumk  
      FROM tmp.hublist cc 
      JOIN tmp.out_network_raw dd ON dd.alias_b = cc.alias_id
      GROUP BY dd.alias_a 
      UNION ALL
      SELECT dd.alias_b AS alias_id, count(*) AS sumk  
      FROM tmp.hublist cc
      JOIN tmp.out_network_raw dd ON dd.alias_a = alias_id 
      GROUP BY dd.alias_b 
    ) cc 
    GROUP BY alias_id
  ) bb 
  WHERE aa.alias_id = bb.alias_id;

  INSERT INTO tmp.bridgelist
  SELECT alias_id 
  FROM (
    SELECT alias_id 
    FROM (
      SELECT cc.alias_id, oo1.alias_b, oo2.alias_a 
      FROM tmp.network_roles_fix cc 
      JOIN tmp.out_network_raw oo1 ON oo1.alias_a = cc.alias_id    
      JOIN tmp.network_roles_fix cc1 ON cc1.alias_id = oo1.alias_b 
      JOIN tmp.out_network_raw oo2 ON oo2.alias_b = cc.alias_id 
      JOIN tmp.network_roles_fix cc2 ON cc2.alias_id = oo2.alias_a 
      WHERE cc.isbridge=1 AND cc.ishub=0 AND cc1.ishub=1 AND cc2.ishub=1 AND cc1.alias_id <> cc2.alias_id 
      AND cc.job_id = xsljobid AND oo1.job_id = xsljobid AND oo2.job_id = xsljobid
    ) x1
    LEFT OUTER JOIN tmp.out_network_raw oo ON (oo.alias_a = x1.alias_a AND oo.alias_b = x1.alias_b) 
    WHERE oo.alias_a IS NULL 
    UNION 
    SELECT alias_id 
    FROM ( 
      SELECT cc.alias_id, oo1.alias_b, oo2.alias_a 
      FROM tmp.network_roles_fix cc 
      JOIN tmp.out_network_raw oo1 ON oo1.alias_a = cc.alias_id 
      JOIN tmp.network_roles_fix cc1 ON cc1.alias_id = oo1.alias_b 
      JOIN tmp.out_network_raw oo2 ON oo2.alias_b = cc.alias_id 
      JOIN tmp.network_roles_fix cc2 ON cc2.alias_id = oo2.alias_a 
      WHERE cc.isbridge=1 AND cc.ishub=0 AND cc1.alias_id <> cc2.alias_id AND cc1.ishub=1 AND cc2.ishub=1 AND cc.job_id = xsljobid
    ) x2
    LEFT OUTER JOIN tmp.out_network_raw oo ON (oo.alias_b = x2.alias_a AND oo.alias_a = x2.alias_b) 
    WHERE oo.alias_a IS NULL
  ) x0
  GROUP BY alias_id;

  INSERT INTO work.network_roles (job_id,alias_id, ishub, isbridge, isoutlier)
  SELECT 
    xsljobid, aa.alias_id, aa.ishub,
    CASE WHEN bb.alias_id IS NOT NULL THEN 1 ELSE 0 END AS isbridge,
    aa.isoutlier
  FROM tmp.network_roles_fix aa 
  LEFT OUTER JOIN tmp.bridgelist bb ON aa.alias_id = bb.alias_id;

  RETURN wlimit::integer;
  
END; 

                    </code>
                </input_parameters>
                <output_parameters>
                    <out></out>
                </output_parameters>
            </calculate_network_roles>
            <calculate_descriptive_variables_base_1>
                <input_parameters>
                    <DataSource1></DataSource1>
                    <queryparameters></queryparameters>
                    <sql>select * from work.calculate_descriptive_variables_base_1($module_id_descvar, $network_id_descvar)</sql>
                    <code>
                        
CREATE FUNCTION calculate_descriptive_variables_base_1(in_mod_job_id integer, netjob_id integer) RETURNS integer
    AS $$
/* Function work.calculate_descriptive_variables_base_1()
 * 
 * SUMMARY
 * The descriptive variables that are computed in function work.calculate_descriptive_variables_base_2
 * are more time-intensive and require the availability of a network the is computed on both 
 * on-net as off-net subscribers. The function work.calculate_descriptive_variables_base_1 can be used 
 * after a basis network computation, for example for the UI Target and Analyse jobs.
 * Limit for new subscribers is 30 days
 * 
 * All descriptive variables which are computed using the tables:
 * -- work.out_network (tmp.out_network_raw)
 * -- work.out_scores
 * can be found in the function work.calculate_descriptive_variables_base_2
 * 
 * VERSION
 * 21.8.2013 QYu added data usage related variables (call type = 5)
 * 5.2.2013 KL added another input variable so that also network roles will be filled
 * 2.1.2013 KL bug 545 (ICIF-37), ICIF-56, ICIF-77
 * 05.10.2011 LBe fixed bug 732
 * 15.04.2011 LBe fixed bug 532
 * 22.02.2011 LBe
 * < 22.02.2011 MAk
 */
DECLARE
  core_job_id integer := -1 ;
  net_job_id integer := -1 ;
  num_weeks integer := 1 ;
  retval integer= 1;
  in_var_id integer;
  t1 date;
  t2 date;
  weeklist text;  
  new_limit integer = 30; -- new subscribers are subscribers for which t2- crm.switch_on_date < new_limit
  this_date date;
  var_id_i integer;
  mb_scale integer:=1;
BEGIN
  SELECT to_date(mjp.value, 'YYYY-MM-DD') INTO t1
  FROM work.module_job_parameters AS mjp
  WHERE mjp.mod_job_id = in_mod_job_id AND mjp.key = 't1';

  SELECT to_date(mjp.value, 'YYYY-MM-DD') INTO t2
  FROM work.module_job_parameters AS mjp
  WHERE mjp.mod_job_id = in_mod_job_id AND mjp.key = 't2';

  num_weeks:=floor((t2-t1)/7);
  
  this_date := t2; --(monday end of source period)

  INSERT INTO tmp.descriptive_variables_subscriber_matrix_base_1a
  (mod_job_id, 
   alias_id, 
   neigh_count_rec,
   neigh_count_made,
   voice_rec_duration,
   neigh_count_made_weekly,
   neigh_count_rec_weekly,
   voice_made_nmb,
   voice_rec_nmb,
   sms_made_nmb,
   sms_rec_nmb,
   share_of_voice,
   voice_made_duration,
   voice_made_nmb_day,
   voice_made_nmb_eve,
   voice_made_nmb_night,
   voice_made_nmb_weekday,
   voice_made_nmb_weekend,
   voice_made_duration_day,
   voice_made_duration_eve,
   voice_made_duration_night,
   voice_made_duration_weekday,
   voice_made_duration_weekend,
   sms_made_day,
   sms_made_eve,
   sms_made_night,
   sms_made_weekday,
   sms_made_weekend,
   data_usage_day,
   data_usage_eve,
   data_usage_night,
   data_usage_weekday,
   data_usage_weekend,
   weekly_data_usage,
   weekly_data_usage_cost)
--   weekly_cost,
--   share_of_data_usage)
  SELECT 
    in_mod_job_id, 
    aa.alias_id, 
    aa.who_called                  AS neigh_count_rec,
    aa.ids_called                  AS neigh_count_made,
    aa.duration                    AS voice_rec_duration,
    nn.weekly_called               AS neigh_count_made_weekly,
    nn.weekly_who_called           AS neigh_count_rec_weekly,
    nn.made_calls                  AS voice_made_nmb,
    nn.received_calls              AS voice_rec_nmb,
    nn.sent_sms                    AS sms_made_nmb,
    nn.received_sms                AS sms_rec_nmb,
    nn.share_voice_activity        AS share_of_voice,
    nn.duration_made               AS voice_made_duration,
    nn.voice_calls_day             AS voice_made_nmb_day,
    nn.voice_calls_evening         AS voice_made_nmb_eve,
    nn.voice_calls_night           AS voice_made_nmb_night,
    nn.voice_calls_weekday         AS voice_made_nmb_weekday,
    nn.voice_calls_weekend         AS voice_made_nmb_weekend,
    nn.voice_calls_day_vol         AS voice_made_duration_day,
    nn.voice_calls_evening_vol     AS voice_made_duration_eve,
    nn.voice_calls_night_vol       AS voice_made_duration_night,
    nn.voice_calls_weekday_vol     AS voice_made_duration_weekday,
    nn.voice_calls_weekend_vol     AS voice_made_duration_weekend,
    nn.sent_sms_day                AS sms_made_day,
    nn.sent_sms_evening            AS sms_made_eve,
    nn.sent_sms_night              AS sms_made_night,
    nn.sent_sms_weekday            AS sms_made_weekday,
    nn.sent_sms_weekend            AS sms_made_weekend,
    nn.data_usage_day              AS data_usage_day,
    nn.data_usage_eve              AS data_usage_eve,
    nn.data_usage_night            AS data_usage_night,
    nn.data_usage_weekday          AS data_usage_weekday,
    nn.data_usage_weekend          AS data_usage_weekend,
    nn.weekly_data_usage           AS weekly_data_usage,
    nn.weekly_data_usage_cost      AS weekly_data_usage_cost
  --nn.weekly_cost                 AS weekly_cost,
  --nn.share_of_data_usage         AS share_of_data_usage
  FROM (
    SELECT 
      t.alias_id, 
      t.who_called, 
      u.ids_called, 
      t.duration 
    FROM (
      SELECT 
        a.alias_id, 
        COALESCE(count(distinct oo.alias_a), 0) AS who_called,   
        COALESCE(sum(oo.v_s), 0)::double precision / (60.0 * num_weeks) AS duration  
      FROM tmp.descriptive_variables_subscriberlist a
      JOIN data.in_split_weekly oo 
	ON a.alias_id = oo.alias_b
      LEFT JOIN work.networkscorer_blacklist oo2 
      ON oo.alias_a=oo2.alias_id AND oo2.job_id=netjob_id
      LEFT JOIN work.networkscorer_blacklist oo1 
      ON oo.alias_b=oo1.alias_id AND oo1.job_id=netjob_id
      WHERE oo.period >= core.date_to_yyyyww(t1) AND oo.period < core.date_to_yyyyww(t2)
      AND oo1.alias_id IS NULL AND oo2.alias_id IS NULL
      GROUP BY a.alias_id
    ) t  
    JOIN (
      SELECT 
        a.alias_id, 
        COALESCE(count(distinct oo.alias_b), 0) AS ids_called   
      FROM tmp.descriptive_variables_subscriberlist a
      JOIN data.in_split_weekly oo 
	ON a.alias_id = oo.alias_a
      LEFT JOIN work.networkscorer_blacklist oo2 
      ON oo.alias_a=oo2.alias_id  AND oo2.job_id=netjob_id 
      LEFT JOIN work.networkscorer_blacklist oo1 
      ON oo.alias_b=oo1.alias_id  AND oo1.job_id=netjob_id
      WHERE oo.period >= core.date_to_yyyyww(t1) AND oo.period < core.date_to_yyyyww(t2)
      AND oo1.alias_id IS NULL AND oo2.alias_id IS NULL
      GROUP BY a.alias_id 
    ) u 
    ON u.alias_id=t.alias_id
  ) aa  
  JOIN (
    SELECT 
      bb.alias_id AS aa_i, 
      COALESCE(sum(gg.mc_alias_count), 0.0)::double precision / num_weeks  AS weekly_called,
      COALESCE(sum(gg.rc_alias_count), 0.0)::double precision / num_weeks  AS weekly_who_called,
      COALESCE(sum(CASE WHEN voicecount > 0 THEN voicecount ELSE 0 END)::double precision, 0) / num_weeks AS made_calls,
      COALESCE(sum(CASE WHEN rc_voicecount > 0 THEN rc_voicecount ELSE 0 END)::double precision, 0) / num_weeks AS received_calls,
      COALESCE(sum(CASE WHEN smscount > 0 THEN smscount ELSE 0 END), 0)::double precision / num_weeks AS sent_sms,
      COALESCE(sum(CASE WHEN rc_smscount > 0 THEN rc_smscount ELSE 0 END), 0)::double precision / num_weeks received_sms,
      COALESCE(CASE WHEN sum(smscount + voicecount) > 0 THEN sum(voicecount)::double precision / sum(smscount + voicecount) ELSE 0 END) AS share_voice_activity,
      COALESCE(sum(CASE WHEN voicesum > 0 THEN voicesum ELSE 0 END)::double precision, 0) / (60.0 * num_weeks) AS duration_made,
      COALESCE(sum(voicecountday), 0)::double precision / num_weeks AS voice_calls_day,
      COALESCE(sum(voicecountevening), 0)::double precision / num_weeks AS voice_calls_evening,
      COALESCE(sum(voicecount - voicecountday - voicecountevening), 0)::double precision / num_weeks AS voice_calls_night,
      COALESCE(sum(voicecountweekday), 0)::double precision / num_weeks AS voice_calls_weekday,
      COALESCE(sum(voicecount - voicecountweekday), 0)::double precision / num_weeks AS voice_calls_weekend,
      COALESCE(sum(voicesumday), 0)::double precision / (60.0 * num_weeks) AS voice_calls_day_vol,
      COALESCE(sum(voicesumevening), 0)::double precision / (60.0 * num_weeks) AS voice_calls_evening_vol,
      COALESCE(sum(voicesum - voicesumday - voicesumevening), 0)::double precision / (60.0 * num_weeks) voice_calls_night_vol,
      COALESCE(sum(voicesumweekday), 0)::double precision / (60.0 * num_weeks) AS voice_calls_weekday_vol,
      COALESCE(sum(voicesum - voicesumday), 0)::double precision / (60.0 * num_weeks)  AS voice_calls_weekend_vol,
      COALESCE(sum(smscountday), 0)::double precision / num_weeks AS sent_sms_day,
      COALESCE(sum(smscountevening), 0)::double precision / num_weeks AS sent_sms_evening,
      COALESCE(sum(smscount - smscountday - smscountevening), 0)::double precision / num_weeks AS sent_sms_night,
      COALESCE(sum(smscountweekday), 0)::double precision / num_weeks AS sent_sms_weekday,
      COALESCE(sum(smscount - smscountweekday), 0)::double precision / num_weeks sent_sms_weekend,
	COALESCE(sum(data_usage_sumday), 0)::double precision / (mb_scale * num_weeks) AS data_usage_day,
      COALESCE(sum(data_usage_sumevening), 0)::double precision / (mb_scale * num_weeks) AS data_usage_eve,
    COALESCE(sum(weekly_data_usage_sum - data_usage_sumday-data_usage_sumevening), 0)::double precision / (mb_scale * num_weeks)  AS data_usage_night,
      COALESCE(sum(data_usage_sumweekday), 0)::double precision / (mb_scale * num_weeks) AS data_usage_weekday,
      COALESCE(sum(weekly_data_usage_sum - data_usage_sumweekday), 0)::double precision / (mb_scale * num_weeks)  AS data_usage_weekend,
    COALESCE(sum(weekly_data_usage_sum), 0)::double precision / (mb_scale * num_weeks)  AS weekly_data_usage,
	COALESCE(sum(weekly_data_usage_cost_sum), 0)::double precision / (mb_scale * num_weeks)  AS weekly_data_usage_cost
	--COALESCE(sum(weekly_cost_sum), 0)::double precision / (mb_scale * num_weeks)  AS weekly_cost,
    --COALESCE(CASE WHEN sum(smscount + voicecount) > 0 THEN sum( 0.8 * sqrt(weekly_data_usage_sum))::double precision / sum(smscount + voicecount) ELSE 0 END) AS share_of_data_usage
    FROM tmp.descriptive_variables_subscriberlist bb
    JOIN data.in_split_aggregates gg 
    ON bb.alias_id = gg.alias_id
    WHERE gg.period >=core.date_to_yyyyww(t1) AND gg.period <core.date_to_yyyyww(t2)
    GROUP BY bb.alias_id
  ) nn 
  ON aa.alias_id=nn.aa_i;
  
  INSERT INTO tmp.descriptive_variables_subscriber_matrix_base_1b (mod_job_id, alias_id, new_subs_within_connections)
  SELECT 
    in_mod_job_id, 
    aa.alias_a, 
    sum(case when cc.switch_on_date is not null and (cc.switch_on_date > this_date::date - new_limit) THEN 1 ELSE 0 END) AS new_subs_within_connections
  FROM tmp.out_network_raw aa
  JOIN tmp.descriptive_variables_subscriberlist sl ON aa.alias_a = sl.alias_id 
  JOIN (
    SELECT alias_id, max(switch_on_date) AS switch_on_date
    FROM data.in_crm 
    WHERE switch_on_date IS NOT NULL 
    AND switch_on_date < t2
    GROUP BY alias_id
  ) cc
  ON aa.alias_b = cc.alias_id 
  GROUP BY aa.alias_a;

  INSERT INTO tmp.descriptive_variables_subscriber_matrix_base_1c (mod_job_id, alias_id, social_role)
  SELECT 
    in_mod_job_id, 
    aa.alias_id,
    CASE 
      WHEN ishub = 1 THEN 'Hub'
      WHEN isbridge = 1 THEN 'Bridge'
      WHEN isoutlier = 1 THEN 'Outlier' 
    END AS social_role
  FROM work.network_roles aa
  JOIN tmp.descriptive_variables_subscriberlist sl ON aa.alias_id = sl.alias_id 
  WHERE aa.job_id = netjob_id;
  
  RETURN retval;
END;

                    </code>
                </input_parameters>
                <output_parameters>
                    <out></out>
                </output_parameters>
            </calculate_descriptive_variables_base_1>
            <email>
                <input_parameters>
                    <command>echo "starting base2 descvar" + $t2 | mailx -v -r "churn@mci.ir" -s "FAA scoring run" -S smtp="mail.mci.ir:25" -S smtp-use-starttls -S smtp-auth=login -S smtp-auth-user="churn@mci.ir -S smtp-auth-password="CHch@4782" -S ssl-verify=ignore -S nss-config-dir="/etc/pki/nssdb" ext.ho.ashtari@mci.or"</command>
                    <host>localhost</host>
                    <identity></identity>
                    <password>$LocalhostPassword</password>
                    <std.err.file></stderr.file>
                    <stdout.file></stdout.file>
                    <timeout>20000</timeout>
                    <username>$LocalhostUsername</username>
                </input_parameters>
                <output_parameters>
                    <exitstatus></exitstatus>
                    <stderr></stderr>
                    <stdout></stdout>
                </output_parameters>
            </email>
            <calculate_descriptive_variables_base_2>
                <input_parameters>
                    <DataSource1></DataSource1>
                    <queryparameters></queryparameters>
                    <sql>select * from work.calculate_descriptive_variables_base_2($module_id_descvar, $network_id_descvar)</sql>
                    <code>
                        
CREATE FUNCTION calculate_descriptive_variables_base_2(in_mod_job_id integer, net_job_id integer) RETURNS integer
    AS $$
/* Function work.calculate_descriptive_variables_base_2(integer, integer, integer)
 * 
 * SUMMARY
 * The descriptive variables that are computed in function work.calculate_descriptive_variables_base_2
 * are more time-intensive and require the availability of a network that is computed on both 
 * on-net as off-net subscribers. The function work.calculate_descriptive_variables_base_1 can be used 
 * after a basic network computation, for example for the UI Target and Analyse jobs.
 * 
 * All descriptive variables which are computed using the tables:
 * -- work.out_network (tmp.out_network_raw)
 * -- work.out_scores
 * -- data.in_split_weekly and work.aliases
 * can be found in the function work.calculate_descriptive_variables_base_2
 * 
 * Note! This function is an addition to the function calculate_descriptive_variables_base_1. 
 * 
 * VERSION
 * 14.06.2013 HMa ICIF-146
 * 18.02.2013 MOj ICIF-107
 * 24.01.2013 HMa Added in_delayed_mob_job_id to parameters to check churners within connections correctly
 * 22.01.2013 HMa changed 'target' to 'target_churn_inactivity'
 * 2.1.2013 KL bug 545 (ICIF-37), ICIF-56, ICIF-77
 * 18.09.2012 HMa Bug ICIF-70
 * 29.06.2012 HMa Bug 890
 * 26.10.2011 LBe Bug 760
 * 15.04.2011 LBe Bug 532
 * 22.02.2011 LBe
 * < 22.02.2011 MAk
 */
DECLARE
  core_job_id integer := -1 ;
  num_weeks integer := 1 ;
  t1 date;
  t2 date;
 
  -- the variable 'null_values_as_churn': 
  --   == 0 if subscribers for which module_results.target = 'null' are _NOT_ considered as churners
  --   == 1 if subscribers for which module_results.target = 'null' are considered as churners
  null_values_as_churn  integer = 1;   
 
  retval integer= 1;
  in_var_id integer;
BEGIN

  SELECT to_date(mjp.value, 'YYYY-MM-DD') INTO t1
  FROM work.module_job_parameters AS mjp
  WHERE mjp.mod_job_id = in_mod_job_id AND mjp.key = 't1';
 
  SELECT to_date(mjp.value, 'YYYY-MM-DD') INTO t2
  FROM work.module_job_parameters AS mjp
  WHERE mjp.mod_job_id = in_mod_job_id AND mjp.key = 't2';

  num_weeks:=floor((t2-t1)/7);
  
  INSERT INTO tmp.descriptive_variables_subscriber_matrix_base_2a 
  (mod_job_id,
   alias_id,
   social_revenue,
   community_id,
   connectedness_of_connections,
   connections,
   social_connectivity_score,
   offnet_connections_of_onnet_connections,
   connections_of_onnet_connections,
   offnet_connections,
   share_of_offnet_connections)
  SELECT
    in_mod_job_id                        AS mod_job_id,
    ac.alias_id                          AS alias_id,
    ac.socrev                            AS social_revenue,
    ac.community_id                      AS community_id,
    ac.C                                 AS connectedness_of_connections,
    ac.k                                 AS connections,
    ac.alpha                             AS social_connectivity_score,
    ab.off_conn_onnet_conn               AS offnet_connections_of_onnet_connections,
    ab.conn_onnet_conn                   AS connections_of_onnet_connections,
    ad.off_net_conn                      AS offnet_connections,
    ad.share_off_net_conn                AS share_of_offnet_connections
  FROM (
    SELECT 
      aa.alias_id, 
      aa.community_id AS community_id, 
      alpha,  
      COALESCE(k, 0) AS k, 
      COALESCE(c, 0) AS C, 
      COALESCE(socrev, 0) AS socrev
    FROM work.out_scores aa 
    JOIN tmp.descriptive_variables_subscriberlist sl
    ON aa.alias_id = sl.alias_id
    WHERE aa.job_id = net_job_id 
  ) ac
  LEFT  JOIN (
    SELECT 
      sl.alias_id AS a_i,
      count(distinct cc.alias_b) AS conn_onnet_conn, 
      count(distinct (CASE WHEN bb.in_out_network = 0 THEN cc.alias_b ELSE NULL END)) AS off_conn_onnet_conn
    FROM tmp.descriptive_variables_subscriberlist sl 
    JOIN tmp.out_network_raw dd ON sl.alias_id = dd.alias_a
    JOIN tmp.out_network_raw cc ON cc.alias_a = dd.alias_b
    JOIN work.aliases aa ON aa.alias_id = cc.alias_a 
    JOIN work.aliases bb ON bb.alias_id = cc.alias_b AND aa.job_id = bb.job_id
    WHERE aa.job_id = net_job_id 
    AND aa.in_out_network = 1 -- require subscribers' connections to be on-net
    GROUP BY sl.alias_id
  ) ab  
  ON ab.a_i=ac.alias_id
  LEFT JOIN (
    SELECT 
      sl.alias_id AS al_id, 
      COALESCE(sum(1-bb.in_out_network), 0) AS off_net_conn, 
      CASE 
        WHEN (sum(bb.in_out_network) + sum(1 - bb.in_out_network)) > 0 THEN 
          COALESCE(sum(1-bb.in_out_network), 0)::double precision / (sum(bb.in_out_network) + sum(1 - bb.in_out_network)) 
        ELSE 0 
      END AS share_off_net_conn
    FROM work.aliases bb
    JOIN tmp.out_network_raw cc ON bb.alias_id = cc.alias_b
    JOIN tmp.descriptive_variables_subscriberlist sl ON alias_a = sl.alias_id
    WHERE bb.job_id = net_job_id 
    GROUP BY sl.alias_id
  ) AS ad 
  ON ad.al_id=alias_id; 

  INSERT INTO  tmp.descriptive_variables_subscriber_matrix_base_2b (mod_job_id, alias_id, no_connections) 
  SELECT 
    in_mod_job_id AS mod_job_id, 
    aa.alias_id,
    CASE WHEN ss.k IS  NULL THEN 1 ELSE 0 END AS no_connections
  FROM tmp.descriptive_variables_subscriberlist aa 
  LEFT OUTER JOIN (
    SELECT alias_id, k 
    FROM work.out_scores 
    WHERE k>0 
    AND job_id = net_job_id
  ) ss 
  ON aa.alias_id = ss.alias_id;

  -- *** TODO check and discuss definition ***    
  INSERT INTO  results.descriptive_variables_community_matrix 
  (mod_job_id, 
   community_id, 
   "Linkedness within community",
   "Intracommunication ratio",
   "Communication density")
  SELECT
    in_mod_job_id,
    af.community_id,
    af.linkedness            AS "Linkedness within community",
    af.intra_communication   AS "Intracommunication ratio",
    ae.communication_density AS "Communication density"
  FROM (
    SELECT 
      aa.community_id,  
      sum(
        CASE 
          WHEN cc.weight > 0 AND aa.community_id = bb.community_id 
          THEN cc.weight
          ELSE 0 
        END
      ) / (
        1 + sum(
          CASE 
            WHEN cc.weight > 0 AND aa.community_id != bb.community_id 
            THEN cc.weight
            ELSE 0 
          END
        )
      ) AS intra_communication, 
      CASE 
        WHEN sum(CASE WHEN aa.community_id != bb.community_id THEN 1 ELSE 0 END) > 0 
        THEN 
          sum(CASE WHEN aa.community_id = bb.community_id THEN 1 ELSE 0 END)::double precision / 
          sum(CASE WHEN aa.community_id != bb.community_id THEN 1 ELSE 0 END)::double precision
        ELSE 0 
      END AS linkedness
    FROM work.out_scores aa
    JOIN work.out_scores bb ON aa.job_id = bb.job_id
    JOIN tmp.out_network_raw cc ON aa.alias_id = cc.alias_a AND bb.alias_id = cc.alias_b
    JOIN tmp.descriptive_variables_subscriberlist sl ON aa.alias_id = sl.alias_id
    WHERE bb.job_id = net_job_id         
    GROUP BY aa.community_id
  ) af
  JOIN (
    SELECT 
      aa.community_id, 
      CASE 
        WHEN cc.weight > 0 AND aa.community_id = bb.community_id 
        THEN avg(cc.weight) 
        ELSE NULL 
      END AS communication_density 
    FROM work.out_scores aa
    JOIN work.out_scores bb ON aa.job_id = bb.job_id
    JOIN tmp.out_network_raw cc ON aa.alias_id = cc.alias_a AND bb.alias_id = cc.alias_b
    JOIN tmp.descriptive_variables_subscriberlist sl ON aa.alias_id = sl.alias_id
    WHERE bb.job_id = net_job_id
    GROUP by aa.community_id, cc.weight, bb.community_id --THIS IS WRONG, SEE BUG ICIF-95!!
  ) ae
  ON af.community_id = ae.community_id; 

  /* Descriptive variable 'Share of on-net calls among calls made'
   * -------------------------------------------------------------
   * fraction (# voice_calls+SMSses made during the source period 
   * to on-net subscribers)/(# voice_calls+SMSses made during the 
   * source period)
   */
 
  INSERT INTO tmp.descriptive_variables_subscriber_matrix_base_2c (
    mod_job_id, 
    alias_id, 
    share_of_onnet_made
  )
  SELECT 
    in_mod_job_id,
    alias_id, 
    CASE 
      WHEN (coalesce(n_calls_innet,0) + coalesce(n_calls_outnet,0)) > 0 
        THEN coalesce(n_calls_innet,0) / (coalesce(n_calls_innet,0) + coalesce(n_calls_outnet,0))::double precision
      ELSE 0
    END AS share_of_onnet_made
  FROM (
    SELECT
      alias_id, 
      sum(CASE WHEN in_out_network = 1 THEN call_count ELSE NULL END) AS n_calls_innet,
      sum(CASE WHEN in_out_network = 0 THEN call_count ELSE NULL END) AS n_calls_outnet
    FROM (
      SELECT
        sl.alias_id,
        ia.in_out_network,
        ia.voicecount + ia.smscount AS call_count
      FROM tmp.descriptive_variables_subscriberlist AS sl 
      INNER JOIN data.in_split_aggregates as ia
      ON sl.alias_id = ia.alias_id
      WHERE ia.period >= core.date_to_yyyyww(t1) 
        AND ia.period < core.date_to_yyyyww(t2)
    ) AS p
    GROUP BY p.alias_id
  ) aa;

  /* Descriptive variable 'Share of on-net calls among calls received'
   * -----------------------------------------------------------------
   * fraction (# voice_calls+SMSses received during the source period 
   * FROM on-net subscribers)/(# voice_calls+SMSses received during 
   * the source period)
   */
 
  INSERT INTO tmp.descriptive_variables_subscriber_matrix_base_2d (
    mod_job_id, 
    alias_id, 
    share_of_onnet_rec
  )
  SELECT 
    in_mod_job_id,
    alias_id, 
    CASE 
      WHEN (coalesce(n_calls_innet,0) + coalesce(n_calls_outnet,0)) > 0 
        THEN coalesce(n_calls_innet,0) / (coalesce(n_calls_innet,0) + coalesce(n_calls_outnet,0))::double precision
      ELSE 0
    END AS share_of_onnet_rec
  FROM (
    SELECT
      alias_id, 
      sum(CASE WHEN in_out_network = 1 THEN call_count ELSE NULL END) AS n_calls_innet,
      sum(CASE WHEN in_out_network = 0 THEN call_count ELSE NULL END) AS n_calls_outnet
    FROM (
      SELECT
        sl.alias_id,
        ia.in_out_network,
        ia.rc_voicecount + ia.rc_smscount AS call_count
      FROM tmp.descriptive_variables_subscriberlist AS sl 
      INNER JOIN data.in_split_aggregates as ia
      ON sl.alias_id = ia.alias_id
      WHERE ia.period >= core.date_to_yyyyww(t1) 
        AND ia.period < core.date_to_yyyyww(t2)
    ) AS p
    GROUP BY p.alias_id
  ) aa;

  RETURN retval;
END;

                    </code>
                </input_parameters>
                <output_parameters>
                    <out></out>
                </output_parameters>
            </calculate_descriptive_variables_base_2>
            <calculate_descriptive_variables_topup>
                <input_parameters>
                    <DataSource1></DataSource1>
                    <queryparameters></queryparameters>
                    <sql>select * from work.calculate_descriptive_variables_topup($module_id_descvar)</sql>
                    <code>
                        
CREATE FUNCTION calculate_descriptive_variables_topup(in_mod_job_id integer) RETURNS integer
    AS $$
/* -- Copyright (c) 2011 Xtract Oy. All rights reserved --
 *         -- Use is subject to license terms --
 * 
 * 
 * SUMMARY
 * Supplementary function for the descriptive variables. This function 
 * is called FROM the function 'work.calculate_descriptive_variables'.
 * 
 * INPUT
 * module job identifier
 * 
 * OUTPUT
 * integer: -1 when the is something wrong, else the module job id
 *
 * VERSION
 * 07.11.2017 ZY Follow Iranian localization (Saturday as first DOW)
 * 02.01.2013 KL bug 545 (ICIF-37), ICIF-56, ICIF-77
 * 24.10.2011 LBe
 */
DECLARE
  retval integer := 1;
  core_job_id integer := -1 ;
  net_job_id integer := -1 ;
  num_weeks integer := 1 ;
  t1 date;
  t2 date;
BEGIN
 SELECT to_date(mjp.value, 'YYYY-MM-DD') INTO t1
 FROM work.module_job_parameters AS mjp
 WHERE mjp.mod_job_id = in_mod_job_id AND mjp.key = 't1';

 SELECT to_date(mjp.value, 'YYYY-MM-DD') INTO t2
 FROM work.module_job_parameters AS mjp
 WHERE mjp.mod_job_id = in_mod_job_id AND mjp.key = 't2';

 num_weeks := floor((t2 - t1) / 7);

  IF (num_weeks IS NULL OR num_weeks = 0) THEN 
    SELECT 1 INTO num_weeks;
  END IF;

  INSERT INTO tmp.descriptive_variables_subscriber_matrix_topup (
    mod_job_id, 
    alias_id,
    topup_count,
    topup_amount,
    topup_days_from_last,
    topup_typical,
    topup_shareof_typical,
    topup_count_firsthalf_lastmonth,
    topup_count_lasthalf_lastmonth,
    topup_count_day,
    topup_count_eve,
    topup_count_night,
    topup_count_weekday,
    topup_count_weekend ) 
  SELECT 
    in_mod_job_id                                                                         AS mod_job_id,
    aa.alias_id                                                                           AS alias_id,
    aa.topup_number                                                                       AS topup_count,
    aa.topup_avg                                                                          AS topup_amount,
    aa.num_days_last_topup                                                                AS topup_days_from_last,
    aa.typical_topup                                                                      AS topup_typical,
    aa.num_typical_topup::double precision / GREATEST(aa.topup_number, 1)                 AS topup_shareof_typical,
    aa.num_topups_first_half_last_month                                                   AS topup_count_firsthalf_lastmonth,
    aa.num_topups_last_half_last_month                                                    AS topup_count_lasthalf_lastmonth,
    aa.topup_daytime::double precision / num_weeks                                        AS topup_count_day,
    aa.topup_evening::double precision / num_weeks                                        AS topup_count_eve,
    (aa.topup_number - aa.topup_daytime - aa.topup_evening)::double precision / num_weeks AS topup_count_night,
    aa.topup_weekday::double precision / num_weeks                                        AS topup_count_weekday,
    (aa.topup_number - aa.topup_weekday)::double precision / num_weeks                    AS topup_count_weekend 
  FROM (
    SELECT *
    FROM (
      SELECT
        a.alias_id,
        -- Total number of top-ups during the source period:
        coalesce(SUM(CASE WHEN b.credit_amount > 0 AND b.timestamp >= t1 THEN 1 ELSE 0 END), 0)::double precision AS topup_number,
        -- Average top-up amount during the source period:
        coalesce(SUM(CASE WHEN b.timestamp >= t1 THEN b.credit_amount ELSE 0 END), 0)::double precision / greatest(SUM(CASE WHEN b.timestamp >= t1 AND b.credit_amount > 0 THEN 1 ELSE 0 END), 1) AS topup_avg,
        -- Number of days between the last top-up and t2:
        min(t2 - b.timestamp::date) AS num_days_last_topup,
        -- Typical topup amount during the source period:
        avg(CASE WHEN c.row_number = 1 THEN c.credit_amount ELSE NULL END) AS typical_topup,  
        -- Number of topup's with the typical topup amount during the source period:
        avg(CASE WHEN c.row_number = 1 THEN c.count ELSE NULL END)::double precision AS num_typical_topup,
        -- Number of topup's during days 1-15 of last (whole) month before t2
        sum(CASE WHEN b.credit_amount > 0 AND b.timestamp >= date_trunc('month',(SELECT t2 - interval '1 month'))::date 
          AND b.timestamp < (SELECT date_trunc('month',(SELECT t2 - interval '1 month'))::date + interval '15 days')::date
          THEN 1 ELSE 0 END) AS num_topups_first_half_last_month,
        -- Number of topup's during days 16-end of last (whole) month before t2
        sum(CASE WHEN b.credit_amount > 0 AND b.timestamp < date_trunc('month',t2)::date 
          AND b.timestamp >= (SELECT date_trunc('month',(SELECT t2 - interval '1 month'))::date + interval '15 days')::date
          THEN 1 ELSE 0 END) AS num_topups_last_half_last_month,
        -- Number of topups during the source period during daytime, 
        sum(CASE WHEN b.credit_amount > 0 AND b.timestamp >= t1 
          AND b.timestamp::time >= '07:00:00' AND b.timestamp::time < '17:00:00'
          THEN 1 ELSE 0 END)::double precision AS topup_daytime,  
        -- Number of topups during the source period during evenings, 
        sum(CASE WHEN b.credit_amount > 0 AND b.timestamp >= t1 
          AND b.timestamp::time >= '17:00:00' AND b.timestamp::time < '22:00:00'
          THEN 1 ELSE 0 END)::double precision AS topup_evening, 
        -- Number of topups during weekdays:
        sum(CASE WHEN b.credit_amount > 0 AND b.timestamp >= t1
          AND timestamp < (date_trunc('week', timestamp + interval '2 days') + interval '2 days' + interval '17 hours')
          THEN 1 ELSE 0 END) AS topup_weekday
      FROM tmp.descriptive_variables_subscriberlist AS a
      LEFT JOIN ( 
        SELECT * FROM data.topup WHERE timestamp < t2
      ) AS b
      ON a.alias_id = b.charged_id
      LEFT JOIN ( -- DV's related to the 'typical' (most frequent) topup amount during the source period
        SELECT 
          charged_id, 
          credit_amount, 
          count(*) AS count,
          row_number() over (PARTITION BY charged_id ORDER BY count(*) DESC, credit_amount DESC)
        FROM data.topup
        WHERE timestamp >= t1 AND timestamp < t2
        GROUP BY charged_id, credit_amount
      ) AS c
      ON a.alias_id = c.charged_id
      WHERE c.row_number = 1
      GROUP BY a.alias_id
    ) b
  ) aa;

  RETURN in_mod_job_id;

END;

                    </code>
                </input_parameters>
                <output_parameters>
                    <out></out>
                </output_parameters>
            </calculate_descriptive_variables_topup>
            <combine_descriptive_variables_to_one_matrix>
            </combine_descriptive_variables_to_one_matrix>
            <descriptice_variables_to_charts>
                <input_parameters>
                    <DataSource1></DataSource1>
                    <queryparameters></queryparameters>
                    <sql>select * from work.add_descriptive_variables_values_to_chart_tables($module_id_descvar,$mod_job_id)</sql>
                    <code>
                        

CREATE FUNCTION add_descriptive_variables_values_to_chart_tables(in_mod_job_id integer, target_list_mod_job_id integer) RETURNS void
    AS $$

/*
 * SUMMARY
 * 
 * Adds the values (averages) of descriptive variables needed for charts at the end of descriptive variables workflow
 * This is the function for churn inactivity!
 * 
 * VERSION
 * 30.07.2014 JVi Fixed bug where wrong mod job id was used for monthly arpu retrieval in some cases
 * 30.07.2014 JVi Modifications related to fix done for ICIF-217 (removed reference to table work.descriptive_variables_subscriber)
 * 17.06.2014 JVi Added support for ZDP (otherwise would crash if only ZDP was included in run)
 * 20.06.2013 AVe When calculating All/Mature/New diff factors, audience condition is calculated using use cases into account 
 * 18.04.2013 HMa ICIF-126
 * 22.02.2013 HMa Made the descriptive variables calculation dynamic so that all the variables found in table work.differentiating_factors are calculated. 
 * 18.02.2013 MOj ICIF-107
 * 04.02.2013 KL
 * 13.09.2017 LZu portout for MCI
 */

DECLARE

  t2 date;
  marpu_mod_job_id integer;
  tcrm date;
  i integer;
  query text;
  query_all text := '';
  query_new text := '';
  query_mature text := '';
  this_long_name text;
  this_var_name text;
  target_t2 date;
  products text;
  product_arr text[];
  p_id integer; 
  this_product_id text;
  query_product text = '';
  audience_text text;
  job_use_cases text;
  use_cases text[];
  use_case text;

BEGIN

  SELECT to_date(mjp.value, 'YYYY-MM-DD') INTO t2
  FROM work.module_job_parameters AS mjp
  WHERE mjp.mod_job_id = in_mod_job_id AND mjp.key = 't2';

  SELECT to_date(mjp.value, 'YYYY-MM-DD') INTO tcrm
  FROM work.module_job_parameters AS mjp
  WHERE mjp.mod_job_id = in_mod_job_id AND lower(mjp.key) = 'tcrm';
  
  SELECT value INTO products
  FROM work.module_job_parameters
  WHERE key='uc_product_products'
  AND mod_job_id=target_list_mod_job_id;
  
  SELECT to_date(mjp.value, 'YYYY-MM-DD') INTO target_t2
  FROM work.module_job_parameters AS mjp
  WHERE mjp.mod_job_id = target_list_mod_job_id AND mjp.key = 't2';
  
  SELECT value INTO job_use_cases
  FROM work.module_job_parameters AS mjp
  WHERE mjp.mod_job_id = target_list_mod_job_id AND mjp.key = 'job_use_cases';

  -- For monthly ARPU, use data that has same t2 as descvar run and same mod job id as 
  -- either descvar run or target list run
  SELECT MAX(mjp.mod_job_id) INTO marpu_mod_job_id
  FROM work.module_job_parameters AS mjp
  INNER JOIN ( 
    SELECT DISTINCT(mod_job_id) FROM work.monthly_arpu
    WHERE mod_job_id IN (in_mod_job_id, target_list_mod_job_id)
  ) ma
  USING (mod_job_id)
  WHERE mjp.key = 't2' AND mjp.value::date = t2;

  query_all := '';
  query_new := '';
  query_mature := '';
  audience_text := '(';

  use_cases := string_to_array(trim(job_use_cases),',');
  
  FOR uc_ind IN 1..array_upper(use_cases, 1) LOOP
    
    use_case := use_cases[uc_ind];
    IF use_case ~ 'churn' THEN
      audience_text := audience_text || ' c.audience_' || use_case || '= 1 OR';
    END IF;
    IF use_case ~ 'product' THEN
      audience_text := audience_text || ' c.audience_' || use_case || '= 1 OR';
    END IF;
    IF use_case ~ 'zero_day_prediction' THEN
      audience_text := audience_text || ' c.audience_' || use_case || '= 1 OR';
    END IF;
    IF use_case ~ 'portout' THEN
      audience_text := audience_text || ' c.audience_' || use_case || '= 1 OR';
    END IF;
    
  END LOOP;
  
  audience_text := trim(TRAILING ' OR' FROM audience_text);
  audience_text := audience_text || ')';
  
  FOR i IN
    SELECT var_id FROM work.differentiating_factors
  LOOP

    this_long_name := long_name FROM work.differentiating_factors WHERE var_id = i;
    this_var_name  := var_name  FROM work.differentiating_factors WHERE var_id = i;

    IF this_long_name = 'Typical top-up amount' THEN
      query_all    := query_all    || 'WHEN long_name = '''||this_long_name||''' THEN avg('||this_var_name||') '; -- no coalesce for typical top-up amount (because no top-up is not amount 0)
      query_new    := query_new    || 'WHEN long_name = '''||this_long_name||''' THEN avg('||this_var_name||' * new_1) ';
      query_mature := query_mature || 'WHEN long_name = '''||this_long_name||''' THEN avg('||this_var_name||' * mature) ';
    ELSE
      query_all    := query_all    || 'WHEN long_name = '''||this_long_name||''' THEN avg(coalesce('||this_var_name||', 0)) ';
      query_new    := query_new    || 'WHEN long_name = '''||this_long_name||''' THEN avg(coalesce('||this_var_name||', 0) * new_1) ';
      query_mature := query_mature || 'WHEN long_name = '''||this_long_name||''' THEN avg(coalesce('||this_var_name||', 0) * mature) ';
    END IF;

  END LOOP;

  query := '
  INSERT INTO charts.chart_data(mod_job_id, stat_name, group_name, data_date, var_name, var_value, order_id)
  SELECT
    '||in_mod_job_id||' AS mod_job_id,
    df.stat_name,
    u.gr AS group_name, 
    '''||t2||'''::date AS data_date,
    bb.var_name,
    CASE 
      WHEN u.gr = ''ALL'' THEN bb.average_all
      WHEN u.gr = ''NEW'' THEN bb.average_new_1
      WHEN u.gr = ''MATURE'' THEN bb.average_mature
      ELSE NULL
    END AS var_value,
    df.order_id
  FROM (
    SELECT
      CASE 
        '||query_all||'
      ELSE NULL END AS average_all, 
      CASE 
        '||query_new||'
      ELSE NULL END AS average_new_1, 
      CASE 
        '||query_mature||'
      ELSE NULL END AS average_mature, 
      long_name AS var_name
    FROM (
      SELECT 
        af.*,
        keys.long_name,
        keys.var_name,
        CASE WHEN b.switch_on_date>'''||t2||'''::date - 30 THEN 1 
          ELSE NULL END AS new_1, 
        CASE WHEN b.switch_on_date<'''||t2||'''::date - 90 THEN 1 
          ELSE NULL END AS mature 
      FROM results.descriptive_variables_subscriber_matrix AS af
      INNER JOIN work.module_targets c 
      ON  af.alias_id=c.alias_id
      INNER JOIN data.in_crm b 
      ON af.alias_id = b.alias_id 
      CROSS JOIN
      (SELECT long_name, var_name FROM work.differentiating_factors) AS keys
      WHERE af.mod_job_id = '||in_mod_job_id||'
      AND c.mod_job_id = '||target_list_mod_job_id||'
      AND '|| audience_text ||'
      AND b.switch_on_date IS NOT NULL
      AND b.date_inserted = '''||tcrm||'''::date
    ) aa
    GROUP BY long_name
  ) bb
  INNER JOIN work.differentiating_factors df
  ON bb.var_name = df.long_name
  INNER JOIN (
    SELECT ''MATURE'' AS gr UNION 
    SELECT ''NEW'' AS gr UNION 
    SELECT ''ALL'' AS gr
  ) AS u
  ON (u.gr = ''MATURE'' AND bb.average_mature IS NOT NULL)
  OR (u.gr = ''NEW'' AND bb.average_new_1 IS NOT NULL)
  OR (u.gr = ''ALL'' AND bb.average_all IS NOT NULL);';

  EXECUTE query;
  IF products IS NOT NULL THEN 
  
    FOR i IN
      SELECT var_id FROM work.differentiating_factors
    LOOP

      this_long_name := long_name FROM work.differentiating_factors WHERE var_id = i;
      this_var_name  := var_name  FROM work.differentiating_factors WHERE var_id = i;


      IF this_long_name = 'Typical top-up amount' THEN
        query_product:= query_product || 'WHEN long_name = '''||this_long_name||''' THEN avg('||this_var_name||' * has_product) '; -- no coalesce for typical top-up amount (because no top-up is not amount 0)
      ELSE
        query_product:= query_product || 'WHEN long_name = '''||this_long_name||''' THEN avg(coalesce('||this_var_name||', 0) * has_product) ';
      END IF;

    END LOOP;
    
    product_arr := string_to_array(products, ',');

    -- Loop over products
    FOR p_id IN 1..array_upper(product_arr, 1) LOOP

      this_product_id := product_arr[p_id];
      
      query := '
      INSERT INTO charts.chart_data(mod_job_id, stat_name, group_name, data_date, var_name, var_value, order_id)
      SELECT
        '||in_mod_job_id||' AS mod_job_id,
        df.stat_name,
        pi.product_name AS group_name, 
        '''||t2||'''::date AS data_date,
        bb.var_name,
        bb.var_value,
        df.order_id
      FROM (
        SELECT
          CASE 
            '||query_product||'
          ELSE NULL END AS var_value, 
          long_name AS var_name
        FROM (
          SELECT 
            af.*,
            keys.long_name,
            keys.var_name,
            CASE WHEN p.product_taken_date IS NOT NULL AND p.product_id = '''|| this_product_id ||''' AND (p.product_churn_date IS NULL OR p.product_churn_date >= '''||target_t2||'''::date-7) THEN 1 -- Has the product at some point during the last week of the source period (latest data if in_mod_job_id corresponds to apply period)
            ELSE NULL
            END AS has_product
          FROM results.descriptive_variables_subscriber_matrix AS af
          INNER JOIN work.module_targets c 
          ON  af.alias_id = c.alias_id
          LEFT JOIN (SELECT * FROM data.product WHERE date_inserted = '''||target_t2||'''::date - 1) AS p
          ON c.alias_id = p.alias_id
          CROSS JOIN
          (SELECT long_name, var_name FROM work.differentiating_factors) AS keys
          WHERE af.mod_job_id = '||in_mod_job_id||'
          AND c.mod_job_id = '||target_list_mod_job_id||'
        ) aa
        GROUP BY long_name
      ) bb
      INNER JOIN work.differentiating_factors df
      ON bb.var_name = df.long_name
      INNER JOIN data.product_information pi
      ON pi.product_id = '''|| this_product_id ||''';';
    
      EXECUTE query; 
      
    END LOOP;  
  END IF;

    INSERT INTO charts.chart_data 
    SELECT
      in_mod_job_id, hu.stat_name, 'ALL', current_date, hu.value_class,
      COALESCE(var_value, 0), ord
    FROM (
      SELECT stat_name, value_class, count(*) as var_value
      FROM (
        SELECT
          stat_name,
          (CASE WHEN stat_name = 'SUBS_ARPU' THEN value_class_arpu 
                WHEN stat_name = 'SUBS_SOC_REVENUE' THEN value_class_sr
           END) AS value_class
        FROM (
          SELECT arpu, sr,
            (CASE WHEN sr <= 5 and sr >= 0 THEN '0-5'
               WHEN sr <= 10 THEN '5-10'
               WHEN sr <= 15 THEN '10-15'
               WHEN sr <= 20 THEN '15-20'
               WHEN sr <= 30 THEN '20-30'
               WHEN sr <= 40 THEN '30-40'
               WHEN sr <= 50 THEN '40-50'
               WHEN sr <= 75 THEN '50-75'
               WHEN sr <= 100 THEN '75-100'
               WHEN sr <= 150 THEN '100-150'
               WHEN sr > 150 THEN 'over 150'
             END) AS value_class_sr,
            (CASE WHEN arpu <=5 and arpu >= 0 THEN '0-5'
               WHEN arpu <= 10 THEN '5-10'
               WHEN arpu <= 15 THEN '10-15'
               WHEN arpu <= 20 THEN '15-20'
               WHEN arpu <= 30 THEN '20-30'
               WHEN arpu <= 40 THEN '30-40'
               WHEN arpu <= 50 THEN '40-50'
               WHEN arpu <= 75 THEN '50-75'
               WHEN arpu <= 100 THEN '75-100'
               WHEN arpu <= 150 THEN '100-150'
               WHEN arpu > 150 THEN 'over 150'
             END) AS value_class_arpu        
          FROM (
            SELECT  
              a.alias_id AS alias_id,
              a.monthly_arpu AS arpu, 
              b.social_revenue AS sr
            FROM work.monthly_arpu a
            RIGHT JOIN results.descriptive_variables_subscriber_matrix b
            ON a.alias_id = b.alias_id
            WHERE b.mod_job_id = in_mod_job_id
            AND a.mod_job_id = marpu_mod_job_id
            GROUP BY a.alias_id, a.monthly_arpu, b.social_revenue 
          ) d
        ) g
        CROSS JOIN (
          SELECT 'SUBS_SOC_REVENUE' AS stat_name UNION SELECT 'SUBS_ARPU' AS stat_name
        ) k  
        WHERE (value_class_sr IS NOT NULL AND stat_name = 'SUBS_SOC_REVENUE')
        OR (value_class_arpu IS NOT NULL AND stat_name = 'SUBS_ARPU') 
      ) j
      GROUP BY stat_name, value_class
    ) ji
    RIGHT OUTER JOIN ((
       SELECT  '0-5'      AS value_class,  1 AS ord
       UNION
       SELECT  '5-10'     AS value_class,  2 AS ord
       UNION
       SELECT  '10-15'    AS value_class,  3 AS ord 
       UNION
       SELECT  '15-20'    AS value_class,  4 AS ord 
       UNION
       SELECT  '20-30'    AS value_class,  5 AS ord 
       UNION
       SELECT  '30-40'    AS value_class,  6 AS ord 
       UNION
       SELECT  '40-50'    AS value_class,  7 AS ord 
       UNION
       SELECT  '50-75'    AS value_class,  8 AS ord 
       UNION
       SELECT  '75-100'   AS value_class,  9 AS ord 
       UNION
       SELECT  '100-150'  AS value_class, 10 AS ord 
       UNION
       SELECT  'over 150' AS value_class, 11 AS ord 
      ) ui
      CROSS JOIN 
        (SELECT 'SUBS_SOC_REVENUE' AS stat_name UNION SELECT 'SUBS_ARPU' AS stat_name) ti
  ) hu 
  ON hu.value_class=ji.value_class AND hu.stat_name=ji.stat_name;


  -- social connectivity score 
  INSERT INTO charts.chart_data (mod_job_id, stat_name, group_name, data_date, var_name, var_value, order_id) 
  SELECT target_list_mod_job_id AS mod_job_id,
    'SCORE_DISTRIBUTION' AS stat_name,
    'sna' AS group_name,
    now() AS data_date,
    var_name,
    var_value,
    NULL AS order_id
  FROM (
    SELECT
       ROUND(social_connectivity_score::numeric - (0.005), 1)::text AS var_name,
       count(*) AS var_value
     FROM results.descriptive_variables_subscriber_matrix
     WHERE mod_job_id = in_mod_job_id
     GROUP BY var_name
  ) aa;	

END;

                    </code>
                </input_parameters>
                <output_parameters>
                    <out></out>
                </output_parameters>
            </descriptice_variables_to_charts>
        </CalculateDescriptiveVariables_MCI_unforcenewnet>
        <email>
            <input_parameters>
                <command>echo "starting model apply" + $t2 | mailx -v -r "churn@mci.ir" -s "FAA scoring run" -S smtp="mail.mci.ir:25" -S smtp-use-starttls -S smtp-auth=login -S smtp-auth-user="churn@mci.ir -S smtp-auth-password="CHch@4782" -S ssl-verify=ignore -S nss-config-dir="/etc/pki/nssdb" ext.ho.ashtari@mci.or"</command>
                <host>localhost</host>
                <identity></identity>
                <password>$LocalhostPassword</password>
                <std.err.file></stderr.file>
                <stdout.file></stdout.file>
                <timeout>20000</timeout>
                <username>$LocalhostUsername</username>
            </input_parameters>
            <output_parameters>
                <exitstatus></exitstatus>
                <stderr></stderr>
                <stdout></stdout>
            </output_parameters>
        </email>
        <split:modelling>
            <when>parameter with name equal to apply_model and value equal to true</when>
            <tasks>Run_modelling_MCI, join</tasks>
            <when>parameter with name equal to apply_model and value equal to false</when>
            <tasks>join</tasks>
        </split:modelling>
        <Run_modelling_MCI>
            <DefineModelPreprocessing_MCI>
                <split>
                    <when></when>
                    <tasks>split:Run_churn_inactivity,split:Run_churn_postpaid, split:Run_portout</tasks>
                    <when></when>
                    <tasks>split:Run_zero_day_prediction, split:Run_product</tasks>
                </split>
                <split:Run_churn_inactivity>
                    <when>
                    parameter with name equal to uc_include_churn_inacticity and value equal to true
                    parameter with name equal to uc_churn_inactivity_model_id and value is null
                    </when>
                    <tasks>TemplateModelToPremodelTask(churn_inactivity), join</tasks>
                    <else></else>
                    <tasks>join</tasks>
                </split:Run_churn_inactivity>
                <split:Run_portout>
                    <when>
                    parameter with name equal to uc_include_churn_postpaid and value equal to true
                    parameter with name equal to uc_churn_postpaid_model_id and value is null
                    </when>
                    <tasks>TemplateModelToPremodelTask(churn_postpaid), join</tasks>
                    <else></else>
                    <tasks>join</tasks>
                </split:Run_churn_postpaid>
                <split:Run_portout>
                    <when>
                    parameter with name equal to uc_include_portout and value equal to true
                    parameter with name equal to uc_portout_model_id and value is null
                    </when>
                    <tasks>TemplateModelToPremodelTask(portout), join</tasks>
                    <else></else>
                    <tasks>join</tasks>
                </split:Run_portout>
                <split:Run_zero_day_prediction>
                    <when>
                    parameter with name equal to uc_include_zero_day_prediction and value equal to true
                    parameter with name equal to uc_zero_day_prediction_model_id and value is null
                    </when>
                    <tasks>TemplateModelToPremodelTask(zero_day_prediction), join</tasks>
                    <else></else>
                    <tasks>join</tasks>
                </split:Run_zero_day_prediction>
                <split:Run_product>
                    <when>
                    parameter with name equal to uc_include_zero_day_prediction and value equal to true
                    parameter with name equal to uc_zero_day_prediction_model_id and value is null
                    </when>
                    <tasks>TemplateModelToPremodelTask(product), join</tasks>
                    <else></else>
                    <tasks>join</tasks>
                </split:Run_product>
                <TemplateModelToPremodelTask(product)>
                </TemplateModelToPremodelTask(product)>
                <TemplateModelToPremodelTask(zero_day_prediction)>
                </TemplateModelToPremodelTask(zero_day_prediction)>
                <TemplateModelToPremodelTask(churn_postpaid)>
                </TemplateModelToPremodelTask(churn_postpaid)>
                <TemplateModelToPremodelTask(portout)>
                </TemplateModelToPremodelTask(portout)>
                <TemplateModelToPremodelTask(churn_inactivity)>
                </TemplateModelToPremodelTask(portout)>
            </DefineModelPreprocessing_MCI>
            <split>
                <when></when>
                <tasks>split, split:run_portout, join</tasks>
                <when></when>
                <tasks>split:run_churn_inactivity, split:run_churn_postpid, split:run_zero_day_prediction, join</tasks>
            </split>
            <split>
                <when>parameter wth name equal to uc_include_product and str value equal to true</when>
                <tasks>UC_Product_modelling, join</tasks>
                <when>parameter wth name equal to uc_include_product and str value equal to false</when>
                <tasks>join</tasks>
            </split>
            <split:run_portout>
                <when>parameter wth name equal to uc_include_portout and str value equal to true</when>
                <tasks>UC_portout_modelling_MCI, join</tasks>
                <when>parameter wth name equal to uc_include_portout and str value equal to false</when>
                <tasks>join</tasks>
            </split:run_portout>
            <split:run_churn_inactivity>
                <when>parameter wth name equal to uc_include_churn_inactivity and str value equal to true</when>
                <tasks>UC_ChurnInactivity_modelling_MCI, join</tasks>
                <when>parameter wth name equal to uc_include_churn_inactivity and str value equal to false</when>
                <tasks>join</tasks>
            </split:run_churn_inactivity>
            <split:run_churn_postpaid>
                <when>parameter wth name equal to uc_include_churn_postpaid and str value equal to true</when>
                <tasks>UC_ChurnPostpaid_modelling, join</tasks>
                <when>parameter wth name equal to uc_include_churn_postpaid and str value equal to false</when>
                <tasks>join</tasks>
            </split:run_churn_postpaid>
            <split:run_zero_day_perdiction>
                <when>parameter wth name equal to uc_include_zero_day_perdiction and str value equal to true</when>
                <tasks>UC_ZeroDayPrediction_modelling, join</tasks>
                <when>parameter wth name equal to uc_include_zero_day_perdiction and str value equal to false</when>
                <tasks>join</tasks>
            </split:run_zero_day_perdiction>
            <UC_Portout_modelling_MCI>
                <split:modelling>
                    <when>parameter wth name equal to apply_model and value equal to true</when>
                    <tasks>split:fit_model, join</tasks>
                    <when>parameter wth name equal to apply_model and value equal to false</when>
                    <tasks>join</tasks>
                </split:modelling>
                <split:fit_model>
                    <when>parameter wth name equal to fit_model and value equal to true</when>
                    <tasks>regressionFitterTask, regressionApplierTask,join</tasks>
                    <when>parameter wth name equal to fit_model and value equal to false</when>
                    <tasks>regressionApplierTask, join</tasks>
                </split:fit_model>
                <regressionFitterTask>
                </regressionFitterTask>
                <regressionApplierTask>
                </regressionApplierTask>
            </UC_Portout_modelling_MCI>
            <UC_Product_modelling>
                <split:modelling>
                    <when>parameter wth name equal to apply_model and value equal to true</when>
                    <tasks>split:fit_model, join</tasks>
                    <when>parameter wth name equal to apply_model and value equal to false</when>
                    <tasks>join</tasks>
                </split:modelling>
                <split:fit_model>
                    <when>parameter wth name equal to fit_model and value equal to true</when>
                    <tasks>regressionFitterTask, regressionApplierTask,join</tasks>
                    <when>parameter wth name equal to fit_model and value equal to false</when>
                    <tasks>regressionApplierTask, join</tasks>
                </split:fit_model>
                <regressionFitterTask>
                </regressionFitterTask>
                <regressionApplierTask>
                </regressionApplierTask>
            </UC_Product_modelling>
            <UC_ChurnInactivity_modelling_MCI>
                <split:modelling>
                    <when>parameter wth name equal to apply_model and value equal to true</when>
                    <tasks>split:fit_model, join</tasks>
                    <when>parameter wth name equal to apply_model and value equal to false</when>
                    <tasks>join</tasks>
                </split:modelling>
                <split:fit_model>
                    <when>parameter wth name equal to fit_model and value equal to true</when>
                    <tasks>regressionFitterTask, regressionApplierTask,join</tasks>
                    <when>parameter wth name equal to fit_model and value equal to false</when>
                    <tasks>regressionApplierTask, join</tasks>
                </split:fit_model>
                <regressionFitterTask>
                </regressionFitterTask>
                <regressionApplierTask>
                </regressionApplierTask>
            </UC_ChurnInactivity_modelling_MCI>
            <UC_ChurnPostpaid_modelling_MCI>
                <split:modelling>
                    <when>parameter wth name equal to apply_model and value equal to true</when>
                    <tasks>split:fit_model, join</tasks>
                    <when>parameter wth name equal to apply_model and value equal to false</when>
                    <tasks>join</tasks>
                </split:modelling>
                <split:fit_model>
                    <when>parameter wth name equal to fit_model and value equal to true</when>
                    <tasks>regressionFitterTask, regressionApplierTask,join</tasks>
                    <when>parameter wth name equal to fit_model and value equal to false</when>
                    <tasks>regressionApplierTask, join</tasks>
                </split:fit_model>
                <regressionFitterTask>
                </regressionFitterTask>
                <regressionApplierTask>
                </regressionApplierTask>
            </UC_ChurnPostpaid_modelling_MCI>
            <UC_ZeroDayPrediction_modelling_MCI>
                <split:modelling>
                    <when>parameter wth name equal to apply_model and value equal to true</when>
                    <tasks>split:fit_model, join</tasks>
                    <when>parameter wth name equal to apply_model and value equal to false</when>
                    <tasks>join</tasks>
                </split:modelling>
                <split:fit_model>
                    <when>parameter wth name equal to fit_model and value equal to true</when>
                    <tasks>regressionFitterTask, regressionApplierTask,join</tasks>
                    <when>parameter wth name equal to fit_model and value equal to false</when>
                    <tasks>regressionApplierTask, join</tasks>
                </split:fit_model>
                <regressionFitterTask>
                </regressionFitterTask>
                <regressionApplierTask>
                </regressionApplierTask>
            </UC_ZeroDayPrediction_modelling_MCI>
            <split>
                <when></when>
                <tasks>split:varGroups, join</tasks>
                <when></when>
                <tasks>Copy_from_output_to_results, join</tasks>
            </split>
            <split:varGroups>
                <when>parameter wth name equal to calculate_vargroup_output and value equal to true</when>
                <tasks>calculate_variable_group_balance_factors, copy_variable_group_output_to_results,join</tasks>
                <when>parameter wth name equal to calculate_vargroup_output and value equal to true</when>
                <tasks>join</tasks>
            </split:varGroups>
            <Copy_from_output_to_results>
                <input_parameters>
                    <DataSource1></DataSource1>
                    <queryparameters></queryparameters>
                    <sql>select * from results.copy_from_output_to_results($mod_job_id)</sql>
                    <code>
                        
CREATE FUNCTION copy_from_output_to_results(module_job_id integer) RETURNS integer
    AS $$

/* SUMMARY
 * This function copies model output from work.module_model_output table to
 * tmp.module_results_tmp table for the included use cases. 
 * Afterwards, results.combine_results function is called to combine the
 * results in the results.module_results table. 
 *
 * INPUTS
 * module_job_id : integer : Indentifier of module job.
 *
 * OUTPUT
 * count         : integer : The number of copied rows
 * 
 * VERSION
 * 01.02.2013 HMa - improved use-case modularity
 * 11.01.2013 HMa - support for multiple use cases
 * 05.12.2012 JTI 
 * 14.09.2017 Lzu - MCI port out usecase
 */

DECLARE

  job_use_cases text;
  
  use_cases     text[];
  use_case      text;
  uc_model_id   integer;
  
  records_found integer;
  n_rows        integer;

  product_run   boolean;
  
BEGIN

  -- Check if the results.module_results table already contains data for the given module_job_id:
  records_found := count(mr.*) FROM results.module_results AS mr WHERE mr.mod_job_id = module_job_id;
  IF records_found IS NOT NULL AND records_found > 0 THEN
    RAISE EXCEPTION 'Table results.module_results already contains data for mob_job_id %', module_job_id;
  END IF;

  job_use_cases := (
    SELECT mjp.value
    FROM work.module_job_parameters AS mjp 
    WHERE mjp.mod_job_id = module_job_id AND mjp.key = 'job_use_cases'
  );

  -- Read the use case names from job_use_cases:
  use_cases := string_to_array(trim(job_use_cases),',');
  
  -- Each product is listed separately in job_use_cases, but results.copy_from_output_to_results_product needs to be called only once. 
  product_run = FALSE; 

  FOR uc_ind IN 1..array_upper(use_cases, 1) LOOP
    
    -- Use case name:
    use_case := use_cases[uc_ind];

    -- Each product is listed separately in job_use_cases and they have their own submodels, but the submodel list model id is saved in module job parameters. 
    IF use_case ~ 'product' THEN
      use_case := 'product';
    END IF;

    -- Use case model id: 
    uc_model_id :=  mjp.value FROM work.module_job_parameters AS mjp WHERE mjp.mod_job_id = module_job_id AND mjp.key = 'uc_' || use_case || '_model_id';
    
    IF uc_model_id IS NULL THEN    
      RAISE EXCEPTION '%_model_id not found!', use_case;
    ELSIF use_case = 'churn_inactivity' THEN
      PERFORM results.copy_from_output_to_results_churn(module_job_id, uc_model_id, 'inactivity');
    ELSIF use_case = 'portout' THEN
      PERFORM results.copy_from_output_to_results_portout(module_job_id, uc_model_id);
    ELSIF use_case = 'churn_postpaid' THEN
      PERFORM results.copy_from_output_to_results_churn(module_job_id, uc_model_id, 'postpaid');
    ELSIF use_case = 'zero_day_prediction' THEN
      PERFORM results.copy_from_output_to_results_zero_day_prediction(module_job_id, uc_model_id);
    ELSIF use_case = 'product' AND NOT product_run THEN
      PERFORM results.copy_from_output_to_results_product(module_job_id, uc_model_id);
      product_run = TRUE;
    ELSIF product_run THEN
      -- Do not raise notice
    ELSE
      RAISE NOTICE 'Copy results for use case ''%'' not supported', use_case;
    END IF;
  
  END LOOP;

  SELECT * FROM results.combine_results(module_job_id) INTO n_rows;

  RETURN n_rows;

END;

                    </code>
                </input_parameters>
                <output_parameters>
                    <out></out>
                </output_parameters>
            </Copy_from_output_to_results>
            <calculate_variable_group_balance_factors>
                <input_parameters>
                    <DataSource1></DataSource1>
                    <queryparameters></queryparameters>
                    <sql>select * from work.variable_group_balance_factors($mod_job_id)</sql>
                    <code>
                        
CREATE FUNCTION variable_group_balance_factors(in_mod_job_id integer) RETURNS void
    AS $$
/*
 * SUMMARY
 * 
 * Calculates balance factors for variable groups for adjusting logit propensity score calculation later. Consider:
 *
 *   aps = 1 / (1 + bf*exp(-als))
 *
 * where aps = adjusted propensity score, bf = balance factor stored in work.variable_group_balance_factors table
 * and als = adjusted linear score stored in work.module_model_output_variable_group
 *
 * Idea is that AVG(aps) =~ churn rate in population.
 * 
 * VERSION
 * 15.04.2014 JVi - Created
 */

DECLARE

  mjp record;
  
BEGIN

  SELECT DISTINCT
    in_mod_job_id AS mod_job_id,
    max(CASE WHEN d.key        IN ('t2') THEN to_date(d.value, 'YYYY-MM-DD') ELSE NULL END) AS t2
  INTO mjp
  FROM work.module_job_parameters AS d
  WHERE d.mod_job_id = in_mod_job_id;

  INSERT INTO work.variable_group_balance_factors
  (mod_job_id, model_id, output_id, vargroup, balance_factor)
  SELECT -- Calculate balance factor
    in_mod_job_id AS mod_job_id,
    model_id,
    output_id,
    mmovg.vargroup,
    ((1 - avg_propensity_score) / (NULLIF(avg_propensity_score,0))::numeric) * ((avg_vargroup_score) / (NULLIF(1 - avg_vargroup_score,0))::numeric) AS balance_factor
  FROM ( -- Calculate average logit score for whole population
    SELECT
      model_id,
      output_id,
      AVG(value) AS avg_propensity_score
    FROM work.module_model_output 
    WHERE mod_job_id = in_mod_job_id
    GROUP BY model_id, output_id
  ) mmo
  INNER JOIN ( -- Calculate average logit score for each vargroup
    SELECT 
      mmovg.model_id,
      mmovg.output_id,
      mmovg.vargroup,
      AVG(1 / NULLIF(1 + exp(-mmovg.value),0)::numeric) AS avg_vargroup_score
    FROM work.module_model_output_variable_group mmovg
    INNER JOIN ( -- Get only logistic models
      SELECT DISTINCT model_id FROM work.module_models 
      WHERE aid = 0 AND key = 'model_type' AND value = 'logistic' 
    ) mm
    USING (model_id)
    WHERE mmovg.mod_job_id = in_mod_job_id
    GROUP BY model_id, output_id, vargroup
  ) mmovg
  USING (model_id, output_id);

END;

                    </code>
                </input_parameters>
                <output_parameters>
                    <out></out>
                </output_parameters>
            </calculate_variable_group_balance_factors>
            <copy_variable_group_output_to_results>
                <input_parameters>
                    <DataSource1></DataSource1>
                    <queryparameters></queryparameters>
                    <sql>select * from results.copy_vargroup_output_to_results($mod_job_id)</sql>
                    <code>
                        
CREATE FUNCTION copy_vargroup_output_to_results(module_job_id integer) RETURNS integer
    AS $$

/* SUMMARY
 * This function copies variable group -specific output from work.module_model_output_variable_group table to
 * tmp.module_results_vargroup_tmp table for the included use cases. Afterwards, 
 * results.combine_variable_group_results function is called to combine the results in the 
 * results.module_results_variable_group table. 
 *
 * INPUTS
 * module_job_id : integer : Indentifier of module job.
 *
 * OUTPUT
 * count         : integer : The number of copied rows
 * 
 * VERSION
 * 03.05.2014 JVi - Created
 * 14.09.2017 LZu added MCI portout
 */

DECLARE

  job_use_cases text;
  
  use_cases     text[];
  use_case      text;
  uc_model_id   integer;
  
  records_found integer;
  n_rows        integer;

  product_run   boolean;
  
BEGIN

  -- Check if the results.module_results table already contains data for the given module_job_id:
  records_found := count(mrvg.*) FROM results.module_results_variable_group AS mrvg WHERE mrvg.mod_job_id = module_job_id;
  IF records_found IS NOT NULL AND records_found > 0 THEN 
    RAISE EXCEPTION 'Table results.module_results_variable_group already contains data for mob_job_id %', module_job_id;
  END IF;

  job_use_cases := (
    SELECT mjp.value
    FROM work.module_job_parameters AS mjp 
    WHERE mjp.mod_job_id = module_job_id AND mjp.key = 'job_use_cases'
  );

  -- Read the use case names from job_use_cases:
  use_cases := string_to_array(trim(job_use_cases),',');
  
  FOR uc_ind IN 1..array_upper(use_cases, 1) LOOP
    
    -- Use case name:
    use_case := use_cases[uc_ind];

    -- Use case model id: 
    uc_model_id :=  mjp.value FROM work.module_job_parameters AS mjp WHERE mjp.mod_job_id = module_job_id AND mjp.key = 'uc_' || use_case || '_model_id';
    
    IF uc_model_id IS NULL THEN    
      RAISE EXCEPTION '%_model_id not found!', use_case;
    ELSIF use_case = 'churn_inactivity' THEN
      PERFORM results.copy_vargroup_output_to_results_churn(module_job_id, uc_model_id, 'inactivity');
    ELSIF use_case = 'portout' THEN
      PERFORM results.copy_vargroup_output_to_results_portout(module_job_id, uc_model_id, 'portout');
	ELSIF use_case = 'churn_postpaid' THEN
      PERFORM results.copy_vargroup_output_to_results_churn(module_job_id, uc_model_id, 'postpaid');
      -- Do not raise notice
    ELSE
      RAISE NOTICE 'Copy results for use case ''%'' not supported', use_case;
    END IF;
  
  END LOOP;

  SELECT * FROM results.combine_variable_group_results(module_job_id) INTO n_rows;

  RETURN n_rows;

END;

                    </code>
                </input_parameters>
                <output_parameters>
                    <out></out>
                </output_parameters>
            </copy_variable_group_output_to_results>
            <split:verification>
                <when>parameter with name equal to fit_model and value equal to true</when>
                <tasks>Run_modelling_verification_MCI, join</tasks>
                <when>parameter with name equal to fit_model and value equal to false</when>
                <tasks>join</tasks>
            </split:verification>
            <Run_modelling_verification_MCI>
                <split>
                    <when></when>
                    <tasks>split:run_product, split:run_portout,join</tasks>
                    <when>split:run_churn_inactivity, split:run_churn_postpaid, split:run_zero_day_prediction, join</when>
                    <tasks>join</tasks>
                </split>
                <split:run_product>
                    <when>parameter with name equal to uc_include_product and str value equal to true</when>
                    <tasks>verify_product_results, join</tasks>
                    <when>parameter with name equal to uc_include_product and str value equal to true</when>
                    <tasks>join</tasks>
                </split:run_product>
                <split:run_portout>
                    <when>parameter with name equal to uc_include_portout and str value equal to true</when>
                    <tasks>verify_portout_results, join</tasks>
                    <when>parameter with name equal to uc_include_portout and str value equal to true</when>
                    <tasks>join</tasks>
                </split:run_portout>
                <split:run_churn_inactivity>
                    <when>parameter with name equal to uc_include_churn_inactivity and str value equal to true</when>
                    <tasks>verify_churn_inactivity_results, join</tasks>
                    <when>parameter with name equal to uc_include_churn_inactivity and str value equal to true</when>
                    <tasks>join</tasks>
                </split:run_churn_inactivity>
                <split:run_churn_postpaid>
                    <when>parameter with name equal to uc_include_churn_postpaid and str value equal to true</when>
                    <tasks>verify_churn_postpaid_results, join</tasks>
                    <when>parameter with name equal to uc_include_churn_postpaid and str value equal to true</when>
                    <tasks>join</tasks>
                </split:run_churn_postpaid>
                <split:run_churn_zero_day_perdiction>
                    <when>parameter with name equal to uc_include_zero_day_perdiction and str value equal to true</when>
                    <tasks>verify_churn_zero_day_perdiction_results, join</tasks>
                    <when>parameter with name equal to uc_include_zero_day_perdiction and str value equal to true</when>
                    <tasks>join</tasks>
                </split:run_zero_day_perdiction>
                <verify_product_results>
                </verify_product_results>
                <verify_portout_results>
                </verify_portout_results>
                <verify_churn_inactivity_results>
                </verify_churn_inactivity_results>
                <verify_churn_postpaid_results>
                </verify_churn_postpaid_results>
                <verify_zero_day_prediction_results>
                </verify_zero_day_prediction_results>
            </Run_modelling_verification_MCI>
        </Run_modelling_MCI>
    </Run_MCI>
    <email1>
        <input_parameters>
            <command>echo "fitting completed for" + $t2 | mailx -v -r "churn@mci.ir" -s "FAA Scoring run" $emailaddress</command>
            <host>localhost</host>
            <identity></identity>
            <password>$LocalhostPassword</password>
            <std.err.file></stderr.file>
            <stdout.file></stdout.file>
            <timeout>20000</timeout>
            <username>$LocalhostUsername</username>
        </input_parameters>
        <output_parameters>
            <exitstatus></exitstatus>
            <stderr></stderr>
            <stdout></stdout>
        </output_parameters>
    </email1>
    <Info:Applying_Models>
        <input_parameters>
            <DataSource1></DataSource1>
            <queryparameters></queryparameters>
            <sql>update core_runbean_attributes set attributes = 'Applying model on historical data' where runbean_id = '$WORKFLOW_RUN_ID' and attributes_key = 'checkpoint'</sql>
        </input_parameters>
        <output_parameters>
            <out></out>
        </output_parameters>
    </Info:Applying_Models>
    <Init_Parameters_for_Apply>
        <input_parameters>
        </input_parameters>
        <output_parameters>
            <lda_cell_events_input_id>-1</lda_cell_events_input_id>
            <lda_cell_events_output_id_old>-1</lda_cell_events_output_id_old>
            <lda_input_id>-1</lda_input_id>
            <lda_output_id_old>-1</lda_output_id_old>
            <mod_job_id>-1</mod_job_id>
            <lda_cell_events_input_options>"No LDA run"</lda_cell_events_input_options>
            <lda_cell_events_model_options>"No LDA run"</lda_cell_events_model_options>
            <lda_input_options>"No LDA run"</lda_input_options>
            <lda_model_options>"No LDA run"</lda_model_options>
            <run_type>"Predictors + Apply"</run_type>
            <run_descvar>false</run_descvar>
            <t2>"t2"</t2>
            <descvar_interval_weeks>2</descvar_interval_weeks>
        </output_parameters>
    </Init_Parameters_for_Apply>
    <Run_MCI>
    </Run_MCI>
    <email2>
        <input_parameters>
            <command>echo "apply completed for" + $t2 | mailx -v -r "churn@mci.ir" -s "FAA Scoring run" $emailaddress</command>
            <host>localhost</host>
            <identity></identity>
            <password>$LocalhostPassword</password>
            <std.err.file></stderr.file>
            <stdout.file></stdout.file>
            <timeout>20000</timeout>
            <username>$LocalhostUsername</username>
        </input_parameters>
        <output_parameters>
            <exitstatus></exitstatus>
            <stderr></stderr>
            <stdout></stdout>
        </output_parameters>
    </email2>
    <split>
        <when></when>
        <tasks>Init_Names_of_Files, split, Transfer Files, join</tasks>
        <when></when>
        <tasks>RTPAggregationFlow, RTPOutputGenerateFlow_CSV, Export_MCI, email3, join</tasks>
    </split>
    <Init_Names_of_Files>
        <input_parameters>
            <DataSource1></DataSource1>
            <queryparameters></queryparameters>
            <sql>select 'churn_inactivity_prediction_score_list_' || to_char(now(), 'YYYYMMDDHH24MISS') || '.txt' as ItemFileNameChurn, 'portout_prediction_scor_list' || to_char(now(), 'YYYYMMDDHH24MISS') || '.txt' as ItemFileNameportout</sql>
        </input_parameters>
        <output_parameters>
            <out></out>
            <ItemFileNameChurn>out.get(0).get(0)</ItemFileNameChurn>
            <ItemFileNameportout>out.get(0).get(1)</ItemFileNameportout>
        </output_parameters>
    </Init_Names_of_Files>
    <split>
        <when></when>
        <tasks>Export_Portout_Results, join</tasks>
        <when></when>
        <tasks>Export_Churn_Results, join</tasks>
    </split>
    <Export_Portout_Results>
        <input_parameters>
            <command>
            psql -d $AnalyticsDatabaseName -h $AnalyticsDatabaseHost -p $AnalyticsDatabasePort -U xsl -c "" copy (select a.mod_job_id as score_id, b.string_id as msisdn, a.score_value as portout_propensity_score, date "$t2" + integer '1' as date_of_analyss
            from (select alias_id, score_value, mod_job_id from tmp.module_results_tmp where mod_job_id = (select max(mod_job_id) from tmp.module_results_tmp)
            and score_name = 'portout_propensity_score') a
            left join (select distinct alias_id, string_id from aliases.aliases_update where in_out_network = 1) as b
            on a.alias_id = b.alias_id where b.alias_id is not null)
            to stdout delimiter as ';' null as '' csv header"" > $LocalhostOutputPath/$ItemFileNameportout
            </command>
            <host>localhost</host>
            <identity></identity>
            <password>$LocalhostPassword</password>
            <std.err.file></stderr.file>
            <stdout.file></stdout.file>
            <timeout>20000</timeout>
            <username>$LocalhostUsername</username>
        </input_parameters>
        <output_parameters>
            <exitstatus></exitstatus>
            <stderr></stderr>
            <stdout></stdout>
        </output_parameters>
    </Export_Portout_Results>
    <Export_Churn_Results>
        <input_parameters>
            <command>
            psql -d $AnalyticsDatabaseName -h $AnalyticsDatabaseHost -p $AnalyticsDatabasePort -U xsl -c "" copy (select a.mod_job_id as score_id, b.string_id as msisdn, a.score_value as churn_propensity_score, date "$t2" + integer '1' as date_of_analysis
            from (select alias_id, score_value, mod_job_id from tmp.module_results_tmp where mod_job_id = (select max(mod_job_id) from tmp.module_results_tmp)
            and score_name = 'churn_inactivity_propensity_score') a
            left join (select distinct alias_id, string_id from aliases.aliases_update where in_out_network = 1) as b
            on a.alias_id = b.alias_id where b.alias_id is not null)
            to stdout delimiter as ';' null as '' csv header"" > $LocalhostOutputPath/$ItemFileNamechurn
            </command>
            <host>localhost</host>
            <identity></identity>
            <password>$LocalhostPassword</password>
            <std.err.file></stderr.file>
            <stdout.file></stdout.file>
            <timeout>20000</timeout>
            <username>$LocalhostUsername</username>
        </input_parameters>
        <output_parameters>
            <exitstatus></exitstatus>
            <stderr></stderr>
            <stdout></stdout>
        </output_parameters>
    </Export_Churn_Results>
    <Transfer_Files>
        <input_parameters>
            <command>
            scp $LocalhostOutputPath/*churn_inactivity*score_list*.txt $OutputHostUsername@$OutputHost:#OutputHostPath/churn_inactivity_score_list; scp $LocalhostOutputPath/*portout*score_list*.txt $OutputHostUsername@$OutputHost:#OutputHostPath/port_out_score_list; rm $LocalhostOutputPath/*score_list*.txt
            </command>
            <host>localhost</host>
            <identity></identity>
            <password>$LocalhostPassword</password>
            <std.err.file></stderr.file>
            <stdout.file></stdout.file>
            <timeout>20000</timeout>
            <username>$LocalhostUsername</username>
        </input_parameters>
        <output_parameters>
            <exitstatus></exitstatus>
            <stderr></stderr>
            <stdout></stdout>
        </output_parameters>
    </Transfer_Files>
    <RTPAggregationFlow>
        <split>
            <when></when>
            <tasks>RTP_Aggregation_1, join</tasks>
            <when></when>
            <tasks>RTP_Aggregation_2, RTP_Aggregation_3, join</tasks>
        </split>
        <RTP_Aggregation_1>
            <input_parameters>
                <DataSource1></DataSource1>
                <queryparameters></queryparameters>
                <sql>select * from work.rtp_aggregation_script_1($mod_job_id)</sql>
                <code>
                    
CREATE FUNCTION rtp_aggregation_script_1(in_mod_job_id integer) RETURNS void
    AS $$

DECLARE

  tcrm date;

BEGIN

  tcrm := mjp.value FROM work.module_job_parameters mjp WHERE mjp.mod_job_id = in_mod_job_id AND mjp.key = 'tCRM';
  
  TRUNCATE tmp.rtp_aggregation_1;
  
  INSERT INTO tmp.rtp_aggregation_1
  (mod_job_id, alias_id, latest_saturday, peak_type_voice, peak_type_data)
  SELECT
    in_mod_job_id AS mod_job_id,
    aa.alias_id AS alias_id,
    aa.latest_saturday,
    aa.peak_type_voice,
    aa.peak_type_data
  FROM (
    SELECT
      alias_id,
      max(monday) AS latest_saturday,
      CASE
        WHEN sum(onnet_voice_min_peakhour) >= sum(onnet_voice_min_offpeakhour) THEN 'peak'
        WHEN sum(onnet_voice_min_peakhour) < sum(onnet_voice_min_offpeakhour) THEN 'offpeak'
        ELSE 'normal' END AS peak_type_voice,
      CASE
        WHEN sum(onnet_data_mb_peakhour) >= sum(onnet_data_mb_offpeakhour) THEN 'peak'
        WHEN sum(onnet_data_mb_peakhour) < sum(onnet_data_mb_offpeakhour) THEN 'offpeak'
        ELSE 'normal' END AS peak_type_data -- 09-05-2017 LZu: The on-net voice minutes made during peak hours is greater than or equal to that during off-peak hours in the past 1 month
    FROM data.call_types_weekly
    WHERE monday <= tcrm
    AND monday >= (SELECT max(monday) FROM data.call_types_weekly WHERE monday <= tcrm) - interval '4 weeks'
    AND direction IN ('m')
    GROUP BY alias_id
    -- HAVING monday >= (SELECT max(monday) FROM data.call_types_weekly) - interval '4 weeks' -- get last 4 weeks of data here to do comparison
    -- AND direction IN ('m')
  ) aa
  LEFT OUTER JOIN (
    SELECT latest_saturday FROM tmp.rtp_aggregation_1 GROUP BY latest_saturday
  ) bb
  ON aa.latest_saturday = bb.latest_saturday
  WHERE bb.latest_saturday IS NULL;

  ANALYZE tmp.rtp_aggregation_1;
  
  
  TRUNCATE tmp.rtp_aggregation_2;
  
  INSERT INTO tmp.rtp_aggregation_2
  (mod_job_id, alias_id, latest_saturday, avg_business_voice_usage, avg_non_business_voice_usage,
   avg_business_data_usage, avg_non_business_data_usage
  )
  SELECT
    in_mod_job_id AS mod_job_id,
    aa.alias_id AS alias_id,
    aa.latest_saturday,
    aa.avg_business_voice_usage,
    aa.avg_non_business_voice_usage,
    aa.avg_business_data_usage,
    aa.avg_non_business_data_usage
  FROM (
    SELECT
      alias_id,
      max(monday) AS latest_saturday,
      sum(voice_usage_min_businesshours) / 6 AS avg_business_voice_usage,
      sum(voice_usage_min_nonbusinesshours) / 6 AS avg_non_business_voice_usage,
      sum(data_usage_min_businesshours) / (6 * 1000000) AS avg_business_data_usage,
      sum(data_usage_min_nonbusinesshours) / (6 * 1000000) AS avg_non_business_data_usage -- 09-05-2017 LZu: The average monthly voice or data usage (in minutes or MB) during business hours in the past 6 months.
    FROM data.call_types_weekly
    WHERE monday <= tcrm
    AND monday >= (SELECT max(monday) FROM data.call_types_weekly WHERE monday <= tcrm) - interval '26 weeks'
    AND direction IN ('m', 'r')
    GROUP BY alias_id
    -- HAVING monday >= (SELECT max(monday) FROM data.call_types_weekly) - interval '26 weeks' -- get last 26 weeks of data here to do comparison
    -- AND direction IN ('m', 'r') -- check direction for both ways is needed here or do we do two functions since data r = NULL
  ) aa
  LEFT OUTER JOIN (
    SELECT latest_saturday FROM tmp.rtp_aggregation_2 GROUP BY latest_saturday
  ) bb
  ON aa.latest_saturday = bb.latest_saturday
  WHERE bb.latest_saturday IS NULL;

  ANALYZE tmp.rtp_aggregation_2;
  
  
  TRUNCATE tmp.rtp_aggregation_3;
  
  INSERT INTO tmp.rtp_aggregation_3
  (mod_job_id, alias_id, latest_saturday, avg_daily_2g_data_usage,avg_daily_3g_data_usage, avg_daily_4g_data_usage, monthly_data_usage)
  SELECT
    in_mod_job_id AS mod_job_id,
    aa.alias_id AS alias_id,
    aa.latest_saturday,
    aa.avg_daily_2g_data_usage,
    aa.avg_daily_3g_data_usage,
    aa.avg_daily_4g_data_usage,
    aa.monthly_data_usage
  FROM (
    SELECT
      alias_id,
      max(monday) AS latest_saturday,
      sum(data_usage_2g)/28000000000 AS avg_daily_2g_data_usage,
      sum(data_usage_3g)/28000000000 AS avg_daily_3g_data_usage,
      sum(data_usage_4g)/28000000000 AS avg_daily_4g_data_usage, -- 09-05-2017 LZu: The average daily data usage (in MB) in the past 1 month
      (sum(data_usage_2g) + sum(data_usage_3g) + sum(data_usage_4g))/1000 AS monthly_data_usage
    FROM data.call_types_weekly
    WHERE monday <= tcrm
    AND monday >= (SELECT max(monday) FROM data.call_types_weekly WHERE monday <= tcrm) - interval '4 weeks'
    AND direction IN ('m')
    GROUP BY alias_id
    -- HAVING monday >= (SELECT max(monday) FROM data.call_types_weekly) - interval '4 weeks' -- get last 4 weeks of data here to do comparison
    -- AND direction IN ('m') -- check direction for both ways is needed here since data r = NULL
  ) aa
  LEFT OUTER JOIN (
    SELECT latest_saturday FROM tmp.rtp_aggregation_3 GROUP BY latest_saturday
  ) bb
  ON aa.latest_saturday = bb.latest_saturday
  WHERE bb.latest_saturday IS NULL;

  ANALYZE tmp.rtp_aggregation_3;

END;

                </code>
            </input_parameters>
            <output_parameters>
                <out></out>
            </output_parameters>
        </RTP_Aggregation_1>
        <RTP_Aggregation_2>
            <input_parameters>
                <DataSource1></DataSource1>
                <queryparameters></queryparameters>
                <sql>select * from work.rtp_aggregation_script_2($mod_job_id)</sql>
                <code>
                    
CREATE FUNCTION rtp_aggregation_script_2(in_mod_job_id integer) RETURNS void
    AS $$

DECLARE

  tcrm date;

BEGIN

  tcrm := mjp.value FROM work.module_job_parameters mjp WHERE mjp.mod_job_id = in_mod_job_id AND mjp.key =  'tCRM';
  
  TRUNCATE tmp.rtp_aggregation_4;
  
  INSERT INTO tmp.rtp_aggregation_4
  (mod_job_id, alias_id, contact_cust_care_total, contact_cust_care_ntwk, contact_cust_care_bill, contact_cust_care_promo)
  SELECT
    in_mod_job_id AS mod_job_id,
    aa.alias_id AS alias_id,
    count(*) AS contact_cust_care_total,
    sum(aa.is_contact_cust_care_ntwk) AS contact_cust_care_ntwk,
    sum(aa.is_contact_cust_care_bill) AS contact_cust_care_bill,
    sum(aa.is_contact_cust_care_promo) AS contact_cust_care_promo
  FROM (
    SELECT
      alias_id,
      CASE WHEN substring(service_type FROM 1 FOR 6)::integer IN (111001, 111004, 111005, 111011) THEN 1 ELSE 0 END AS is_contact_cust_care_ntwk,
      CASE WHEN substring(service_type FROM 1 FOR 6)::integer IN (111002) THEN 1 ELSE 0 END AS is_contact_cust_care_bill,
      CASE WHEN substring(service_type FROM 1 FOR 6)::integer IN (111003, 111006, 111007, 111012) THEN 1 ELSE 0 END AS is_contact_cust_care_promo
    FROM data.customer_care
    WHERE date_reported BETWEEN tcrm - interval '4 weeks' AND tcrm -- Get last 4 weeks of data here to do comparison
  ) aa
  GROUP BY alias_id
  ;

  ANALYZE tmp.rtp_aggregation_4;
  
  
  TRUNCATE tmp.rtp_aggregation_5;
  
  INSERT INTO tmp.rtp_aggregation_5
  (mod_job_id, alias_id, avg_monthly_topup_nmb, avg_monthly_topup_cost)
  SELECT
    in_mod_job_id AS mod_job_id,
    aa.receiving_id AS alias_id,
    aa.avg_monthly_topup_nmb,
    aa.avg_monthly_topup_cost
  FROM (
    SELECT
      receiving_id,
      count(*)::double precision / 6 AS avg_monthly_topup_nmb,
      sum(credit_amount)::double precision / (6 * 1000) AS avg_monthly_topup_cost
    FROM data.topup
    WHERE is_credit_transfer::integer = 0
    AND "timestamp" BETWEEN tcrm - interval '26 weeks' AND tcrm -- Get last 26 weeks of data here to do comparison
    GROUP BY receiving_id
  ) aa
  ;

  ANALYZE tmp.rtp_aggregation_5;
  
END;

                </code>
            </input_parameters>
            <output_parameters>
                <out></out>
            </output_parameters>
        </RTP_Aggregation_2>
        <RTP_Aggregation_3>
            <input_parameters>
                <DataSource1></DataSource1>
                <queryparameters></queryparameters>
                <sql>select * from work.rtp_aggregation_script_3($mod_job_id)</sql>
                <code>
                    
CREATE FUNCTION rtp_aggregation_script_3(in_mod_job_id integer) RETURNS void
    AS $$

DECLARE

  tcrm date;
  t2 date;
  product_bonus_date date;
  product_discount_date date;

BEGIN
  t2 := mjp.value FROM work.module_job_parameters mjp WHERE mjp.mod_job_id = in_mod_job_id AND mjp.key = 't2';
  tcrm := mjp.value FROM work.module_job_parameters mjp WHERE mjp.mod_job_id = in_mod_job_id AND mjp.key = 'tCRM';
  SELECT max(date_inserted) INTO product_bonus_date FROM data.product_information_bonus WHERE date_inserted <= t2;
  SELECT max(date_inserted) INTO product_discount_date FROM data.product_information_discount WHERE date_inserted <= t2;

  
  TRUNCATE tmp.rtp_aggregation_product_tmp;
  
  -- Extracts all product (pure voice and data only) subscription information for each subscriber.
  -- By default, a subsciption has the start and expected end time. If a package subscription is deactivated earlier than expected, it will be reflected in the package end time.
  INSERT INTO tmp.rtp_aggregation_product_tmp (
    alias_id,
    package_type,
    package_id,
    promo_type,
    duration_of_package,
    data_volume,
    pkg_start_time,
    pkg_end_time
  )
  SELECT
    aa.alias_id,
    prod.pkg_type AS package_type,
    prod.pkg_id AS package_id,
    coalesce(bonus.promo_type, disc.promo_type) AS promo_type,
    coalesce(bonus.duration_of_package, disc.duration_of_package) AS duration_of_package,
    coalesce(bonus.gprs_bonus, disc.gprs_percent * data_use.monthly_data_usage) AS data_volume, -- For discount packages, data usage is estimated by GPRS percent * total data usage in the last 4 weeks. This approach is incorrcect for non-monthly data discount packages, but data usage for these packages are not of RTP's interest and thus will be ignored.
    pkg_start_date AS pkg_start_time,
    pkg_end_date AS pkg_end_time
  FROM (
    SELECT
      alias_id,
      pkg_id,
      pkg_type,
      max(pkg_start_date) AS pkg_start_date,
      min(pkg_end_date) AS pkg_end_date
    FROM data.product_takeup
    WHERE date_inserted <= tcrm
    GROUP BY alias_id, pkg_id, pkg_type
  ) prod
  LEFT JOIN (
    SELECT
      alias_id,
      payment_type
    FROM data.in_crm
  ) aa
  ON aa.alias_id = prod.alias_id
  LEFT JOIN (
    SELECT
      package_no,
      product_id,
      payment_type,
      'Bonus' AS pkg_type,
      CASE
        WHEN gprs_bonus <> 0 AND sms_bonus = 0 AND mms_bonus = 0 AND call_bonus = 0 THEN 'Data'
        WHEN call_bonus <> 0 AND sms_bonus = 0 AND mms_bonus = 0 AND gprs_bonus = 0 THEN 'Voice'
        ELSE 'Others'
      END AS promo_type,
      gprs_bonus,
      duration_of_package
    FROM data.product_information_bonus
    WHERE date_inserted = product_bonus_date
  ) bonus
  ON aa.payment_type = bonus.payment_type
  AND prod.pkg_id = bonus.package_no
  AND prod.pkg_type = bonus.pkg_type
  LEFT JOIN (
    SELECT
      package_no,
      product_id,
      payment_type,
      'Discount' AS pkg_type,
      CASE
        WHEN gprs_percent <> 0 AND sms_percent = 0 AND mms_percent = 0 AND call_percent = 0 THEN 'Data'
        WHEN call_percent <> 0 AND sms_percent = 0 AND mms_percent = 0 AND gprs_percent = 0 THEN 'Voice'
        ELSE 'Others'
      END AS promo_type,
      abs(gprs_percent::double precision) / 100 AS gprs_percent,
      duration_of_package
    FROM data.product_information_discount
    WHERE date_inserted = product_discount_date
  ) disc
  ON aa.payment_type = disc.payment_type
  AND prod.pkg_id = disc.package_no
  AND prod.pkg_type = disc.pkg_type
  LEFT JOIN (
    SELECT
      alias_id,
	  monthly_data_usage
    FROM tmp.rtp_aggregation_3
    WHERE mod_job_id = in_mod_job_id
  ) data_use
  ON aa.alias_id = data_use.alias_id
  WHERE (bonus.promo_type <> 'Others' AND disc.promo_type <> 'Others')
  ;

  ANALYZE tmp.rtp_aggregation_product_tmp;

  
  TRUNCATE tmp.rtp_aggregation_6_tmp;

  INSERT INTO tmp.rtp_aggregation_6_tmp (
    alias_id,
    monthly_data_pkg_size,
    daily_data_pkg_id,
    daily_data_pkg_start_time,
    daily_data_pkg_end_time,
    weekly_data_pkg_id,
    weekly_data_pkg_start_time,
    weekly_data_pkg_end_time,
    monthly_data_pkg_id,
    monthly_data_pkg_start_time,
    monthly_data_pkg_end_time,
    voice_pkg_1_id,
    voice_pkg_1_start_time,
    voice_pkg_1_end_time,
    voice_pkg_2_id,
    voice_pkg_2_start_time,
    voice_pkg_2_end_time,
    voice_pkg_3_id,
    voice_pkg_3_start_time,
    voice_pkg_3_end_time,
    voice_pkg_4_id,
    voice_pkg_4_start_time,
    voice_pkg_4_end_time,
    voice_pkg_5_id,
    voice_pkg_5_start_time,
    voice_pkg_5_end_time,
    voice_pkg_6_id,
    voice_pkg_6_start_time,
    voice_pkg_6_end_time,
    voice_pkg_7_id,
    voice_pkg_7_start_time,
    voice_pkg_7_end_time,
    voice_pkg_8_id,
    voice_pkg_8_start_time,
    voice_pkg_8_end_time,
    voice_pkg_9_id,
    voice_pkg_9_start_time,
    voice_pkg_9_end_time,
    voice_pkg_10_id,
    voice_pkg_10_start_time,
    voice_pkg_10_end_time,
    voice_pkg_11_id,
    voice_pkg_11_start_time,
    voice_pkg_11_end_time,
    voice_pkg_12_id,
    voice_pkg_12_start_time,
    voice_pkg_12_end_time,
    voice_pkg_13_id,
    voice_pkg_13_start_time,
    voice_pkg_13_end_time,
    voice_pkg_14_id,
    voice_pkg_14_start_time,
    voice_pkg_14_end_time,
    voice_pkg_15_id,
    voice_pkg_15_start_time,
    voice_pkg_15_end_time
  )
  SELECT
    aa.alias_id,
    CASE
      WHEN dat.duration_type = 'Monthly' AND dat.data_volume <= 500 THEN 'S'
      WHEN dat.duration_type = 'Monthly' AND dat.data_volume BETWEEN 501 AND 2000 THEN 'M'
      WHEN dat.duration_type = 'Monthly' AND dat.data_volume BETWEEN 2001 AND 5000 THEN 'L'
      WHEN dat.duration_type = 'Monthly' AND dat.data_volume > 5000 THEN 'XL'
      ELSE NULL
    END AS monthly_data_pkg_size,
    CASE WHEN dat.duration_type = 'Daily' THEN dat.package_id ELSE NULL END AS daily_data_pkg_id,
    CASE WHEN dat.duration_type = 'Daily' THEN dat.pkg_start_time ELSE NULL END AS daily_data_pkg_start_time,
    CASE WHEN dat.duration_type = 'Daily' THEN dat.pkg_end_time ELSE NULL END AS daily_data_pkg_end_time,
    CASE WHEN dat.duration_type = 'Weekly' THEN dat.package_id ELSE NULL END AS weekly_data_pkg_id,
    CASE WHEN dat.duration_type = 'Weekly' THEN dat.pkg_start_time ELSE NULL END AS weekly_data_pkg_start_time,
    CASE WHEN dat.duration_type = 'Weekly' THEN dat.pkg_end_time ELSE NULL END AS weekly_data_pkg_end_time,
    CASE WHEN dat.duration_type = 'Monthly' THEN dat.package_id ELSE NULL END AS monthly_data_pkg_id,
    CASE WHEN dat.duration_type = 'Monthly' THEN dat.pkg_start_time ELSE NULL END AS monthly_data_pkg_start_time,
    CASE WHEN dat.duration_type = 'Monthly' THEN dat.pkg_end_time ELSE NULL END AS monthly_data_pkg_end_time,
    array_to_string(array_agg(cal.voice_pkg_1_id), ',') AS voice_pkg_1_id,
    array_to_string(array_agg(cal.voice_pkg_1_start_time), ',') AS voice_pkg_1_start_time,
    array_to_string(array_agg(cal.voice_pkg_1_end_time), ',') AS voice_pkg_1_end_time,
    array_to_string(array_agg(cal.voice_pkg_2_id), ',') AS voice_pkg_2_id,
    array_to_string(array_agg(cal.voice_pkg_2_start_time), ',') AS voice_pkg_2_start_time,
    array_to_string(array_agg(cal.voice_pkg_2_end_time), ',') AS voice_pkg_2_end_time,
    array_to_string(array_agg(cal.voice_pkg_3_id), ',') AS voice_pkg_3_id,
    array_to_string(array_agg(cal.voice_pkg_3_start_time), ',') AS voice_pkg_3_start_time,
    array_to_string(array_agg(cal.voice_pkg_3_end_time), ',') AS voice_pkg_3_end_time,
    array_to_string(array_agg(cal.voice_pkg_4_id), ',') AS voice_pkg_4_id,
    array_to_string(array_agg(cal.voice_pkg_4_start_time), ',') AS voice_pkg_4_start_time,
    array_to_string(array_agg(cal.voice_pkg_4_end_time), ',') AS voice_pkg_4_end_time,
    array_to_string(array_agg(cal.voice_pkg_5_id), ',') AS voice_pkg_5_id,
    array_to_string(array_agg(cal.voice_pkg_5_start_time), ',') AS voice_pkg_5_start_time,
    array_to_string(array_agg(cal.voice_pkg_5_end_time), ',') AS voice_pkg_5_end_time,
    array_to_string(array_agg(cal.voice_pkg_6_id), ',') AS voice_pkg_6_id,
    array_to_string(array_agg(cal.voice_pkg_6_start_time), ',') AS voice_pkg_6_start_time,
    array_to_string(array_agg(cal.voice_pkg_6_end_time), ',') AS voice_pkg_6_end_time,
    array_to_string(array_agg(cal.voice_pkg_7_id), ',') AS voice_pkg_7_id,
    array_to_string(array_agg(cal.voice_pkg_7_start_time), ',') AS voice_pkg_7_start_time,
    array_to_string(array_agg(cal.voice_pkg_7_end_time), ',') AS voice_pkg_7_end_time,
    array_to_string(array_agg(cal.voice_pkg_8_id), ',') AS voice_pkg_8_id,
    array_to_string(array_agg(cal.voice_pkg_8_end_time), ',') AS voice_pkg_8_end_time,
    array_to_string(array_agg(cal.voice_pkg_8_start_time), ',') AS voice_pkg_8_start_time,
    array_to_string(array_agg(cal.voice_pkg_9_id), ',') AS voice_pkg_9_id,
    array_to_string(array_agg(cal.voice_pkg_9_start_time), ',') AS voice_pkg_9_start_time,
    array_to_string(array_agg(cal.voice_pkg_9_end_time), ',') AS voice_pkg_9_end_time,
    array_to_string(array_agg(cal.voice_pkg_10_id), ',') AS voice_pkg_10_id,
    array_to_string(array_agg(cal.voice_pkg_10_start_time), ',') AS voice_pkg_10_start_time,
    array_to_string(array_agg(cal.voice_pkg_10_end_time), ',') AS voice_pkg_10_end_time,
    array_to_string(array_agg(cal.voice_pkg_11_id), ',') AS voice_pkg_11_id,
    array_to_string(array_agg(cal.voice_pkg_11_start_time), ',') AS voice_pkg_11_start_time,
    array_to_string(array_agg(cal.voice_pkg_11_end_time), ',') AS voice_pkg_11_end_time,
    array_to_string(array_agg(cal.voice_pkg_12_id), ',') AS voice_pkg_12_id,
    array_to_string(array_agg(cal.voice_pkg_12_start_time), ',') AS voice_pkg_12_start_time,
    array_to_string(array_agg(cal.voice_pkg_12_end_time), ',') AS voice_pkg_12_end_time,
    array_to_string(array_agg(cal.voice_pkg_13_id), ',') AS voice_pkg_13_id,
    array_to_string(array_agg(cal.voice_pkg_13_start_time), ',') AS voice_pkg_13_start_time,
    array_to_string(array_agg(cal.voice_pkg_13_end_time), ',') AS voice_pkg_13_end_time,
    array_to_string(array_agg(cal.voice_pkg_14_id), ',') AS voice_pkg_14_id,
    array_to_string(array_agg(cal.voice_pkg_14_start_time), ',') AS voice_pkg_14_start_time,
    array_to_string(array_agg(cal.voice_pkg_14_end_time), ',') AS voice_pkg_14_end_time,
    array_to_string(array_agg(cal.voice_pkg_15_id), ',') AS voice_pkg_15_id,
    array_to_string(array_agg(cal.voice_pkg_15_start_time), ',') AS voice_pkg_15_start_time,
    array_to_string(array_agg(cal.voice_pkg_15_end_time), ',') AS voice_pkg_15_end_time
  FROM (
    SELECT DISTINCT alias_id FROM tmp.rtp_aggregation_product_tmp
  ) aa
  LEFT JOIN (
    SELECT
      alias_id,
      package_id,
      data_volume / (1000 * 1000) AS data_volume, -- Convert from bytes into MB.
      CASE
        WHEN duration_of_package BETWEEN 1 AND 4 THEN 'Daily'
        WHEN duration_of_package BETWEEN 5 AND 19 THEN 'Weekly'
        WHEN duration_of_package BETWEEN 20 AND 30 THEN 'Monthly'
        ELSE 'Others' -- Temporarily verified by MCI
      END AS duration_type,
      pkg_start_time,
      pkg_end_time
    FROM (
      SELECT
        *,
    	row_number() OVER (PARTITION BY alias_id ORDER BY pkg_start_time DESC) AS pkg_rank
      FROM tmp.rtp_aggregation_product_tmp
      WHERE promo_type = 'Data'
    ) dat_tmp
    WHERE pkg_rank = 1 -- Considers only information on the latest data package
  ) dat
  ON aa.alias_id = dat.alias_id
  LEFT JOIN (
    SELECT
      alias_id,
      CASE WHEN pkg_id_rank = 1 THEN package_id ELSE NULL END AS voice_pkg_1_id,
      CASE WHEN pkg_id_rank = 1 THEN pkg_start_time ELSE NULL END AS voice_pkg_1_start_time,
      CASE WHEN pkg_id_rank = 1 THEN pkg_end_time ELSE NULL END AS voice_pkg_1_end_time,
      CASE WHEN pkg_id_rank = 2 THEN package_id ELSE NULL END AS voice_pkg_2_id,
      CASE WHEN pkg_id_rank = 2 THEN pkg_start_time ELSE NULL END AS voice_pkg_2_start_time,
      CASE WHEN pkg_id_rank = 2 THEN pkg_end_time ELSE NULL END AS voice_pkg_2_end_time,
      CASE WHEN pkg_id_rank = 3 THEN package_id ELSE NULL END AS voice_pkg_3_id,
      CASE WHEN pkg_id_rank = 3 THEN pkg_start_time ELSE NULL END AS voice_pkg_3_start_time,
      CASE WHEN pkg_id_rank = 3 THEN pkg_end_time ELSE NULL END AS voice_pkg_3_end_time,
      CASE WHEN pkg_id_rank = 4 THEN package_id ELSE NULL END AS voice_pkg_4_id,
      CASE WHEN pkg_id_rank = 4 THEN pkg_start_time ELSE NULL END AS voice_pkg_4_start_time,
      CASE WHEN pkg_id_rank = 4 THEN pkg_end_time ELSE NULL END AS voice_pkg_4_end_time,
      CASE WHEN pkg_id_rank = 5 THEN package_id ELSE NULL END AS voice_pkg_5_id,
      CASE WHEN pkg_id_rank = 5 THEN pkg_start_time ELSE NULL END AS voice_pkg_5_start_time,
      CASE WHEN pkg_id_rank = 5 THEN pkg_end_time ELSE NULL END AS voice_pkg_5_end_time,
      CASE WHEN pkg_id_rank = 6 THEN package_id ELSE NULL END AS voice_pkg_6_id,
      CASE WHEN pkg_id_rank = 6 THEN pkg_start_time ELSE NULL END AS voice_pkg_6_start_time,
      CASE WHEN pkg_id_rank = 6 THEN pkg_end_time ELSE NULL END AS voice_pkg_6_end_time,
      CASE WHEN pkg_id_rank = 7 THEN package_id ELSE NULL END AS voice_pkg_7_id,
      CASE WHEN pkg_id_rank = 7 THEN pkg_start_time ELSE NULL END AS voice_pkg_7_start_time,
      CASE WHEN pkg_id_rank = 7 THEN pkg_end_time ELSE NULL END AS voice_pkg_7_end_time,
      CASE WHEN pkg_id_rank = 8 THEN package_id ELSE NULL END AS voice_pkg_8_id,
      CASE WHEN pkg_id_rank = 8 THEN pkg_start_time ELSE NULL END AS voice_pkg_8_start_time,
      CASE WHEN pkg_id_rank = 8 THEN pkg_end_time ELSE NULL END AS voice_pkg_8_end_time,
      CASE WHEN pkg_id_rank = 9 THEN package_id ELSE NULL END AS voice_pkg_9_id,
      CASE WHEN pkg_id_rank = 9 THEN pkg_start_time ELSE NULL END AS voice_pkg_9_start_time,
      CASE WHEN pkg_id_rank = 9 THEN pkg_end_time ELSE NULL END AS voice_pkg_9_end_time,
      CASE WHEN pkg_id_rank = 10 THEN package_id ELSE NULL END AS voice_pkg_10_id,
      CASE WHEN pkg_id_rank = 10 THEN pkg_start_time ELSE NULL END AS voice_pkg_10_start_time,
      CASE WHEN pkg_id_rank = 10 THEN pkg_end_time ELSE NULL END AS voice_pkg_10_end_time,
      CASE WHEN pkg_id_rank = 11 THEN package_id ELSE NULL END AS voice_pkg_11_id,
      CASE WHEN pkg_id_rank = 11 THEN pkg_start_time ELSE NULL END AS voice_pkg_11_start_time,
      CASE WHEN pkg_id_rank = 11 THEN pkg_end_time ELSE NULL END AS voice_pkg_11_end_time,
      CASE WHEN pkg_id_rank = 12 THEN package_id ELSE NULL END AS voice_pkg_12_id,
      CASE WHEN pkg_id_rank = 12 THEN pkg_start_time ELSE NULL END AS voice_pkg_12_start_time,
      CASE WHEN pkg_id_rank = 12 THEN pkg_end_time ELSE NULL END AS voice_pkg_12_end_time,
      CASE WHEN pkg_id_rank = 13 THEN package_id ELSE NULL END AS voice_pkg_13_id,
      CASE WHEN pkg_id_rank = 13 THEN pkg_start_time ELSE NULL END AS voice_pkg_13_start_time,
      CASE WHEN pkg_id_rank = 13 THEN pkg_end_time ELSE NULL END AS voice_pkg_13_end_time,
      CASE WHEN pkg_id_rank = 14 THEN package_id ELSE NULL END AS voice_pkg_14_id,
      CASE WHEN pkg_id_rank = 14 THEN pkg_start_time ELSE NULL END AS voice_pkg_14_start_time,
      CASE WHEN pkg_id_rank = 14 THEN pkg_end_time ELSE NULL END AS voice_pkg_14_end_time,
      CASE WHEN pkg_id_rank = 15 THEN package_id ELSE NULL END AS voice_pkg_15_id,
      CASE WHEN pkg_id_rank = 15 THEN pkg_start_time ELSE NULL END AS voice_pkg_15_start_time,
      CASE WHEN pkg_id_rank = 15 THEN pkg_end_time ELSE NULL END AS voice_pkg_15_end_time
    FROM (
      SELECT
        alias_id,
        package_id,
        pkg_start_time,
        pkg_end_time,
        row_number() OVER (PARTITION BY alias_id ORDER BY package_id ASC) AS pkg_id_rank
      FROM (
        SELECT
          *,
          row_number() OVER (PARTITION BY alias_id ORDER BY pkg_start_time DESC) AS pkg_start_rank
        FROM tmp.rtp_aggregation_product_tmp
        WHERE promo_type = 'Voice'
      ) call_tmp1
      WHERE pkg_start_rank <= 15 -- considers only 15 latest voice packages
    ) call_tmp2
  ) cal
  ON aa.alias_id = cal.alias_id
  GROUP BY 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11
  ;
  
  ANALYZE tmp.rtp_aggregation_6_tmp;
  
  
  TRUNCATE tmp.rtp_aggregation_6;
  
  INSERT INTO tmp.rtp_aggregation_6 (
    mod_job_id,
    alias_id,
    monthly_data_pkg_size,
    daily_data_pkg_id,
    daily_data_pkg_start_time,
    daily_data_pkg_end_time,
    weekly_data_pkg_id,
    weekly_data_pkg_start_time,
    weekly_data_pkg_end_time,
    monthly_data_pkg_id,
    monthly_data_pkg_start_time,
    monthly_data_pkg_end_time,
    voice_pkg_1_id,
    voice_pkg_1_start_time,
    voice_pkg_1_end_time,
    voice_pkg_2_id,
    voice_pkg_2_start_time,
    voice_pkg_2_end_time,
    voice_pkg_3_id,
    voice_pkg_3_start_time,
    voice_pkg_3_end_time,
    voice_pkg_4_id,
    voice_pkg_4_start_time,
    voice_pkg_4_end_time,
    voice_pkg_5_id,
    voice_pkg_5_start_time,
    voice_pkg_5_end_time,
    voice_pkg_6_id,
    voice_pkg_6_start_time,
    voice_pkg_6_end_time,
    voice_pkg_7_id,
    voice_pkg_7_start_time,
    voice_pkg_7_end_time,
    voice_pkg_8_id,
    voice_pkg_8_start_time,
    voice_pkg_8_end_time,
    voice_pkg_9_id,
    voice_pkg_9_start_time,
    voice_pkg_9_end_time,
    voice_pkg_10_id,
    voice_pkg_10_start_time,
    voice_pkg_10_end_time,
    voice_pkg_11_id,
    voice_pkg_11_start_time,
    voice_pkg_11_end_time,
    voice_pkg_12_id,
    voice_pkg_12_start_time,
    voice_pkg_12_end_time,
    voice_pkg_13_id,
    voice_pkg_13_start_time,
    voice_pkg_13_end_time,
    voice_pkg_14_id,
    voice_pkg_14_start_time,
    voice_pkg_14_end_time,
    voice_pkg_15_id,
    voice_pkg_15_start_time,
    voice_pkg_15_end_time
  )
  SELECT
    in_mod_job_id,
    alias_id,
    monthly_data_pkg_size,
    daily_data_pkg_id,
    daily_data_pkg_start_time,
    daily_data_pkg_end_time,
    weekly_data_pkg_id,
    weekly_data_pkg_start_time,
    weekly_data_pkg_end_time,
    monthly_data_pkg_id,
    monthly_data_pkg_start_time,
    monthly_data_pkg_end_time,
    CASE WHEN voice_pkg_1_id <> '' THEN voice_pkg_1_id::integer ELSE NULL END,
    CASE WHEN voice_pkg_1_start_time <> '' THEN to_timestamp(voice_pkg_1_start_time, 'YYYY-MM-DD HH24:MI:SS') ELSE NULL END,
    CASE WHEN voice_pkg_1_end_time <> '' THEN to_timestamp(voice_pkg_1_end_time, 'YYYY-MM-DD HH24:MI:SS') ELSE NULL END,
    CASE WHEN voice_pkg_2_id <> '' THEN voice_pkg_2_id::integer ELSE NULL END,
    CASE WHEN voice_pkg_2_start_time <> '' THEN to_timestamp(voice_pkg_2_start_time, 'YYYY-MM-DD HH24:MI:SS') ELSE NULL END,
    CASE WHEN voice_pkg_2_end_time <> '' THEN to_timestamp(voice_pkg_2_end_time, 'YYYY-MM-DD HH24:MI:SS') ELSE NULL END,
    CASE WHEN voice_pkg_3_id <> '' THEN voice_pkg_3_id::integer ELSE NULL END,
    CASE WHEN voice_pkg_3_start_time <> '' THEN to_timestamp(voice_pkg_3_start_time, 'YYYY-MM-DD HH24:MI:SS') ELSE NULL END,
    CASE WHEN voice_pkg_3_end_time <> '' THEN to_timestamp(voice_pkg_3_end_time, 'YYYY-MM-DD HH24:MI:SS') ELSE NULL END,
    CASE WHEN voice_pkg_4_id <> '' THEN voice_pkg_4_id::integer ELSE NULL END,
    CASE WHEN voice_pkg_4_start_time <> '' THEN to_timestamp(voice_pkg_4_start_time, 'YYYY-MM-DD HH24:MI:SS') ELSE NULL END,
    CASE WHEN voice_pkg_4_end_time <> '' THEN to_timestamp(voice_pkg_4_end_time, 'YYYY-MM-DD HH24:MI:SS') ELSE NULL END,
    CASE WHEN voice_pkg_5_id <> '' THEN voice_pkg_5_id::integer ELSE NULL END,
    CASE WHEN voice_pkg_5_start_time <> '' THEN to_timestamp(voice_pkg_5_start_time, 'YYYY-MM-DD HH24:MI:SS') ELSE NULL END,
    CASE WHEN voice_pkg_5_end_time <> '' THEN to_timestamp(voice_pkg_5_end_time, 'YYYY-MM-DD HH24:MI:SS') ELSE NULL END,
    CASE WHEN voice_pkg_6_id <> '' THEN voice_pkg_6_id::integer ELSE NULL END,
    CASE WHEN voice_pkg_6_start_time <> '' THEN to_timestamp(voice_pkg_6_start_time, 'YYYY-MM-DD HH24:MI:SS') ELSE NULL END,
    CASE WHEN voice_pkg_6_end_time <> '' THEN to_timestamp(voice_pkg_6_end_time, 'YYYY-MM-DD HH24:MI:SS') ELSE NULL END,
    CASE WHEN voice_pkg_7_id <> '' THEN voice_pkg_7_id::integer ELSE NULL END,
    CASE WHEN voice_pkg_7_start_time <> '' THEN to_timestamp(voice_pkg_7_start_time, 'YYYY-MM-DD HH24:MI:SS') ELSE NULL END,
    CASE WHEN voice_pkg_7_end_time <> '' THEN to_timestamp(voice_pkg_7_end_time, 'YYYY-MM-DD HH24:MI:SS') ELSE NULL END,
    CASE WHEN voice_pkg_8_id <> '' THEN voice_pkg_8_id::integer ELSE NULL END,
    CASE WHEN voice_pkg_8_start_time <> '' THEN to_timestamp(voice_pkg_8_start_time, 'YYYY-MM-DD HH24:MI:SS') ELSE NULL END,
    CASE WHEN voice_pkg_8_end_time <> '' THEN to_timestamp(voice_pkg_8_end_time, 'YYYY-MM-DD HH24:MI:SS') ELSE NULL END,
    CASE WHEN voice_pkg_9_id <> '' THEN voice_pkg_9_id::integer ELSE NULL END,
    CASE WHEN voice_pkg_9_start_time <> '' THEN to_timestamp(voice_pkg_9_start_time, 'YYYY-MM-DD HH24:MI:SS') ELSE NULL END,
    CASE WHEN voice_pkg_9_end_time <> '' THEN to_timestamp(voice_pkg_9_end_time, 'YYYY-MM-DD HH24:MI:SS') ELSE NULL END,
    CASE WHEN voice_pkg_10_id <> '' THEN voice_pkg_10_id::integer ELSE NULL END,
    CASE WHEN voice_pkg_10_start_time <> '' THEN to_timestamp(voice_pkg_10_start_time, 'YYYY-MM-DD HH24:MI:SS') ELSE NULL END,
    CASE WHEN voice_pkg_10_end_time <> '' THEN to_timestamp(voice_pkg_10_end_time, 'YYYY-MM-DD HH24:MI:SS') ELSE NULL END,
    CASE WHEN voice_pkg_11_id <> '' THEN voice_pkg_11_id::integer ELSE NULL END,
    CASE WHEN voice_pkg_11_start_time <> '' THEN to_timestamp(voice_pkg_11_start_time, 'YYYY-MM-DD HH24:MI:SS') ELSE NULL END,
    CASE WHEN voice_pkg_11_end_time <> '' THEN to_timestamp(voice_pkg_11_end_time, 'YYYY-MM-DD HH24:MI:SS') ELSE NULL END,
    CASE WHEN voice_pkg_12_id <> '' THEN voice_pkg_12_id::integer ELSE NULL END,
    CASE WHEN voice_pkg_12_start_time <> '' THEN to_timestamp(voice_pkg_12_start_time, 'YYYY-MM-DD HH24:MI:SS') ELSE NULL END,
    CASE WHEN voice_pkg_12_end_time <> '' THEN to_timestamp(voice_pkg_12_end_time, 'YYYY-MM-DD HH24:MI:SS') ELSE NULL END,
    CASE WHEN voice_pkg_13_id <> '' THEN voice_pkg_13_id::integer ELSE NULL END,
    CASE WHEN voice_pkg_13_start_time <> '' THEN to_timestamp(voice_pkg_13_start_time, 'YYYY-MM-DD HH24:MI:SS') ELSE NULL END,
    CASE WHEN voice_pkg_13_end_time <> '' THEN to_timestamp(voice_pkg_13_end_time, 'YYYY-MM-DD HH24:MI:SS') ELSE NULL END,
    CASE WHEN voice_pkg_14_id <> '' THEN voice_pkg_14_id::integer ELSE NULL END,
    CASE WHEN voice_pkg_14_start_time <> '' THEN to_timestamp(voice_pkg_14_start_time, 'YYYY-MM-DD HH24:MI:SS') ELSE NULL END,
    CASE WHEN voice_pkg_14_end_time <> '' THEN to_timestamp(voice_pkg_14_end_time, 'YYYY-MM-DD HH24:MI:SS') ELSE NULL END,
    CASE WHEN voice_pkg_15_id <> '' THEN voice_pkg_15_id::integer ELSE NULL END,
    CASE WHEN voice_pkg_15_start_time <> '' THEN to_timestamp(voice_pkg_15_start_time, 'YYYY-MM-DD HH24:MI:SS') ELSE NULL END,
    CASE WHEN voice_pkg_15_end_time <> '' THEN to_timestamp(voice_pkg_15_end_time, 'YYYY-MM-DD HH24:MI:SS') ELSE NULL END
  FROM tmp.rtp_aggregation_6_tmp;
  
  ANALYZE tmp.rtp_aggregation_6;
  
END;

                </code>
            </input_parameters>
            <output_parameters>
                <out></out>
            </output_parameters>
        </RTP_Aggregation_3>
        <Score_Threshold>
            <input_parameters>
            </input_parameters>
            <output_parameters>
                <score_theshold>15</score_theshold>
            </output_parameters>
        </Score_Threshold>
        <Populate_RTPs>
            <input_parameters>
                <DataSource1></DataSource1>
                <queryparameters></queryparameters>
                <sql>select work.rtp_dump($mod_job_id, $score_threshold)</sql>
                <code>
                    
CREATE FUNCTION rtp_dump(this_dump_id integer, score_threshold integer) RETURNS text
    AS $$

/* SUMMARY
 * This function combines subscriber information from various sources (CRM, device TAC, pre-aggregates, big data aggregates and prediction scores) into a table.
 * The table is used for RTP MASS update and MCI use.
 *
 */

DECLARE

  t2 date;
  tcrm date;
  crm_date date;
  pre_aggregates_date date;
  device_tac_date date;
  
BEGIN

  t2 := mjp.value FROM work.module_job_parameters mjp WHERE mjp.mod_job_id = this_dump_id AND mjp.key = 't2';
  tcrm := mjp.value FROM work.module_job_parameters mjp WHERE mjp.mod_job_id = this_dump_id AND mjp.key = 'tCRM';
  SELECT max(date_inserted) INTO pre_aggregates_date FROM data.pre_aggregates WHERE date_inserted <= t2;
  SELECT max(date_inserted) INTO device_tac_date FROM data.device_tac WHERE date_inserted <= t2;

  INSERT INTO results.rtp_dump (
    msisdn,
    dump_id,
    pkg_to_payg_voice, pkg_to_payg_data, roam_int_rev_to_arpu, vas_cost_to_arpu, -- Pre-Aggregates
    payg_data_rev_to_arpu, onnet_to_offnet_rev, arpu, -- Pre-Aggregates
    status, -- CRM
    peak_type_voice, peak_type_data, -- RTP Agg 1
    payment_type, -- CRM
    monthly_data_pkg_size, -- RTP Agg 6
    is_multi_sim_phone, -- Pre-Aggregates
    daily_data_pkg_id, -- RTP Agg 6
    daily_data_pkg_start_time,  -- RTP Agg 6
    daily_data_pkg_end_time, -- RTP Agg 6
    weekly_data_pkg_id, -- RTP Agg 6
    weekly_data_pkg_start_time,  -- RTP Agg 6
    weekly_data_pkg_end_time, -- RTP Agg 6
    monthly_data_pkg_id, -- RTP Agg 6
    monthly_data_pkg_start_time,  -- RTP Agg 6
    monthly_data_pkg_end_time, -- RTP Agg 6
    voice_pkg_1_id, -- RTP Agg 6
    voice_pkg_1_start_time, -- RTP Agg 6
    voice_pkg_1_end_time, -- RTP Agg 6
    voice_pkg_2_id, -- RTP Agg 6
    voice_pkg_2_start_time, -- RTP Agg 6
    voice_pkg_2_end_time, -- RTP Agg 6
    voice_pkg_3_id, -- RTP Agg 6
    voice_pkg_3_start_time, -- RTP Agg 6
    voice_pkg_3_end_time, -- RTP Agg 6
    voice_pkg_4_id, -- RTP Agg 6
    voice_pkg_4_start_time, -- RTP Agg 6
    voice_pkg_4_end_time, -- RTP Agg 6
    voice_pkg_5_id, -- RTP Agg 6
    voice_pkg_5_start_time, -- RTP Agg 6
    voice_pkg_5_end_time, -- RTP Agg 6
    voice_pkg_6_id, -- RTP Agg 6
    voice_pkg_6_start_time, -- RTP Agg 6
    voice_pkg_6_end_time, -- RTP Agg 6
    voice_pkg_7_id, -- RTP Agg 6
    voice_pkg_7_start_time, -- RTP Agg 6
    voice_pkg_7_end_time, -- RTP Agg 6
    voice_pkg_8_id, -- RTP Agg 6
    voice_pkg_8_start_time, -- RTP Agg 6
    voice_pkg_8_end_time, -- RTP Agg 6
    voice_pkg_9_id, -- RTP Agg 6
    voice_pkg_9_start_time, -- RTP Agg 6
    voice_pkg_9_end_time, -- RTP Agg 6
    voice_pkg_10_id, -- RTP Agg 6
    voice_pkg_10_start_time, -- RTP Agg 6
    voice_pkg_10_end_time, -- RTP Agg 6
    voice_pkg_11_id, -- RTP Agg 6
    voice_pkg_11_start_time, -- RTP Agg 6
    voice_pkg_11_end_time, -- RTP Agg 6
    voice_pkg_12_id, -- RTP Agg 6
    voice_pkg_12_start_time, -- RTP Agg 6
    voice_pkg_12_end_time, -- RTP Agg 6
    voice_pkg_13_id, -- RTP Agg 6
    voice_pkg_13_start_time, -- RTP Agg 6
    voice_pkg_13_end_time, -- RTP Agg 6
    voice_pkg_14_id, -- RTP Agg 6
    voice_pkg_14_start_time, -- RTP Agg 6
    voice_pkg_14_end_time, -- RTP Agg 6
    voice_pkg_15_id, -- RTP Agg 6
    voice_pkg_15_start_time, -- RTP Agg 6
    voice_pkg_15_end_time, -- RTP Agg 6
    arpu_segment, -- Pre-Aggregates
    avg_monthly_topup_nmb, avg_monthly_topup_cost, -- RTP Agg 5
    is_smart_phone_user, -- Pre-Aggregates
    brand, model, os, -- Device TAC
    age, -- CRM, with calculation
    gender, -- CRM
    churn_propensity, -- Churn Propensity Score
    portout_propensity, -- Port-Out Propensity Score
    tenure, -- CRM, with calculation
    portout_status, -- Portability
    subscriber_segment, -- CRM
    contact_cust_care_ntwk, contact_cust_care_bill, contact_cust_care_promo, -- RTP Agg 4
    provience, -- CRM
    avg_onnet_voice_min, avg_offnet_voice_min, avg_nmb_sms_sent, avg_data_usage, -- Pre-Aggregates
    avg_business_voice_usage, avg_business_data_usage, avg_non_business_voice_usage, avg_non_business_data_usage, -- RTP Agg 2
    special_segments, b2b_service_subscription, company_type, -- CRM
    b2b_segments, industry, enterprise_national_id, -- CRM
    avg_daily_2g_data_usage, avg_daily_3g_data_usage, avg_daily_4g_data_usage, -- RTP Agg 3
    date_of_analysis)
  SELECT
    als.string_id,
    this_dump_id,
    preagg.pkg_to_payg_voice, preagg.pkg_to_payg_data, preagg.roam_int_rev_to_arpu, preagg.vas_cost_to_arpu,
    preagg.payg_data_rev_to_arpu, preagg.onnet_to_offnet_rev, preagg.arpu,
    crm.status,
    rtp1.peak_type_voice, rtp1.peak_type_data,
    crm.payment_type,
    rtp6.monthly_data_pkg_size,  
    CASE WHEN preagg.is_multi_sim_phone = 1 THEN 'Yes' ELSE 'No' END AS is_multi_sim_phone,
    rtp6.daily_data_pkg_id,
    to_char(rtp6.daily_data_pkg_start_time, 'YYYYMMDDHH24MISS'),
    to_char(rtp6.daily_data_pkg_end_time, 'YYYYMMDDHH24MISS'),
    rtp6.weekly_data_pkg_id,
    to_char(rtp6.weekly_data_pkg_start_time, 'YYYYMMDDHH24MISS'),
    to_char(rtp6.weekly_data_pkg_end_time, 'YYYYMMDDHH24MISS'),
    rtp6.monthly_data_pkg_id,
    to_char(rtp6.monthly_data_pkg_start_time, 'YYYYMMDDHH24MISS'),
    to_char(rtp6.monthly_data_pkg_end_time, 'YYYYMMDDHH24MISS'),
    rtp6.voice_pkg_1_id,
    to_char(rtp6.voice_pkg_1_start_time, 'YYYYMMDDHH24MISS'),
    to_char(rtp6.voice_pkg_1_end_time, 'YYYYMMDDHH24MISS'),
    rtp6.voice_pkg_2_id,
    to_char(rtp6.voice_pkg_2_start_time, 'YYYYMMDDHH24MISS'),
    to_char(rtp6.voice_pkg_2_end_time, 'YYYYMMDDHH24MISS'),
    rtp6.voice_pkg_3_id,
    to_char(rtp6.voice_pkg_3_start_time, 'YYYYMMDDHH24MISS'),
    to_char(rtp6.voice_pkg_3_end_time, 'YYYYMMDDHH24MISS'),
    rtp6.voice_pkg_4_id,
    to_char(rtp6.voice_pkg_4_start_time, 'YYYYMMDDHH24MISS'),
    to_char(rtp6.voice_pkg_4_end_time, 'YYYYMMDDHH24MISS'),
    rtp6.voice_pkg_5_id,
    to_char(rtp6.voice_pkg_5_start_time, 'YYYYMMDDHH24MISS'),
    to_char(rtp6.voice_pkg_5_end_time, 'YYYYMMDDHH24MISS'),
    rtp6.voice_pkg_6_id,
    to_char(rtp6.voice_pkg_6_start_time, 'YYYYMMDDHH24MISS'),
    to_char(rtp6.voice_pkg_6_end_time, 'YYYYMMDDHH24MISS'),
    rtp6.voice_pkg_7_id,
    to_char(rtp6.voice_pkg_7_start_time, 'YYYYMMDDHH24MISS'),
    to_char(rtp6.voice_pkg_7_end_time, 'YYYYMMDDHH24MISS'),
    rtp6.voice_pkg_8_id,
    to_char(rtp6.voice_pkg_8_start_time, 'YYYYMMDDHH24MISS'),
    to_char(rtp6.voice_pkg_8_end_time, 'YYYYMMDDHH24MISS'),
    rtp6.voice_pkg_9_id,
    to_char(rtp6.voice_pkg_9_start_time, 'YYYYMMDDHH24MISS'),
    to_char(rtp6.voice_pkg_9_end_time, 'YYYYMMDDHH24MISS'),
    rtp6.voice_pkg_10_id,
    to_char(rtp6.voice_pkg_10_start_time, 'YYYYMMDDHH24MISS'),
    to_char(rtp6.voice_pkg_10_end_time, 'YYYYMMDDHH24MISS'),
    rtp6.voice_pkg_11_id,
    to_char(rtp6.voice_pkg_11_start_time, 'YYYYMMDDHH24MISS'),
    to_char(rtp6.voice_pkg_11_end_time, 'YYYYMMDDHH24MISS'),
    rtp6.voice_pkg_12_id,
    to_char(rtp6.voice_pkg_12_start_time, 'YYYYMMDDHH24MISS'),
    to_char(rtp6.voice_pkg_12_end_time, 'YYYYMMDDHH24MISS'),
    rtp6.voice_pkg_13_id,
    to_char(rtp6.voice_pkg_13_start_time, 'YYYYMMDDHH24MISS'),
    to_char(rtp6.voice_pkg_13_end_time, 'YYYYMMDDHH24MISS'),
    rtp6.voice_pkg_14_id,
    to_char(rtp6.voice_pkg_14_start_time, 'YYYYMMDDHH24MISS'),
    to_char(rtp6.voice_pkg_14_end_time, 'YYYYMMDDHH24MISS'),
    rtp6.voice_pkg_15_id,
    to_char(rtp6.voice_pkg_15_start_time, 'YYYYMMDDHH24MISS'),
    to_char(rtp6.voice_pkg_15_end_time, 'YYYYMMDDHH24MISS'),
    preagg.arpu_segment,
    rtp5.avg_monthly_topup_nmb, rtp5.avg_monthly_topup_cost,
    CASE WHEN lower(dev.internal_model_name) ~ 'smart phone|smartphone' OR lower(dev.device_type) ~ 'smart phone|smartphone' THEN 'Yes' ELSE 'No' END AS is_smart_phone_user,
    dev.manufacturer, dev.model_name, dev.operating_system,
    date_part('year', crm.age) * 12 + date_part('month', crm.age) AS age,
    crm.gender,
    CASE
      WHEN score1.churn_percentile IS NOT NULL AND score1.churn_percentile <= score_threshold THEN 'High'
      WHEN score1.churn_percentile IS NOT NULL AND score1.churn_percentile > score_threshold THEN 'Low'
      ELSE NULL
    END AS churn_propensity, -- Take top 15% as high propensity scores
    CASE
      WHEN score2.portout_percentile <= score_threshold THEN 'High'
      WHEN score2.portout_percentile > score_threshold THEN 'Low'
      ELSE NULL
    END AS portout_propensity, -- Take top 15% as high propensity scores
    date_part('year', crm.tenure) * 12 + date_part('month', crm.tenure) AS tenure,
    portout.port_status AS portout_status,
    preagg.customer_segment,
    rtp4.contact_cust_care_ntwk, rtp4.contact_cust_care_bill, rtp4.contact_cust_care_promo,
    crm.provience,
    preagg.avg_onnet_voice_min, preagg.avg_offnet_voice_min, preagg.avg_nmb_sms_sent, preagg.avg_data_usage,
    rtp2.avg_business_voice_usage, rtp2.avg_business_data_usage, rtp2.avg_non_business_voice_usage, rtp2.avg_non_business_data_usage,
    crm.special_segments, crm.b2b_service_subscription, crm.company_type,
    crm.b2b_segments, crm.industry, crm.enterprise_national_id,
    rtp3.avg_daily_2g_data_usage, rtp3.avg_daily_3g_data_usage, rtp3.avg_daily_4g_data_usage,
    to_char(t2, 'YYYYMMDD') AS date_of_analysis
  FROM (
    SELECT
      *,
      age(now(), birth_date) AS age,
      age(now(), switch_on_date) AS tenure
    FROM data.in_crm WHERE date_inserted = tcrm
  ) crm
  LEFT JOIN aliases.string_id als
  ON crm.alias_id = als.alias_id
  LEFT JOIN (
    SELECT * FROM (
      SELECT
        *,
        row_number() OVER (PARTITION BY alias_id ORDER BY date_inserted DESC) AS date_rank
      FROM (
        SELECT
          alias_id,
          port_status,
          date_inserted
        FROM data.portability
        WHERE port_direction = 'portout'
        AND date_inserted <= t2
      ) portout_tmp1
    ) portout_tmp2
    WHERE date_rank = 1 -- Get the latest port-out information
  ) portout
  ON crm.alias_id = portout.alias_id
  LEFT JOIN (
    SELECT
      *
    FROM data.pre_aggregates
    WHERE date_inserted = pre_aggregates_date
  ) preagg
  ON crm.alias_id = preagg.alias_id
  LEFT JOIN (
    SELECT * FROM data.device_tac WHERE date_inserted = device_tac_date
  ) dev
  ON substring(preagg.imei, 1, 8) = dev.tac
  LEFT JOIN (
    SELECT
      mod_job_id,
      res_tmp.alias_id,
      churn_inactivity_propensity_score,
      ntile(100) OVER (PARTITION BY crm_tmp.payment_type ORDER BY churn_inactivity_propensity_score DESC) AS churn_percentile,
      date_inserted
    FROM results.module_results res_tmp
    LEFT JOIN (
      SELECT * FROM data.in_crm WHERE date_inserted = tcrm
    ) crm_tmp
    ON res_tmp.alias_id = crm_tmp.alias_id
    WHERE mod_job_id = this_dump_id
    AND churn_inactivity_propensity_score IS NOT NULL
  ) score1
  ON crm.alias_id = score1.alias_id
  LEFT JOIN (
    SELECT
      mod_job_id,
      alias_id,
      portout_propensity_score,
      ntile(100) OVER (ORDER BY portout_propensity_score DESC) AS portout_percentile
    FROM results.module_results
    WHERE mod_job_id = this_dump_id
    AND portout_propensity_score IS NOT NULL
  ) score2
  ON crm.alias_id = score2.alias_id
  LEFT JOIN tmp.rtp_aggregation_1 rtp1
  ON crm.alias_id = rtp1.alias_id
  LEFT JOIN tmp.rtp_aggregation_2 rtp2
  ON crm.alias_id = rtp2.alias_id
  LEFT JOIN tmp.rtp_aggregation_3 rtp3
  ON crm.alias_id = rtp3.alias_id
  LEFT JOIN tmp.rtp_aggregation_4 rtp4
  ON crm.alias_id = rtp4.alias_id
  LEFT JOIN tmp.rtp_aggregation_5 rtp5
  ON crm.alias_id = rtp5.alias_id
  LEFT JOIN tmp.rtp_aggregation_6 rtp6
  ON crm.alias_id = rtp6.alias_id
  ;
  
  RETURN 'rtp_dump_' || this_dump_id || '_' || to_char(now(), 'YYYYMMDDHH24MISS') || '.txt';
  
END;

                </code>
            </input_parameters>
            <output_parameters>
                <out></out>
                <DumpFileName>$out.get(0).get(0)</DumpFileName>
            </output_parameters>
        </Populate_RTPs>
        <RTP_Delta_or_Dump>
            <input_parameters>
                <DataSource1></DataSource1>
                <queryparameters></queryparameters>
                <sql>select work.rtp_dump_delta($mod_job_id, $new_delta_dump)</sql>
                <code>
                    
CREATE FUNCTION rtp_dump_delta(this_dump_id integer, new_delta_dump boolean) RETURNS text
    AS $$

/* SUMMARY
 * This function updates changes on subscriber information from various sources (CRM, device TAC, pre-aggregates, big data aggregates and prediction scores).
 * The table is used for RTP MASS update.
 * Product takeup information are excluded here.
 *
 */

DECLARE
  
  delta_version int;
  last_dump_id int;
  dump_exist_check int;
  
BEGIN

  SELECT nextval('work.delta_output_sequence') INTO delta_version;
  SELECT max(dump_id) INTO last_dump_id FROM results.rtp_dump WHERE dump_id < this_dump_id;

  IF new_delta_dump OR last_dump_id IS NULL THEN
  
    SELECT count(*) INTO dump_exist_check FROM results.rtp_dump_delta WHERE dump_id_x IS NULL AND dump_id_y = this_dump_id;
    
    IF dump_exist_check > 0 THEN -- make sure no duplicated dump is inserted into results.rtp_dump_delta table
    
      RAISE WARNING 'New delta dump for dump ID exists. Existing dump is used for output generation.';
      
      RETURN 'rtp_dump_delta_v' || delta_version || '_' || this_dump_id || '_new_' || to_char(now(), 'YYYYMMDDHH24MISS') || '.txt';

    ELSE
    
      INSERT INTO results.rtp_dump_delta(
        msisdn, dump_id_x, dump_id_y, payment_type_id,
        pkg_to_payg_voice, pkg_to_payg_data, roam_int_rev_to_arpu, vas_cost_to_arpu, -- Pre-Aggregates
        payg_data_rev_to_arpu, onnet_to_offnet_rev, arpu, -- Pre-Aggregates
        status, -- CRM
        peak_type_voice, peak_type_data, -- RTP Agg 1
        payment_type, -- CRM
        monthly_data_pkg_size, -- RTP Agg 6
        is_multi_sim_phone, -- Pre-Aggregates
        arpu_segment, -- Pre-Aggregates
        avg_monthly_topup_nmb, avg_monthly_topup_cost, -- RTP Agg 5
        is_smart_phone_user, -- Pre-Aggregates
        brand, model, os, -- Device TAC
        age, -- CRM, with calculation
        gender, -- CRM
        churn_propensity, -- Churn Propensity Score
        portout_propensity, -- Port-Out Propensity Score
        tenure, -- CRM, with calculation
        portout_status, -- Portability
        subscriber_segment, -- CRM
        contact_cust_care_ntwk, contact_cust_care_bill, contact_cust_care_promo, -- RTP Agg 4
        provience, -- CRM
        avg_onnet_voice_min, avg_offnet_voice_min, avg_nmb_sms_sent, avg_data_usage, -- Pre-Aggregates
        avg_business_voice_usage, avg_business_data_usage, avg_non_business_voice_usage, avg_non_business_data_usage, -- RTP Agg 2
        special_segments, b2b_service_subscription, company_type, -- CRM
        b2b_segments, industry, enterprise_national_id, -- CRM
        avg_daily_2g_data_usage, avg_daily_3g_data_usage, avg_daily_4g_data_usage, -- RTP Agg 3
        date_of_analysis)
      SELECT
        msisdn,
        NULL,
        this_dump_id,
        payment_type,
        pkg_to_payg_voice,
        pkg_to_payg_data,
        roam_int_rev_to_arpu,
        vas_cost_to_arpu,
        payg_data_rev_to_arpu,
        onnet_to_offnet_rev,
        arpu,
        status,
        peak_type_voice,
        peak_type_data,
        payment_type,
        monthly_data_pkg_size,
        is_multi_sim_phone,
        arpu_segment,
        avg_monthly_topup_nmb,
        avg_monthly_topup_cost,
        is_smart_phone_user,
        brand,
        model,
        os,
        age,
        gender,
        churn_propensity,
        portout_propensity,
        tenure,
        portout_status,
        subscriber_segment,
        contact_cust_care_ntwk,
        contact_cust_care_bill,
        contact_cust_care_promo,
        provience,
        avg_onnet_voice_min,
        avg_offnet_voice_min,
        avg_nmb_sms_sent,
        avg_data_usage,
        avg_business_voice_usage,
        avg_business_data_usage,
        avg_non_business_voice_usage,
        avg_non_business_data_usage,
        special_segments,
        b2b_service_subscription,
        company_type,
        b2b_segments,
        industry,
        enterprise_national_id,
        avg_daily_2g_data_usage,
        avg_daily_3g_data_usage,
        avg_daily_4g_data_usage,
        date_of_analysis
      FROM results.rtp_dump
      WHERE dump_id = this_dump_id;
      
      RETURN 'rtp_dump_delta_v' || delta_version || '_' || this_dump_id || '_new_' || to_char(now(), 'YYYYMMDDHH24MISS') || '.txt';
    
    END IF;

  ELSE

    SELECT count(*) INTO dump_exist_check FROM results.rtp_dump_delta WHERE dump_id_x = last_dump_id AND dump_id_y = this_dump_id;
    
    IF dump_exist_check > 0 THEN -- make sure no duplicated dump is inserted into results.rtp_dump_delta table
    
      RAISE WARNING 'Delta dump for dump ID X and dump ID Y exists. Existing dump is used for output generation.';
      
      RETURN 'rtp_dump_delta_v' || delta_version || '_' || this_dump_id || '_' || last_dump_id || '_' || to_char(now(), 'YYYYMMDDHH24MISS') || '.txt';

    ELSE
  
      INSERT INTO results.rtp_dump_delta (
        msisdn, dump_id_x, dump_id_y, payment_type_id,
        pkg_to_payg_voice, pkg_to_payg_data, roam_int_rev_to_arpu, vas_cost_to_arpu, -- Pre-Aggregates
        payg_data_rev_to_arpu, onnet_to_offnet_rev, arpu, -- Pre-Aggregates
        status, -- CRM
        peak_type_voice, peak_type_data, -- RTP Agg 1
        payment_type, -- CRM
        monthly_data_pkg_size, -- RTP Agg 6
        is_multi_sim_phone, -- Pre-Aggregates
        arpu_segment, -- Pre-Aggregates
        avg_monthly_topup_nmb, avg_monthly_topup_cost, -- RTP Agg 5
        is_smart_phone_user, -- Pre-Aggregates
        brand, model, os, -- Device TAC
        age, -- CRM, with calculation
        gender, -- CRM
        churn_propensity, -- Churn Propensity Score
        portout_propensity, -- Port-Out Propensity Score
        tenure, -- CRM, with calculation
        portout_status, -- Portability
        subscriber_segment, -- CRM
        contact_cust_care_ntwk, contact_cust_care_bill, contact_cust_care_promo, -- RTP Agg 4
        provience, -- CRM
        avg_onnet_voice_min, avg_offnet_voice_min, avg_nmb_sms_sent, avg_data_usage, -- Pre-Aggregates
        avg_business_voice_usage, avg_business_data_usage, avg_non_business_voice_usage, avg_non_business_data_usage, -- RTP Agg 2
        special_segments, b2b_service_subscription, company_type, -- CRM
        b2b_segments, industry, enterprise_national_id, -- CRM
        avg_daily_2g_data_usage, avg_daily_3g_data_usage, avg_daily_4g_data_usage, -- RTP Agg 3
        date_of_analysis)
      SELECT
        newer.msisdn,
        last_dump_id,
        this_dump_id,
        newer.payment_type,
        CASE
          WHEN coalesce(older.pkg_to_payg_voice, -999) = coalesce(newer.pkg_to_payg_voice, -999) THEN NULL
          ELSE coalesce(newer.pkg_to_payg_voice, -999) END AS pkg_to_payg_voice,
        CASE
          WHEN coalesce(older.pkg_to_payg_data, -999) = coalesce(newer.pkg_to_payg_data, -999) THEN NULL
          ELSE coalesce(newer.pkg_to_payg_data, -999) END AS pkg_to_payg_data,
        CASE
          WHEN coalesce(older.roam_int_rev_to_arpu, -999) = coalesce(newer.roam_int_rev_to_arpu, -999) THEN NULL
          ELSE coalesce(newer.roam_int_rev_to_arpu, -999) END AS roam_int_rev_to_arpu,
        CASE
          WHEN coalesce(older.vas_cost_to_arpu, -999) = coalesce(newer.vas_cost_to_arpu, -999) THEN NULL
          ELSE coalesce(newer.vas_cost_to_arpu, -999) END AS vas_cost_to_arpu,
        CASE
          WHEN coalesce(older.payg_data_rev_to_arpu, -999) = coalesce(newer.payg_data_rev_to_arpu, -999) THEN NULL
          ELSE coalesce(newer.payg_data_rev_to_arpu, -999) END AS payg_data_rev_to_arpu,
        CASE
          WHEN coalesce(older.onnet_to_offnet_rev, -999) = coalesce(newer.onnet_to_offnet_rev, -999) THEN NULL
          ELSE coalesce(newer.onnet_to_offnet_rev, -999) END AS onnet_to_offnet_rev,
        CASE
          WHEN coalesce(older.arpu, -999) = coalesce(newer.arpu, -999) THEN NULL
          ELSE coalesce(newer.arpu, -999) END AS arpu,
        CASE
          WHEN coalesce(older.status, 'NA') = coalesce(newer.status, 'NA') THEN NULL
          ELSE coalesce(newer.status, 'NA') END AS status,
        CASE
          WHEN coalesce(older.peak_type_voice, 'NA') = coalesce(newer.peak_type_voice, 'NA') THEN NULL
          ELSE coalesce(newer.peak_type_voice, 'NA') END AS peak_type_voice,
        CASE
          WHEN coalesce(older.peak_type_data, 'NA') = coalesce(newer.peak_type_data, 'NA') THEN NULL
          ELSE coalesce(newer.peak_type_data, 'NA') END AS peak_type_data,
        CASE
          WHEN coalesce(older.payment_type, 'NA') = coalesce(newer.payment_type, 'NA') THEN NULL
          ELSE coalesce(newer.payment_type, 'NA') END AS payment_type,
        CASE
          WHEN coalesce(older.monthly_data_pkg_size, 'NA') = coalesce(newer.monthly_data_pkg_size, 'NA') THEN NULL
          ELSE coalesce(newer.monthly_data_pkg_size, 'NA') END AS monthly_data_pkg_size,
        CASE
          WHEN coalesce(older.is_multi_sim_phone, 'NA') = coalesce(newer.is_multi_sim_phone, 'NA') THEN NULL
          ELSE coalesce(newer.is_multi_sim_phone, 'NA') END AS is_multi_sim_phone,
        CASE
          WHEN coalesce(older.arpu_segment, 'NA') = coalesce(newer.arpu_segment, 'NA') THEN NULL
          ELSE coalesce(newer.arpu_segment, 'NA') END AS arpu_segment,
        CASE
          WHEN coalesce(older.avg_monthly_topup_nmb, -999) = coalesce(newer.avg_monthly_topup_nmb, -999) THEN NULL
          ELSE coalesce(newer.avg_monthly_topup_nmb, -999) END AS avg_monthly_topup_nmb,
        CASE
          WHEN coalesce(older.avg_monthly_topup_cost, -999) = coalesce(newer.avg_monthly_topup_cost, -999) THEN NULL
          ELSE coalesce(newer.avg_monthly_topup_cost, -999) END AS avg_monthly_topup_cost,
        CASE
          WHEN coalesce(older.is_smart_phone_user, 'NA') = coalesce(newer.is_smart_phone_user, 'NA') THEN NULL
          ELSE coalesce(newer.is_smart_phone_user, 'NA') END AS is_smart_phone_user,
        CASE
          WHEN coalesce(older.brand, 'NA') = coalesce(newer.brand, 'NA') THEN NULL
          ELSE coalesce(newer.brand, 'NA') END AS brand,
        CASE
          WHEN coalesce(older.model, 'NA') = coalesce(newer.model, 'NA') THEN NULL
          ELSE coalesce(newer.model, 'NA') END AS model,
        CASE
          WHEN coalesce(older.os, 'NA') = coalesce(newer.os, 'NA') THEN NULL
          ELSE coalesce(newer.os, 'NA') END AS os,
        CASE
          WHEN coalesce(older.age, -999) = coalesce(newer.age, -999) THEN NULL
          ELSE coalesce(newer.age, -999) END AS age,
        CASE
          WHEN coalesce(older.gender, 'NA') = coalesce(newer.gender, 'NA') THEN NULL
          ELSE coalesce(newer.gender, 'NA') END AS gender,
        CASE
          WHEN coalesce(older.churn_propensity, 'NA') = coalesce(newer.churn_propensity, 'NA') THEN NULL
          ELSE coalesce(newer.churn_propensity, 'NA') END AS churn_propensity,
        CASE
          WHEN coalesce(older.portout_propensity, 'NA') = coalesce(newer.portout_propensity, 'NA') THEN NULL
          ELSE coalesce(newer.portout_propensity, 'NA') END AS portout_propensity,
        CASE
          WHEN coalesce(older.tenure, -999) = coalesce(newer.tenure, -999) THEN NULL
          ELSE coalesce(newer.tenure, -999) END AS tenure,
        CASE
          WHEN coalesce(older.portout_status, 'NA') = coalesce(newer.portout_status, 'NA') THEN NULL
          ELSE coalesce(newer.portout_status, 'NA') END AS portout_status,
        CASE
          WHEN coalesce(older.subscriber_segment, 'NA') = coalesce(newer.subscriber_segment, 'NA') THEN NULL
          ELSE coalesce(newer.subscriber_segment, 'NA') END AS subscriber_segment,
        CASE
          WHEN coalesce(older.contact_cust_care_ntwk, -999) = coalesce(newer.contact_cust_care_ntwk, -999) THEN NULL
          ELSE coalesce(newer.contact_cust_care_ntwk, -999) END AS contact_cust_care_ntwk,
        CASE
          WHEN coalesce(older.contact_cust_care_bill, -999) = coalesce(newer.contact_cust_care_bill, -999) THEN NULL
          ELSE coalesce(newer.contact_cust_care_bill, -999) END AS contact_cust_care_bill,
        CASE
          WHEN coalesce(older.contact_cust_care_promo, -999) = coalesce(newer.contact_cust_care_promo, -999) THEN NULL
          ELSE coalesce(newer.contact_cust_care_promo, -999) END AS contact_cust_care_promo,
        CASE
          WHEN coalesce(older.provience, 'NA') = coalesce(newer.provience, 'NA') THEN NULL
          ELSE coalesce(newer.provience, 'NA') END AS provience,
        CASE
          WHEN coalesce(older.avg_onnet_voice_min, -999) = coalesce(newer.avg_onnet_voice_min, -999) THEN NULL
          ELSE coalesce(newer.avg_onnet_voice_min, -999) END AS avg_onnet_voice_min,
        CASE
          WHEN coalesce(older.avg_offnet_voice_min, -999) = coalesce(newer.avg_offnet_voice_min, -999) THEN NULL
          ELSE coalesce(newer.avg_offnet_voice_min, -999) END AS avg_offnet_voice_min,
        CASE
          WHEN coalesce(older.avg_nmb_sms_sent, -999) = coalesce(newer.avg_nmb_sms_sent, -999) THEN NULL
          ELSE coalesce(newer.avg_nmb_sms_sent, -999) END AS avg_nmb_sms_sent,
        CASE
          WHEN coalesce(older.avg_data_usage, -999) = coalesce(newer.avg_data_usage, -999) THEN NULL
          ELSE coalesce(newer.avg_data_usage, -999) END AS avg_data_usage,
        CASE
          WHEN coalesce(older.avg_business_voice_usage, -999) = coalesce(newer.avg_business_voice_usage, -999) THEN NULL
          ELSE coalesce(newer.avg_business_voice_usage, -999) END AS avg_business_voice_usage,
        CASE
          WHEN coalesce(older.avg_business_data_usage, -999) = coalesce(newer.avg_business_data_usage, -999) THEN NULL
          ELSE coalesce(newer.avg_business_data_usage, -999) END AS avg_business_data_usage,
        CASE
          WHEN coalesce(older.avg_non_business_voice_usage, -999) = coalesce(newer.avg_non_business_voice_usage, -999) THEN NULL
          ELSE coalesce(newer.avg_non_business_voice_usage, -999) END AS avg_non_business_voice_usage,
        CASE
          WHEN coalesce(older.avg_non_business_data_usage, -999) = coalesce(newer.avg_non_business_data_usage, -999) THEN NULL
          ELSE coalesce(newer.avg_non_business_data_usage, -999) END AS avg_non_business_data_usage,
        CASE
          WHEN coalesce(older.special_segments, 'NA') = coalesce(newer.special_segments, 'NA') THEN NULL
          ELSE coalesce(newer.special_segments, 'NA') END AS special_segments,
        CASE
          WHEN coalesce(older.b2b_service_subscription, 'NA') = coalesce(newer.b2b_service_subscription, 'NA') THEN NULL
          ELSE coalesce(newer.b2b_service_subscription, 'NA') END AS b2b_service_subscription,
        CASE
          WHEN coalesce(older.company_type, 'NA') = coalesce(newer.company_type, 'NA') THEN NULL
          ELSE coalesce(newer.company_type, 'NA') END AS company_type,
        CASE
          WHEN coalesce(older.b2b_segments, 'NA') = coalesce(newer.b2b_segments, 'NA') THEN NULL
          ELSE coalesce(newer.b2b_segments, 'NA') END AS b2b_segments,
        CASE
          WHEN coalesce(older.industry, 'NA') = coalesce(newer.industry, 'NA') THEN NULL
          ELSE coalesce(newer.industry, 'NA') END AS industry,
        CASE
          WHEN coalesce(older.enterprise_national_id, 'NA') = coalesce(newer.enterprise_national_id, 'NA') THEN NULL
          ELSE coalesce(newer.enterprise_national_id, 'NA') END AS enterprise_national_id,
        CASE
          WHEN coalesce(older.avg_daily_2g_data_usage, -999) = coalesce(newer.avg_daily_2g_data_usage, -999) THEN NULL
          ELSE coalesce(newer.avg_daily_2g_data_usage, -999) END AS avg_daily_2g_data_usage,
        CASE
          WHEN coalesce(older.avg_daily_3g_data_usage, -999) = coalesce(newer.avg_daily_3g_data_usage, -999) THEN NULL
          ELSE coalesce(newer.avg_daily_3g_data_usage, -999) END AS avg_daily_3g_data_usage,
        CASE
          WHEN coalesce(older.avg_daily_4g_data_usage, -999) = coalesce(newer.avg_daily_4g_data_usage, -999) THEN NULL
          ELSE coalesce(newer.avg_daily_4g_data_usage, -999) END AS avg_daily_4g_data_usage,
        newer.date_of_analysis
      FROM
      (SELECT * FROM results.rtp_dump AS newer WHERE dump_id = this_dump_id) AS newer
      LEFT JOIN
      (SELECT * FROM results.rtp_dump AS older WHERE dump_id = last_dump_id) AS older
      ON newer.msisdn = older.msisdn
      ;
    END IF;

    RETURN 'rtp_dump_delta_v' || delta_version || '_' || this_dump_id || '_' || last_dump_id || '_' || to_char(now(), 'YYYYMMDDHH24MISS') || '.txt';
    
  END IF;
  
END;

                </code>
            </input_parameters>
            <output_parameters>
                <out></out>
                <DeltaFileName>$out.get(0).get(0)</DeltaFileName>
            </output_parameters>
        </RTP_Delta_or_Dump>
    </RTPAggregationFlow>
    <RTPOutputGenerateFlow_CSV>
        <split>
            <when>parameter with name equal to DumpFileName and value equal to null or name equal to DeltaFileName and value equal to null</when>
            <tasks>Fault:RTP Dump or Delta file names not available</tasks>
            <when></when>
            <tasks>split:Objective</tasks>
        </split>
        <split:Objective>
            <when></when>
            <tasks>Get_Query, Export_RTP_Dump, split, Transfer_Files, join</tasks>
            <when></when>
            <tasks>split,Export_delta_Dump, split, Convert_Fiels, join</tasks>
        </split:Objective>
        <Get_Query>
            <input_parameters>
            </input_parameters>
            <output_parameters>
                <ItemQueryDump>"select * from results.rtp_dump where dump_id = $mod_job_id"</ItemQueryDump>
            </output_parameters>
        </Get_Query>
        <Export_RTP_Dump>
        </Export_RTP_Dump>
        <split>
            <when>parameter with name equal to exitstatus and int value not equal to 0</when>
            <tasks>Transfer_Files, join</tasks>
            <when>parameter with name equal to exitstatus and int value equal to 0</when>
            <tasks>fualt:Non-zero exit status</tasks>
        </split>
        <Transfer_Files>
        </Transfer_Files>
        <split>
            <when>parameter with name equal to new_delta_dump and str value equal to true</when>
            <tasks>GetQuery1, join</tasks>
            <when>parameter with name equal to new_delta_dump and str value equal to false</when>
            <tasks>GetQuery2, join</tasks>
        </split>
        <GetQuery1>
        </GetQuery1>
        <GetQuery2>
        </GetQuery2>
        <Export_Delta_Dump>
        </Export_Delta_Dump>
        <split>
            <when>parameter with name equal to exitstatus and str value not equal to 0</when>
            <tasks>fualt:Non-zero exit status</tasks>
            <when>parameter with name equal to exitstatus and str value equal to 0</when>
            <tasks>Convert_Fiels, join</tasks>
        </split>
        <Convert_Fiels>
        </Convert_Fiels>
    </RTPOutputGenerateFlow_CSV>
    <Export_MCI>
        <Setup_Parameters>
        </Setup_Parameters>
        <List_new_input_files>
        </List_new_input_files>
        <List_precious_errors_files>
        </List_precious_errors_files>
        <split>
            <when>Parameter with name equal to out_is_present and value equal to true</when>
            <tasks>split, Move_error_files_to_archive, join</tasks>
            <when>Parameter with name equal to out_is_present and value equal to false</when>
            <tasks>split, Set_checkpoint_message</tasks>
        </split>
        <split>
            <when>Parameter with name equal to err_is_present and value equal to true</when>
            <tasks>Move_error_files_to_archive, join</tasks>
            <when>Parameter with name equal to err_is_present and value equal to false</when>
            <tasks>join</tasks>
        </split>
        <Move_error_files_to_archive>
        </Move_error_files_to_archive>
        <RtpServiceTask>
        </RtpServiceTask>
        <Remove_File>
        </Remove_File>
        <Set_checkpoint_message>
        </Set_checkpoint_message>
        <split>
            <when>Parameter with name equal to err_is_present and value equal to true</when>
            <tasks>Set_checkpoint_message, join</tasks>
            <when>Parameter with name equal to err_is_present and value equal to false</when>
            <tasks>join</tasks>
        </split>
        <Set_checkpoint_message>
        </Set_checkpoint_message>
        <RtpServiceTask>
        </RtpServiceTask>
        <Remove_error_file>
        </Remove_error_file>
        <Set_checkpoint_message>
        </Set_checkpoint_message>
    </Export_MCI>
    <email3>
        <input_parameters>
            <command>echo "RTP aggregation and export completed for" + $t2 | mailx -v -r "churn@mci.ir" -s "FAA Scoring run" $emailaddress</command>
            <host>localhost</host>
            <identity></identity>
            <password>$LocalhostPassword</password>
            <std.err.file></stderr.file>
            <stdout.file></stdout.file>
            <timeout>20000</timeout>
            <username>$LocalhostUsername</username>
        </input_parameters>
        <output_parameters>
            <exitstatus></exitstatus>
            <stderr></stderr>
            <stdout></stdout>
        </output_parameters>
    </email3>
    <Info:Data_Perion_End>
        <input_parameters>
            <DataSource1></DataSource1>
            <queryparameters></queryparameters>
            <sql>insert into  core_runbean_attributes(runbean_id, attributes_key, attributes) values ($WORKFLOW_RUN_ID, 'dataPerionEnd', EXTRACT(EPOCH FROM '$t2'::date)*1000)</sql>
        </input_parameters>
        <output_parameters>
            <out></out>
        </output_parameters>
    </Info:Data_Perion_End>
    <Workflow_end_time>
        <input_parameters>
            <DataSource1></DataSource1>
            <queryparameters></queryparameters>
            <sql>insert into work.module_job_parameters (mod_job_id, key, value) values ($mod_job_id, 'time_finished', now()::text)</sql>
        </input_parameters>
        <output_parameters>
            <out></out>
        </output_parameters>
    </Workflow_end_time>
    <Approval_Statistics>
        <input_parameters>
            <DataSource1></DataSource1>
            <queryparameters></queryparameters>
            <sql>select * from results.calculate_score_list_approval_stats($mod_job_id)</sql>
            <code>
                
CREATE FUNCTION calculate_score_list_approval_stats(in_mod_job_id integer) RETURNS void
    AS $$ 
/* SUMMARY:
 * This function calculates approval statistics for scored subscribers. 
 * Do not take the score into account, just other properties.
 * Note that most of the statistics use median as the value. 
 * Sometimes this can have unwanted side effects like zeros for all. 
 *
 * 2012-09-28 MOj: ICIF-59
 * 2012-08-10 TSi: Original version
 */
DECLARE
  t1 date; 
  t2 date; 

  job_use_cases text;
  use_cases text[];
  use_case text;
  sql_string text;
  
BEGIN
  t1 := mjp.value::date FROM work.module_job_parameters AS mjp WHERE mjp.mod_job_id = in_mod_job_id AND mjp.key = 't1';
  t2 := mjp.value::date FROM work.module_job_parameters AS mjp WHERE mjp.mod_job_id = in_mod_job_id AND mjp.key = 't2'; 

  -- Common statistic
  INSERT INTO charts.chart_data (mod_job_id, stat_name, group_name, data_date, var_name, var_value, order_id)
  SELECT in_mod_job_id AS mod_job_id, 'SCORING' AS stat_name, 'ALL' AS group_name, now()::date AS data_date, 'SUBSCRIBERS' AS var_name, count(*) AS var_value, 1 AS order_id
  FROM work.modelling_data_matrix AS d
  WHERE d.mod_job_id = in_mod_job_id;
 
  INSERT INTO charts.chart_data (mod_job_id, stat_name, group_name, data_date, var_name, var_value, order_id)
  SELECT DISTINCT
    in_mod_job_id AS mod_job_id,
    'SCORING' AS stat_name,
    'ALL' AS group_name, 
    now()::date AS data_date,
    var_name.name,
    CASE
      WHEN var_name.name = 'ACTIVE_SUBSCRIBERS'     THEN vals.count
      WHEN var_name.name = 'BIDIRECT_CONNECTIONS'   THEN vals.k
      WHEN var_name.name = 'WEEKLY_SMSES'           THEN vals.sms
      WHEN var_name.name = 'WEEKLY_VOICE_CALLS'     THEN vals.voice
      WHEN var_name.name = 'WEEKLY_TOPUPS'          THEN vals.topup
      WHEN var_name.name = 'TENURE'                 THEN vals.tenure
      ELSE NULL
    END AS var_value,
    CASE
      WHEN var_name.name = 'ACTIVE_SUBSCRIBERS'     THEN 2
      WHEN var_name.name = 'BIDIRECT_CONNECTIONS'   THEN 3
      WHEN var_name.name = 'WEEKLY_SMSES'           THEN 4
      WHEN var_name.name = 'WEEKLY_VOICE_CALLS'     THEN 5
      WHEN var_name.name = 'WEEKLY_TOPUPS'          THEN 6
      WHEN var_name.name = 'TENURE'                 THEN 7
      ELSE NULL
    END AS order_id
  FROM (
    SELECT 'ACTIVE_SUBSCRIBERS' AS name   UNION ALL
    SELECT 'BIDIRECT_CONNECTIONS' AS name UNION ALL
    SELECT 'WEEKLY_SMSES' AS name         UNION ALL
    SELECT 'WEEKLY_VOICE_CALLS' AS name   UNION ALL
    SELECT 'WEEKLY_TOPUPS' AS name        UNION ALL
    SELECT 'TENURE'
  ) AS var_name
  CROSS JOIN (
    SELECT
      count(*) AS count,
      avg(z.k     ) FILTER (WHERE z.k_row_num      BETWEEN floor(z.half_count) AND ceil(z.half_count)) AS k,
      avg(z.sms   ) FILTER (WHERE z.sms_row_num    BETWEEN floor(z.half_count) AND ceil(z.half_count)) AS sms,
      avg(z.voice ) FILTER (WHERE z.voice_row_num  BETWEEN floor(z.half_count) AND ceil(z.half_count)) AS voice,
      avg(z.topup ) FILTER (WHERE z.topup_row_num  BETWEEN floor(z.half_count) AND ceil(z.half_count)) AS topup,
      avg(z.tenure) FILTER (WHERE z.tenure_row_num BETWEEN floor(z.half_count) AND ceil(z.half_count)) AS tenure
    FROM (
      SELECT
        ((count(*) OVER ())::double precision - 1.0) / 2.0 + 1.0 AS half_count,
        row_number() OVER (ORDER BY da.k     ) AS k_row_num,
        row_number() OVER (ORDER BY da.sms   ) AS sms_row_num,
        row_number() OVER (ORDER BY da.voice ) AS voice_row_num,
        row_number() OVER (ORDER BY da.topup ) AS topup_row_num,
        row_number() OVER (ORDER BY da.tenure) AS tenure_row_num,
        da.k,
        da.sms,
        da.voice,
        da.topup,
        da.tenure
      FROM (
        SELECT
          coalesce(d.k,0) AS k,
          coalesce(d.smscount,0) AS sms,
          coalesce(d.voicesum,0) AS voice,
          coalesce(d.topup_count_weekly3*d.topup_amount_avg3,0) AS topup, 
          coalesce(d.contr_length,0) AS tenure
        FROM work.modelling_data_matrix AS d
        INNER JOIN (
          SELECT ctw.alias_id
          FROM data.call_types_weekly AS ctw
          WHERE coalesce(ctw.direction,'m') = 'm' -- Made calls, SMSs, etc.
          AND ctw.monday >= t1
          AND ctw.monday < t2
          GROUP BY ctw.alias_id
          HAVING count(DISTINCT ctw.monday) >= (t2-t1) / 7.0 -- Activity in every week of source period is required
        ) AS a
        ON d.alias_id = a.alias_id
        WHERE d.mod_job_id = in_mod_job_id
      ) AS da
    ) AS z
  ) AS vals;

  -- Use case score distribution 
  job_use_cases := (
    SELECT value
    FROM work.module_job_parameters 
    WHERE mod_job_id = in_mod_job_id AND key = 'job_use_cases'
  );

  job_use_cases := regexp_replace(job_use_cases, 'zero_day_prediction', 'zero_day_prediction_low_value,zero_day_prediction_medium_value,zero_day_prediction_high_value');
  use_cases := string_to_array(trim(job_use_cases),',');
  
  FOR uc_ind IN 1..array_upper(use_cases, 1) LOOP
    
    use_case := use_cases[uc_ind];
    
    sql_string := 'INSERT INTO charts.chart_data (mod_job_id, stat_name, group_name, data_date, var_name, var_value, order_id) 
    SELECT '||
    in_mod_job_id || ' AS mod_job_id, 
    ''SCORE_DISTRIBUTION'' AS stat_name,
    ''' || use_case || ''' AS group_name, 
    now() AS data_date,
    var_name, 
    var_value, 
    NULL AS order_id
    FROM
    (SELECT 
      ROUND(' || use_case || '_propensity_score::numeric, 2)::text AS var_name,
      count(*) AS var_value
      FROM results.module_results 
      WHERE mod_job_id = ' || in_mod_job_id || '
      AND ' || use_case || '_propensity_score IS NOT NULL
      GROUP BY var_name
    ) aa;';

    EXECUTE sql_string;
  END LOOP;
  
  PERFORM core.analyze('charts.chart_data', in_mod_job_id);
 
END;

            </code>
        </input_parameters>
        <output_parameters>
            <out></out>
        </output_parameters>
    </Approval_Statistics>
    <cleanup>
        <input_parameters>
            <DataSource1></DataSource1>
            <queryparameters></queryparameters>
            <sql>select * from core.cleanup_sequence_partitions()</sql>
            <code>
                
CREATE FUNCTION cleanup_sequence_partitions() RETURNS void
    AS $$
/* SUMMARY
 * Removes old partitions from all tables that are partitioned over a column taken from 
 * a sequence. The retention times per table are stored in table core.partition_sequence_tables. 
 *
 * Note: only cleanup condition in table core.partition_sequence_create_times is obeyed, not the
 * one in the core.partition_sequence_tables. 
 *
 * VERSION
 * 28.02.2013 MOj
 */
DECLARE
  partition_prop record;
BEGIN
  --find old partitions to delete
  FOR partition_prop IN (
    SELECT a.table_name, a.sequence_id
    FROM core.partition_sequence_create_times a
    JOIN core.partition_sequence_tables b
    ON a.table_name = b.table_name
    AND a.sequence_name = b.sequence_name
    WHERE a.time_created::date <= current_date - b.retention_dates
    AND a.cleanup IS TRUE
  )
  LOOP
    DELETE FROM core.partition_sequence_create_times
    WHERE table_name = partition_prop.table_name
    AND sequence_id = partition_prop.sequence_id;

    EXECUTE 'ALTER TABLE ' || partition_prop.table_name || ' '
         || 'DROP PARTITION IF EXISTS FOR (' || partition_prop.sequence_id || ')';
  END LOOP;
END;

            </code>
        </input_parameters>
        <output_parameters>
            <out></out>
        </output_parameters>
    </cleanup>
    <email3>
        <input_parameters>
            <command>echo "complete scoring run completed for" + $t2 | mailx -v -r "churn@mci.ir" -s "FAA Scoring run" $emailaddress</command>
            <host>localhost</host>
            <identity></identity>
            <password>$LocalhostPassword</password>
            <std.err.file></stderr.file>
            <stdout.file></stdout.file>
            <timeout>20000</timeout>
            <username>$LocalhostUsername</username>
        </input_parameters>
        <output_parameters>
            <exitstatus></exitstatus>
            <stderr></stderr>
            <stdout></stdout>
        </output_parameters>
    </email3>
</ScoringFlow_MCI_organized_nochart>

